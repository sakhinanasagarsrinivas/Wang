% \vspace{-1mm}
% \subsection{Experimental Settings}% combines memory efficiency with task-specific adaptability
% We fine-tuned the Llama 3.2 1B model with QLoRA (Quantized Low-Rank Adaptation) for task-specific applications. The pre-trained weights were frozen and quantized to 4-bit precision, reducing memory usage while supporting long input sequences. Additional low-rank matrices in full precision (e.g., FP16) were introduced to key layers such as query, key-value, output, and gate projections, with a rank of 16 and a scaling factor of 16. During forward computation, the 4-bit weights were combined with LoRA weights, and only the LoRA weights were updated during backward passes, ensuring efficient fine-tuning on resource-constrained hardware without compromising performance. Key training parameters included a batch size of 2 per GPU (the workload for each GPU in a distributed training setup), with gradient accumulation over 4 steps to simulate an effective batch size of 8. This approach allowed gradients to be calculated and accumulated over multiple forward and backward passes before updating the LoRA weights, optimizing memory usage and maintaining training stability. The training process began with 10 warm-up steps, during which the learning rate (\(lr\)) was gradually increased from zero to its target value (\(lr = 1 \times 10^{-3}\)), stabilizing updates and preventing abrupt optimization changes. Training concluded after 60 optimization steps, where accumulated gradients were used to iteratively update the model's trainable parameters, ensuring convergence and effective learning rather than being constrained by a fixed number of epochs. The AdamW optimizer used block-wise quantization to store parameters such as gradients, momentum, and variance in 8-bit precision, significantly reducing memory usage while maintaining performance comparable to 32-bit optimizers. A weight decay rate of 0.01 was applied to prevent overfitting, and the learning rate was dynamically adjusted using a linear scheduler for stable convergence. Mixed-precision training techniques, specifically BF16 (16-bit brain floating point), were optimized for NVIDIA A100 GPUs to accelerate computation and ensure training stability. Random seeds ensured reproducibility, and all training outputs were logged for analysis.

% \section{Experimental Settings}

% The fine-tuning process for the Meta LLaMA 3.2-1B model was carried out in three sequential stages, progressively refining the modelâ€™s ability to generate responses by leveraging different datasets. The first phase involved fine-tuning on a QA dataset, where the model learned fundamental question-answering abilities. This was followed by Direct Preference Optimization (DPO) fine-tuning, which improved response alignment by distinguishing between preferred and rejected responses. Finally, the model was further enhanced through Retrieval-Augmented Instruction Tuning (RAIT), which integrated retrieved contextual information to support multi-hop reasoning and knowledge grounding.

% For all three stages, training was conducted using Low-Rank Adaptation (LoRA), which enabled efficient parameter tuning while keeping the majority of model weights frozen. LoRA was configured with a rank of 16, a scaling factor of 16, and no dropout, optimizing both performance and memory efficiency. The model was trained with a batch size of 2 per device, using 4 gradient accumulation steps to achieve an effective batch size of 8. The AdamW optimizer with 8-bit quantization was employed, using a learning rate of \(2 \times 10^{-4}\) with a linear decay schedule and a weight decay of 0.01. Mixed precision training was applied with FP16 or BF16, depending on hardware compatibility, and all experiments were executed on NVIDIA Tesla T4 GPUs. To handle long-context inputs effectively, all fine-tuning steps were conducted with a maximum sequence length of 4096 tokens.

% Initially, QA and RAIT fine-tuning were performed for 5 epochs, while DPO was trained for 2 epochs. However, the model did not reach convergence in these initial runs. To improve performance, additional fine-tuning was conducted, extending QA and RAIT training to 15 epochs and DPO to 5 epochs. This additional training allowed the model to better capture instruction-following patterns, refine preference-based response ranking, and integrate contextual information from retrieved knowledge.

% The entire fine-tuning pipeline was implemented in Python using PyTorch and Unsloth, leveraging gradient checkpointing to reduce memory consumption. Datasets were processed with the Hugging Face \texttt{datasets} library, and model training was managed using \texttt{SFTTrainer} for supervised fine-tuning and \texttt{DPOTrainer} for preference optimization. Performance was evaluated across all stages using standard NLP metrics, including BLEU, ROUGE, METEOR, BERTScore, and Similarity metric. 
