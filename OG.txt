%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}  % for mathematical symbols and equations
\usepackage{amssymb}  % for additional math symbols
\usepackage{graphicx} % for including images
\usepackage{amsfonts} % for additional font symbols
\usepackage{tikz}     % for drawing graphs (DAG)
\usetikzlibrary{graphs}

\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{fancyhdr}
\usepackage{listings}

\usepackage{booktabs}
\usepackage{array}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.



% Updated color choices without green and bold text
\definecolor{codebg}{rgb}{0.95,0.95,0.95}      % Slightly lighter background
\definecolor{keyword}{rgb}{0.15,0.15,0.75}     % Softer dark blue for keywords
\definecolor{comment}{rgb}{0.5,0.5,0.5}        % Neutral gray for comments
\definecolor{string}{rgb}{0.8,0.15,0.15}       % Softer red for strings

\lstdefinestyle{mypython}{
    backgroundcolor=\color{codebg},
    keywordstyle=\color{keyword}\bfseries,      % Bold keywords
    commentstyle=\color{comment}\bfseries\itshape, % Bold and italic comments
    stringstyle=\color{string}\bfseries,        % Bold strings
    showstringspaces=false,
    basicstyle=\ttfamily\footnotesize\bfseries, % Decreasing the font size and bold
    frame=none,
    breaklines=true,
    language=Python
}



% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title
% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{The AI Inventorney : An Agentic Swarm for Automated Open-Domain Functional Hypothesis Discovery for Patent Ideation and Drafting in Intellectual Property Management}   % Inventa
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

%This comprehensive analysis reveals several key trends in LLM-based scientific discovery systems:
%
%Evolution from single-purpose tools to integrated frameworks
%Increasing emphasis on multi-agent architectures
%Growing focus on rigorous evaluation through specialized benchmarks
%Balance between automation and expert oversight
%
%The field appears to be moving toward more comprehensive systems that can handle end-to-end scientific workflows while maintaining scientific rigor. However, challenges remain in balancing novelty with feasibility and ensuring the practical applicability of generated research ideas.

\maketitle
% The AI Scientist framework is a groundbreaking system designed to fully automate the scientific discovery process using Large Language Models (LLMs). It operates in three main phases: Idea Generation, where it brainstorms novel research directions based on a provided codebase and evaluates their novelty; Experimental Iteration, where it writes code, conducts experiments, collects results, and refines experimental plans; and Paper Write-up, where it drafts a complete scientific manuscript, including visualizations, and performs automated reviews of the output. This end-to-end pipeline leverages tools like Aider for code implementation and automated reviewers for quality assessment, demonstrating the ability to produce high-quality research papers at a low cost. By mimicking human scientific processes, the framework accelerates open-ended scientific exploration while building a growing archive of discoveries.
\begin{abstract}
In this paper, we introduce \textbf{Inventa}, an AI-powered multi-agent framework that automates patent ideation and drafting. Traditional patent methods are time-consuming and prone to information overload, limiting both ideation and drafting efficiency. \textbf{Inventa} employs a meta-agent to orchestrate specialized sub-agents for tasks like hypothesis generation, knowledge extraction, and drafting. By integrating large language models (LLMs) and knowledge graphs, the framework generates novel patent ideas, evaluates prior art, and automates drafting. Error-handling mechanisms, including human experts and AI reviewers, ensure document quality and accuracy. This framework streamlines the patent process, reducing human effort and accelerating innovation in intellectual property management. Empirical experiments show that *Inventa* significantly improves patent ideation and drafting efficiency, enhancing both innovation and the overall patent process.
\vspace{-5mm}
\end{abstract}


\section{Introduction}
``\textit{An invention is something that was 'impossible' until then; that's why governments grant patents.}” (Robert A. Heinlein). A patent is an exclusive right granted by the government, allowing an inventor to make, use, and sell their novel, non-obvious, and useful invention. In return, the inventor publicly discloses the invention’s details, promoting technological advancement and innovation. A standard patent document outlines technical aspects and provides legal protection through specific claims and diagrams, facilitating commercialization. Companies pursue patents to protect innovations and establish market presence, with patent applications serving as a key strategy for safeguarding intellectual property in emerging fields and disruptive technologies. Valued at hundreds of billions of dollars, the global patent market involves specialized IP firms and brokers who play crucial roles in trading and licensing, highlighting patents as vital economic assets that drive innovation and growth. In today’s rapidly advancing technological landscape, there is a need and necessity to accelerate the automation of high-caliber patent ideation and drafting. Traditional patent ideation methods require extensive research, and navigating prior art literature is overwhelming and demands a solid understanding of technical domains and patent law. An overemphasis on technical feasibility can overshadow market needs, resulting in inventions that are technically sound but commercially unviable. Additionally, siloed expertise limits interdisciplinary innovation. Conventional patent drafting, reliant on human experts, is resource-intensive, prone to errors, and inefficient given the multidisciplinary demands, leading to delayed filings that affect market competitiveness. This highlights the need for more efficient and reliable approaches to managing patent creation tasks. To address these challenges, AI-powered research and drafting solutions can transform the patent process by simplifying information navigation, identifying relevant prior art, proposing new inventions, and automating draft generation. This allows patent attorneys to focus on reviewing drafts and providing feedback to enhance patent ideation. In recent times, applications of AI-driven multi-agent frameworks \cite{brown2020language, dibia2024autogen, he2024webvoyager} have revolutionized various sectors by enhancing efficiency and automating complex workflows. There is growing interest in using these frameworks to transform scientific discovery by automating key steps in uncovering new knowledge. This involves analyzing and synthesizing vast amounts of literature, identifying unknown patterns or phenomena, accelerating the formulation of new hypotheses, and testing these hypotheses to generate insights and achieve scientific breakthroughs. However, the use of multi-agent frameworks in patent-related tasks is still emerging and has the potential to revolutionize complex, patent-driven workflows by automating various tasks, thereby transforming the landscape of intellectual property management.


\clearpage
\newpage

\begin{figure*}[!ht]
\vspace{-2mm}
\centering
\resizebox{0.73\linewidth}{!}{ 
\hspace*{0mm}\includegraphics[keepaspectratio,trim=0.0cm 0cm 0cm 0.025cm,clip]{Image1.pdf} % left, bottom, right, top
}
\vspace{-5mm}
\caption{Overview of the proposed AI-powered framework, \textbf{Inventa}, for patent ideation, drafting, and evaluation. The process begins with (a) Idea Submission, followed by (b) Ideation Genesis, where insights from the Patent Hypothesis Exploration Graph (PHEG), the Scientific Knowledge Graph (SciKG), and the Patent Knowledge Graph (PKG) guide the structured development of the idea. In (c) Patent Assessment, the framework conducts a thorough search for existing patents related to the invention to determine novelty of invetion invention and evaluates whether to advance the patent application or revisit the ideation phase for further refinement. In (d) Patent Drafting, the refined concept is transformed into a comprehensive patent document. Finally, in (e) Critique Evaluation, human experts and benchmark AI models assess the patent for quality and compliance, ensuring it satisfies all necessary requirements before submission to the patent office.}
\label{fig:figure1}
\vspace{-2mm}
\end{figure*}




\begin{figure*}[!ht]
\vspace{-1mm}
\centering
\resizebox{0.78\linewidth}{!}{ 
\hspace*{0mm}\includegraphics[keepaspectratio,trim=0.0cm 0cm 0cm 0.05cm,clip]{Image2.pdf} % left, bottom, right, top
}
\vspace{-3mm}
\caption{Overview of the workflow of the proposed AI-powered multi-agent framework, \textbf{Inventa}, for automated patent ideation and drafting. The meta-agent coordinates specialized agents for tasks such as hypothesis generation, scientific discovery, ideation, and patent drafting. The process begins with the inventor submitting problem statements and ideas. Using Subject-Action-Object (SAO) analysis, the hypothesis-generation agent constructs Patent Hypothesis Exploration Graphs (PHEGs) from patent literature to identify gaps and generate new hypotheses. The scientific-discovery agent validates these hypotheses against Scientific Knowledge Graphs (ScKGs) to ensure feasibility and alignment with empirical research. The validated hypotheses are refined by the ideation agent into innovative, non-obvious, and patentable inventions. Following ideation, the prior art search agent utilizes Patent Knowledge Graphs (PKGs) to analyze existing patents and confirm novelty, while the patentability evaluation agent assesses criteria such as feasibility, non-obviousness, and utility. Once approved, the validated inventions are organized as inputs for patent drafting. The meta-agent decomposes the drafting task into subtasks—such as background writing, detailed description, and claim generation—and assigns them to expert agents. An iterative feedback loop, involving human experts and advanced language models, ensures coherence, legal compliance, and accuracy. The final patent document is prepared for submission to the relevant patent office.
}
\label{fig:figure2}
\vspace{-3mm}
\end{figure*}


\clearpage
\newpage

%With multiple AI-driven agents powered by large language models (LLMs) such as OpenAI GPT-4o\cite{openai2024gpt4o} and Anthropic Sonnet\cite{anthropic2024claude} collaborating efficiently—each agent specializing in specific tasks—they can improve accuracy and reduce reliance on human intervention. Multi-agent frameworks can automate both patent ideation and drafting, critical processes for protecting intellectual property by generating innovative ideas, expediting the drafting process, and helping innovators secure intellectual property more quickly. This comprehensive automation boosts competitiveness and fosters technological advancement. In this work, we propose a multi-agent framework, \textbf{Inventa}, with agent chaining to automate patent ideation and drafting. In this framework, a meta-agent orchestrates multiple expert sub-agents—powered by LLMs and specialized in different tasks—that collaborate sequentially to handle the complex processes of patent ideation and drafting. The proposed multi-agent framework dynamically selects, integrates, and sequences expert agent invocations for novel patent creation, considering their relevance according to documented protocols and their interdependencies. Furthermore, we incorporate error-handling mechanisms with attributable reflection by utilizing critique reviewers, including human experts, a gold-standard language model (Gold-LLM-as-a-Judge), and a reward model (Reward-Model-as-Judge) to provide feedback. These critique reviewers are essential for producing robust patent applications, given the high stakes associated with intellectual property protection. The attributable reflection mechanism involves reviewing the outputs of expert agents using critique reviewers to identify and correct errors traceable back to task-specific expert agents, thereby ensuring high-quality patent documents. Figures \ref{fig:figure1}–\ref{fig:figure2} provide an overview of the proposed AI-powered multi-agent framework for patent ideation and drafting. Figure \ref{fig:figure1} illustrates the entire framework, from idea submission to patent assessment, with the meta-agent orchestrating the workflow. Figure \ref{fig:figure2} details the multi-agent framework's workflow, where specialized agents handle distinct tasks and critique evaluators review outputs for quality and compliance.


\section{Hypothesis Approaches in Patents vs. Scientific Articles}
A patent and a scientific article serve distinct purposes in innovation and research. A patent outlines an invention's novelty, usefulness, and non-obviousness, providing technical and legal specifications, including detailed descriptions and illustrations of how the invention works and what is protected. It emphasizes specific, practical applications. In contrast, a scientific article presents research findings, experiments, and theories, focusing on methodology, results, and analysis. These articles aim to facilitate open knowledge exchange and advance scientific understanding, but they do not offer the legal protection that patents do. Given these fundamental differences, the methods for extracting and generating hypotheses from each document diverge significantly. In patents, hypothesis extraction involves identifying practical, functional hypotheses related to the invention's technical claims, with an emphasis on solving specific problems and demonstrating novelty. Conversely, hypothesis extraction from scientific articles focuses on research-driven, theoretical propositions aimed at advancing knowledge. When generating novel hypotheses, the processes also differ: for patents, this involves confirming that the idea is new, undisclosed, and not an obvious extension of existing knowledge, with feasibility tested through prototypes or simulations. In scientific research, hypothesis generation requires comprehensive empirical testing and peer validation. Understanding these distinctions highlights the varied approaches to hypothesis development in these two domains.


\section{\textbf{Inventa}: A Proof of Concept}
The proposed multi-agent framework, \textbf{Inventa}, addresses the complexities of patent ideation and drafting by automating key tasks through the collaboration of specialized agents, orchestrated by a meta-agent and powered by large language models (LLMs). (a) First, we independently construct Patent Hypothesis Exploration Graphs (PHEG) by extracting functional hypotheses from granted patents using Subject-Action-Object (SAO) triplet analysis. This analysis decomposes each patent’s content into three key components: the subject, representing the main entity or technology (for example, a device or system); the action, describing the operation or process applied to or by the subject; and the object, indicating the outcome or result of the action, such as a product or improvement. To efficiently discover novel hypotheses within these exploration graphs, we propose the Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm. We model hypothesis discovery as a Markov Decision Process (MDP) and employ advanced techniques such as graph neural networks (GNNs) and reinforcement learning to navigate complex exploration graphs. This approach aims to uncover unique, non-obvious innovative hypotheses that lead to novel, marketable, and legally protectable inventions, ultimately driving technological innovation. (b) In parallel, we construct a Multimodal Patent Knowledge Graph (MPKG) to efficiently organize and retrieve information across multiple patent documents, enhancing the efficiency of patent searches. MPKG organizes relationships among inventions, technologies, and the underlying concepts found in existing patents. We utilize the Graph RAG technique, which integrates relevant information from the patent graph with advanced language models, improving the relevance and accuracy of search results while providing deeper insights into complex patent information. (c) Simultaneously, we build a Multimodal Scientific Knowledge Graph (ScKG) to organize and retrieve information from scholarly publications. These ScKGs capture relationships among key scientific concepts, experimental methods, empirical results, and novel findings within the existing scientific literature. Using the Graph RAG technique, we extract subgraphs enriched with relational knowledge from the ScKGs. By integrating both the structural and semantic information from the scientific graphs, the language model delivers more accurate search results and provides deeper insights into complex scientific research and discoveries. In short, PHEG discovers new hypotheses from patents, MPKG organizes existing patent knowledge for retrieval, and ScKG captures scientific insights from scholarly articles—each constructed independently and in parallel, with no dependencies. Initially, the inventors present problem statements along with exploratory ideas, outlining the challenges they face, insights from domain expertise, and envisioned innovations. The meta-agent engages the hypothesis generation agent to process these problem statements and ideas. Acting as the Graph RAG agent, the hypothesis generation agent first applies the APHD technique to explore PHEGs, generating initial hypotheses based on the inventors' input. It then extracts insights from PKGs constructed from patent literature to enhance and validate these hypotheses by identifying relevant inventions, technological trends, and gaps in patented solutions. The agent formulates novel, non-obvious hypotheses that directly address the challenges posed by the inventors while also filling gaps in existing technologies. However, unlike scientific journals, patents typically do not disclose detailed methodologies, experimental results, or in-depth discussions of findings and their implications, focusing instead on claimed inventions and their implementation. To validate and strengthen the newly formulated hypotheses, the meta-agent engages the scientific discovery agent. This Graph RAG agent explores structured knowledge in ScKGs to uncover scientific advancements and breakthroughs that support and improve the hypotheses, ensuring they are grounded in current scientific knowledge. Drawing from the robust, data-driven insights provided by the scientific discovery agent, the ideation agent transforms these refined hypotheses into innovative, practical, and patentable inventions that address the inventors' specific challenges. The ideation agent ensures these inventions are feasible and non-obvious, aligning them with current scientific and technological landscapes. Finally, the ideation agent organizes the generated ideas into well-defined primary inputs for the patent drafting process, providing a concise description of the invention or technological innovation to be patented. This description includes the purpose and objectives, technical specifications, unique features, and functionalities. This thorough documentation ensures that all critical aspects of the invention are captured, facilitating an effective and comprehensive patent application.
The prior art search begins with input from the ideation agent, which provides refined hypotheses and proposed inventions. The prior art search agent conducts a thorough search for existing patents related to the invention, analyzing the results to assess the novelty of the proposed ideas. Based on this analysis, the patentability evaluation agent assesses the invention’s potential for patentability according to criteria such as novelty, feasibility, non-obviousness, and utility, determining whether to proceed with the patent application. If the invention is deemed non-patentable, the meta-agent informs the hypothesis generation agent of the lack of novelty and provides details on the prior art that led to this conclusion. This feedback facilitates iterative refinement of the invention, potentially identifying unique aspects that could make it patentable. If the invention is deemed patentable, subsequently, the meta-agent assigns the patent drafting task by decomposing it into smaller, manageable sub-tasks—such as background writing, detailed description, summary generation, claim drafting, abstract generation, and title generation. These sub-tasks are assigned to the appropriate task-specific expert agents sequentially, based on their dependencies. The background-writing agent drafts the background section, describing the problem addressed and discussing existing solutions or technologies to set the context and highlight the need for the invention. The detailed-description agent then expands on the technical aspects, explaining the invention’s features and functionalities. Using these detailed descriptions, the claim-generation agent drafts the patent claims—both independent and dependent—using precise legal language to define the scope of the invention. The summary-generation agent outlines the key aspects and advantages of the invention, emphasizing how it solves the identified problems. Meanwhile, the abstract-generation agent drafts the abstract, providing an overview of the invention, including its purpose, main features, and potential applications. The title-generation agent creates a precise and descriptive title for the patent application that accurately reflects the invention’s nature. Finally, the legal-compliance agent reviews the entire draft for legal accuracy, ensuring compliance with patent office standards and rules. This review includes checking the title, abstract, descriptions, and claims for potential issues. For final validation and error handling, critique reviewers—including human experts and advanced language and reward models—examine the entire patent draft for coherence, completeness, legal compliance, and accuracy. If the reviewers identify errors, the meta-agent orchestrates an iterative feedback loop, routing the draft back to the appropriate expert agents for corrections. After all sections are reviewed and refined, the meta-agent compiles the patent document, ensuring it meets all standards and maximizes reliability. The finalized document is then prepared for submission to the relevant patent office.


\vspace{0mm}
\section{Experiments}

\subsection{Datasets \& Experimental Settings}
The table \ref{tab:universal_transformer_hierarchy} illustrates the hierarchical evolution leading to the Universal Transformer. It starts with **Vaswani et al.’s Transformer** architecture, which introduced **self-attention** to enable parallel processing, followed by **Alex Graves' Adaptive Computation Time (ACT)**, which allowed dynamic adjustment of computational steps. The **Universal Transformer** (US10740433B2) synergizes these advancements by combining self-attention, ACT, and **recurrent depth processing**, resulting in a flexible and general-purpose model for sequence tasks.

\begin{table*}[ht!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, 
title=\textbf{Hierarchical Relationship and SAO Triplets Network}, 
box align=center, width=\textwidth, halign=center, valign=center]

\footnotesize
\renewcommand{\arraystretch}{1.2}  % Adjust row height for better alignment
\begin{tabular}{|p{2.5cm}|p{4cm}|p{4cm}|p{4.5cm}|}
\hline
\textbf{Level} & \textbf{Subject} & \textbf{Action} & \textbf{Object and Insight} \\
\hline
Level 1: Transformer Architecture (Vaswani et al., 2017) 
& Transformer architecture 
& Uses self-attention for sequence modeling, eliminating the need for recurrence. 
& Achieves parallel processing and improved efficiency in NLP tasks (e.g., translation, language modeling). 
\newline \textbf{Insight}: Introduces a parallel way to process sequences, breaking away from sequential dependencies in RNNs. \\
\hline
Level 2: Adaptive Computation Time (Alex Graves, 2017) 
& Adaptive computation in RNNs 
& Introduces a halting mechanism to determine when processing can stop based on input complexity. 
& Optimizes computational steps dynamically, improving efficiency for varying task complexities. 
\newline \textbf{Insight}: ACT allows dynamic control over computation time, complementing the rigid depth structure of Transformers. \\
\hline
Level 3: Universal Transformer (US10740433B2, 2020) 
& Universal Transformer system 
& Combines recurrent depth processing with self-attention and integrates ACT to adapt computational steps dynamically. 
& Achieves superior generalization and flexibility across multiple sequence tasks. 
\newline \textbf{Insight}: Fuses ideas from self-attention (Vaswani) and adaptive computation (Graves) to build a more versatile model for sequence tasks. \\
\hline
\end{tabular}

\end{tcolorbox}
\caption{The table breaks down the subject, action, object, and insights for each hierarchical level, organized from the earliest development (Transformer) to the final model (Universal Transformer). This structure highlights how each concept builds on the previous, with the insights serving as conceptual bridges between levels, culminating in the versatile Universal Transformer.}
\label{tab:universal_transformer_hierarchy}
\end{table*}


\newpage

\subsection{Results}
The evaluation of the tool learning framework spans four key stages: task planning, tool selection, tool invocation, and response generation, ensuring precision and adaptability. In task planning, \textbf{Accuracy (Acc)} measures how well queries are decomposed into ordered subtasks. \textbf{Tool Usage Awareness (TUA)} captures the framework’s ability to identify when external tools are necessary. Metrics like \textbf{Pass Rate (PR)} assess task execution success, while \textbf{Dependency Graph Consistency (DGC)} ensures the correct maintenance of subtask dependencies. All metrics—\textbf{PR}, \textbf{TUA}, \textbf{Acc}, and \textbf{DGC}—range from 0 to 1, with higher values indicating better performance. The framework’s performance, using various language models as computational engines for agents, is compared across these evaluation metrics in Table \ref{tab:task_planning_metrics}.
For tool selection, the framework evaluates its ability to retrieve and rank relevant tools effectively. The \textbf{Recall@K} metric measures the proportion of relevant tools retrieved within the top-K selections, while \textbf{Completeness at K (COMP@K)} ensures that all necessary tools are included among the top-K results. The \textbf{Normalized Discounted Cumulative Gain (NDCG@K)} assesses both the relevance of tools and their ranking positions, emphasizing higher scores for correctly ranked tools. These metrics—\textbf{Recall@K}, \textbf{COMP@K}, and \textbf{NDCG@K}—range from 0 to 1, with higher values reflecting better tool selection performance. The framework's ability to optimize tool retrieval is benchmarked across these metrics in Table \ref{tab:tool_selection_metrics}. In tool invocation, the framework’s ability to accurately identify and execute tool calls with the correct parameters is evaluated. The \textbf{Parameter Consistency (PC)} metric assesses how accurately the extracted parameters align with the ideal parameter set for each query. The \textbf{Error Rate (ER)} metric captures the proportion of incorrectly formatted parameters. Both metrics—\textbf{PC} and \textbf{ER}—range from 0 to 1, with higher values for \textbf{PC} indicating better parameter accuracy and lower values for \textbf{ER} reflecting fewer errors. The framework’s performance in correctly executing tool invocations is compared using these metrics in Table \ref{tab:tool_invocation_metrics}. In response generation, the framework’s ability to produce accurate and relevant responses is evaluated. The \textbf{BLEU (Bilingual Evaluation Understudy)} metric measures the precision of n-grams in the generated responses compared to reference texts, capturing linguistic similarity. The \textbf{ROUGE-L} metric assesses the overlap by calculating the longest common subsequence between generated and reference texts, focusing on content similarity. Both metrics—\textbf{BLEU} and \textbf{ROUGE-L}—range from 0 to 1, with higher values indicating better alignment and relevance of the generated responses. The framework’s performance in generating high-quality responses is benchmarked across these metrics in Table \ref{tab:abstractive_summarization_metrics}. 




%User-centric metrics like **User Satisfaction Score (USS)**, **Perceived Usefulness (PU)**, and **Net Promoter Score (NPS)** provide insight into emotional satisfaction, practical utility, and user advocacy. Together, these metrics offer a holistic view of the framework’s performance, enabling the generation of accurate, high-quality outputs and helping developers identify areas for further improvement.

%In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. 


\vspace{-2mm}
\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.0}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{TUA} & \textbf{PR} & \textbf{Acc} & \textbf{DGC} \\
\midrule
OpenAI GPT-4 & 0.92 & 0.90 & 0.89 & 0.93 \\
Claude 3 Opus & 0.91 & 0.89 & 0.88 & 0.92 \\
Gemini 1.5 Pro & 0.89 & 0.87 & 0.86 & 0.90 \\
Gemini 1.5 Flash & 0.88 & 0.85 & 0.84 & 0.89 \\
OpenAI GPT-4 Turbo & 0.87 & 0.84 & 0.83 & 0.88 \\
Claude 3 Haiku & 0.86 & 0.83 & 0.82 & 0.87 \\ \hline
\textbf{Inventa} & 0.94 & 0.92 & 0.91 & 0.95 \vspace{-0.25mm} \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Comparison of task planning performance across language models using key metrics: PR assesses task execution success, TUA reflects tool usage recognition, Acc measures accurate query decomposition, and DGC ensures correct sub-task dependencies.}
\label{tab:task_planning_metrics}
\vspace{-1mm}
\end{table}


\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.0}
\resizebox{0.485\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Recall@K} & \textbf{NDCG@K} & \textbf{COMP@K} \\
\midrule
OpenAI GPT-4 & 0.93 & 0.91 & 0.89 \\
Claude 3 Opus & 0.92 & 0.90 & 0.88 \\
Gemini 1.5 Pro & 0.90 & 0.88 & 0.86 \\
Gemini 1.5 Flash & 0.89 & 0.87 & 0.85 \\
OpenAI GPT-4 Turbo & 0.88 & 0.86 & 0.84 \\
Claude 3 Haiku & 0.87 & 0.85 & 0.83 \\ \hline
\textbf{Inventa} & 0.95 & 0.93 & 0.91 \vspace{-0.5mm} \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Comparison of tool selection performance across language  models using key metrics. \textbf{Recall@K} measures relevant tool retrieval, \textbf{NDCG@K} evaluates ranking quality, and \textbf{COMP@K} assesses completeness within top-K selections.}
\label{tab:tool_selection_metrics}
\vspace{-1mm}
\end{table}


\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.0}
\resizebox{0.29\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{PC} & \textbf{ER} \\
\midrule
OpenAI GPT-4 & 0.93 & 0.05 \\
Claude 3 Opus & 0.92 & 0.06 \\
Gemini 1.5 Pro & 0.90 & 0.07 \\
Gemini 1.5 Flash & 0.89 & 0.08 \\
OpenAI GPT-4 Turbo & 0.88 & 0.09 \\
Claude 3 Haiku & 0.87 & 0.10 \\ \hline
\textbf{Inventa} & 0.95 & 0.04 \vspace{-0.5mm} \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{The table demonstrates the language's performance on tool invocation using \textbf{PC} (Parameter Consistency) and \textbf{ER} (Error Rate). PC measures how accurately parameters are identified, while ER tracks the proportion of incorrect or missing parameters, with lower values indicating fewer errors.}
\label{tab:tool_calling_metrics}
\vspace{-2mm}
\end{table}


\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.0}
\resizebox{0.345\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BLEU} & \textbf{ROUGE-L} \\
\midrule
OpenAI GPT-4 & 0.72 & 0.70 \\
Claude 3 Opus & 0.70 & 0.69 \\
Gemini 1.5 Pro & 0.64 & 0.62 \\
Gemini 1.5 Flash & 0.61 & 0.59 \\
OpenAI GPT-4 Turbo & 0.60 & 0.58 \\
Claude 3 Haiku & 0.59 & 0.57  \\ \hline
\textbf{Inventa} & 0.85 & 0.83  \vspace{-1mm} \\
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Performance of language models on patent-drafting tasks, with \textbf{BLEU} and \textbf{ROUGE-L} metrics calculated between the framework-generated drafts and publicly available patents as ground truth. Higher values indicate better summary generation.}
\label{tab:abstractive_summarization_metrics}
\vspace{-7mm}
\end{table}


\clearpage
\newpage

Hi

\clearpage
\newpage

\bibliography{aaai25}

\clearpage
\newpage

\appendix
\section{Technical Appendix}


\section{Modeling the Multi-Agent Framework}
The patent ideation and drafting process can be formalized within a multi-agent framework, where a meta-agent coordinates expert agents to manage various subtasks. The overall task \(Q\), such as patent ideation to drafting, is decomposed into a set of subtasks \(\{q_1, q_2, \dots, q_n\}\), where each subtask \(q_i\) represents activities like hypothesis generation or prior art search. For each subtask \(q_i\), the meta-agent selects an expert agent \(t_j\) by maximizing the cosine similarity between the subtask query \(q_i\) and the documented protocol \(d_j\) of the agent, defined as follows: \(\text{sim}(v(q_i), v(d_j)) = \frac{v(q_i) \cdot v(d_j)}{|v(q_i)| |v(d_j)|}\), where \(v(q_i)\) and \(v(d_j)\) are the vector representations of the subtask query and the documented protocol obtained from text-embedding models. The documented protocol of an agent provides guidelines describing its capabilities, input requirements, output formats, and constraints, ensuring that the agent operates within its defined scope. The selection process is formulated as \(\text{SelectAgent}(q_i) = \arg\max_{t_j} \text{sim}(v(q_i), v(d_j))\), where \(t_j\) is the expert agent and \(d_j\) is the protocol describing the agent's relevance to the task. Subtask dependencies are handled through a Directed Acyclic Graph (DAG) \(\mathcal{G} = (\mathcal{V}, \mathcal{E})\), where the nodes \(\mathcal{V}\) represent the subtasks \(q_i\), and the edges \(\mathcal{E}\) indicate the dependencies \(e_{ij}\) between them. The acyclic property of the DAG guarantees that dependencies are respected, with subtasks processed sequentially when necessary. The dependency graph structure ensures that each subtask is completed in the correct order. Expert agent \(t_j\) executes subtask \(q_i\) if and only if all prerequisite subtasks \(q_j\) in its dependency set have been completed. Independent subtasks can be executed simultaneously, enhancing efficiency. The DAG structure also prevents circular dependencies, allowing for parallel processing without the risk of infinite loops. After the meta-agent selects the appropriate expert agent for a subtask, the expert model invocation begins. The selected agent is invoked with the necessary parameters, such as instructions, context, and the specific query associated with the subtask. The invocation process is formalized as \(t_j(p_1, p_2, \dots, p_k)\), where \(t_j\) represents the expert agent, and \(p_1, p_2, \dots, p_k\) are the input parameters. The agent processes the task and generates an output \(r_i\), which is then aggregated with the outputs of other agents involved in the overall task. The meta-agent synthesizes these outputs \(R = \{r_1, r_2, \dots, r_n\}\) into a coherent and comprehensive response \(A\), using pre-trained knowledge \(\theta\) to integrate and refine the information to obtain a complete, coherent, and legally sound patent draft. This draft is subsequently reviewed and refined in an iterative loop, ensuring high quality and accuracy before finalization. Feedback \(F_i\) is provided by human experts or benchmark LLMs to assess the draft's consistency, legal compliance, and completeness. Each iteration refines the draft \(A_i\) by re-invoking expert agents or adjusting subtasks as needed. This process continues until the draft meets the required quality or the maximum number of iterations \(N_{\text{max}}\) is reached. The refinement process is modeled as \(A_{i+1} = \text{MetaAgent}_{\text{LLM}}(Q, A_i, F_i)\). The framework's objective is to produce the optimal patent draft \(A^*\) by maximizing the conditional probability \(P(A \mid Q, T, D, \theta)\) of generating a draft that captures the invention's novelty and complies with legal standards, ensuring it is ready for submission.

\subsection{Evaluation Metrics for Agentic Framework}
We evaluate the agentic framework \cite{qu2024tool} using metrics across four stages: task planning, expert agent selection, agent calling, and response generation. The framework's performance is evaluated through various metrics to ensure precision, adaptability, and efficiency in dynamic problem-solving. (a) In the task planning stage, we evaluate the framework’s ability to decompose queries and execute tasks effectively. The \textbf{Accuracy (Acc)} metric measures how well the framework decomposes queries into ordered subtasks. Given \(N\) queries, we define \(o_{i,k}\) as the number of subtasks for query \(i\), where \(o_{i,k} = 1\) if subtask \(k\) of query \(i\) is correctly identified and ordered, and 0 otherwise. The accuracy across all queries is defined as:

\vspace{-1mm}
\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{Acc} = \frac{\sum_{i=1}^{N} \sum_{k=1}^{K_i} o_{i,k}}{\sum_{i=1}^{N} K_i} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
It provides a comprehensive measure of the framework’s decomposition performance across all queries. Next, we consider the \textbf{Agent Usage Awareness (AUA)}, which quantifies the framework’s recognition of when external expert agents are necessary. Given \(N\) queries, each potentially requiring multiple agents, we define \(R_{i,j}\) as a binary variable for the \(j\)-th agent related to the \(i\)-th query: \(R_{i,j} = 1\) if the correct expert agent \(j\) is recognized for query \(i\), and 0 otherwise. The AUA metric is expressed as:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{AUA} = \frac{\sum_{i=1}^{N} \sum_{j=1}^{M_i} R_{i,j}}{\sum_{i=1}^{N} M_i} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
It captures the ratio of successful agent recognitions to the total opportunities across all queries. Following this, the \textbf{Pass Rate (PR)} evaluates task execution success by tracking completed tasks for each query. For query \(i\), with \(M_i\) tasks, we define \(S_{i,j}\) as a binary indicator of success for the \(j\)-th task: \(S_{i,j} = 1\) if task \(j\) for query \(i\) is executed successfully, and 0 otherwise. The pass rate is given by:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{PR} = \frac{\sum_{i=1}^{N} \sum_{j=1}^{M_i} S_{i,j}}{\sum_{i=1}^{N} M_i} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
It provides a normalized success rate across all tasks. Finally, the \textbf{Dependency Graph Consistency (DGC)} evaluates whether the correct dependency structure among subtasks is maintained for each query. For each query \(i\), consider a directed acyclic graph (DAG) \(\mathcal{G}_i = (\mathcal{V}_i, \mathcal{E}_i)\), where \(\mathcal{V}_i\) is the set of subtasks for query \(i\), and \(\mathcal{E}_i\) represents dependencies among these subtasks. Let \(C_{i,(u, v)} \text{ s.t. } u, v \in \mathcal{V}\) be an indicator for each dependency \((u, v) \in E_i\): \(C_{i,(u, v)} = 1\) if the dependency is correctly maintained, and 0 otherwise. The DGC metric across all queries is expressed as:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{DGC} = \frac{\sum_{i=1}^{N} \sum_{(u, v) \in E_i} C_{i,(u, v)}}{\sum_{i=1}^{N} |E_i|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
It captures the framework’s ability to maintain the correct ordering and relationships among subtasks across all queries. These metrics highlight the importance of accurate subtask decomposition, agent usage awareness, task execution success, and dependency graph management. (b) \textbf{Agent} selection metrics focus on retrieving and ranking relevant expert agents for query handling. The \textbf{Recall@K} metric evaluates the proportion of relevant expert agents retrieved within the top-K selections across \(N\) queries:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{Recall@K} = \frac{1}{N} \sum_{i=1}^{N} \frac{|T_{i}^* \cap T_{i}^K|}{|T_{i}^*|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(T_{i}^*\) is the ground-truth set of expert agents for query \(i\), and \(T_{i}^K\) is the top-K selected expert agents for the \(i\)-th query. The \textbf{Completeness at K (COMP@K)} metric checks if all relevant expert agents are included within the top-K selections for each query:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{COMP@K} = \frac{1}{N} \sum_{i=1}^{N} I(T_{i}^* \subseteq T_{i}^K) \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(I(T_{i}^* \subseteq T_{i}^K)\) equals 1 if the relevant expert agents are included in \(T_{i}^K\) and 0 otherwise. The \textbf{Normalized Discounted Cumulative Gain at K (NDCG@K)} evaluates the effectiveness of expert agent rankings by considering both the relevance of the expert agents and their positions within the ranking. NDCG@K is calculated as follows:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{NDCG@K} = \frac{1}{N} \sum_{i=1}^{N} \frac{\text{DCG@K}_i}{\text{IDCG@K}_i} \nonumber
\end{equation}
\end{minipage}
}

NDCG@K is computed using two key components: Discounted Cumulative Gain (DCG@K) and Ideal Discounted Cumulative Gain (IDCG@K). DCG@K measures the usefulness of expert agents based on their position in the ranked list and is calculated as:

\vspace{-1mm}
\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{DCG@K} = \sum_{j=1}^{K} \frac{2^{g_{i,j}} - 1}{\log_2(j + 1)} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(g_{i,j}\) is the relevance score of the expert agent at position \(j\) for query \(i\). It assigns higher scores to expert agents that are ranked higher while also considering their relevance. IDCG represents the maximum possible DCG for a set of expert agents, computed using the ideal ranking based on their relevance. IDCG@K is calculated as:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{IDCG@K} = \sum_{j=1}^{K} \frac{2^{g_{i,j}^{*}} - 1}{\log_2(j + 1)} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(g_{i,j}^{*}\) are the relevance scores of the expert agents in the ideal ranking at position \(j\) for query \(i\). Relevance scores \(g_{i,j}\) and \(g_{i,j}^*\) are derived from ground-truth data obtained through human annotations or external evaluation mechanisms. In short, NDCG@K combines the relevance of expert agents with their ranking positions, providing a comprehensive measure of how effectively the expert agents meet the needs of the queries across \(N\) queries. (c) The \textbf{agent invocation} metrics measure the framework's ability to identify and execute expert agent calls with the correct parameters. \textbf{Parameter Consistency (PC)} evaluates the accuracy of parameter extraction across $N$ queries. It is defined as:

\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{PC} = \frac{1}{N} \sum_{i=1}^{N} \frac{|P_i \cap P_i^R|}{|P_i^R|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where $P_i^R$ is the ideal parameter set for query $i$, and $P_i$ represents the extracted parameters for that query. The \textbf{Error Rate (ER)} evaluates the errors in parameter formatting. It is expressed as:

\vspace{-1mm}
\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{ER} = \frac{1}{N} \sum_{i=1}^{N} \frac{|P_i^E|}{|P_i|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where $P_i^E$ denotes the erroneous parameters extracted for query $i$. (d) The \textbf{Response Generation} metrics evaluate the quality of generated responses through linguistic and semantic similarity measures across \(N\) queries. The \textbf{BLEU (Bilingual Evaluation Understudy)} measures n-gram precision:

\vspace{-1mm}
\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{BLEU} = \frac{1}{N} \sum_{i=1}^{N} \left( \text{BP}_i \cdot \exp\left( \sum_{n=1}^{K} w_n \log p_{i,n} \right) \right) \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(w_n\) represents the n-gram weight, \(p_{i,n}\) is the n-gram precision for query \(i\), and \(\text{BP}_i\) denotes the brevity penalty for that query. \(K\) represents the maximum n-gram length considered. The \textbf{ROUGE-L} calculates the longest common subsequence between the generated and reference texts for each query:

\vspace{-1mm}
\resizebox{0.985\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{ROUGE-L} = \frac{1}{N} \sum_{i=1}^{N} \frac{LCS(X_i, Y_i)}{|Y_i|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(LCS(X_i, Y_i)\) denotes the longest common subsequence between the generated response \(X_i\) and the reference text \(Y_i\), and \(|Y_i|\) is the length of the reference text for query \(i\). In short, these evaluation metrics collectively demonstrate the expert agent learning framework’s effectiveness in generating relevant, accurate, and high-quality responses for patent-related tasks.

\vspace{-2mm}
\section{User-Centric Evaluation} 
\vspace{0mm}
User-centric evaluation metrics, such as User Satisfaction Score (USS), Perceived Usefulness (PU), and Net Promoter Score (NPS), provide a comprehensive analysis of the proposed multi-agent framework's performance. \textbf{USS} measures emotional satisfaction during interactions, calculated as \( USS = \frac{1}{N} \sum_{i=1}^{N} s_i \), where \( s_i \) represents satisfaction scores on a Likert scale from 1 (very unsatisfied) to 5 (very satisfied). \textbf{PU} evaluates the framework's practical utility, defined as \( PU = \frac{1}{N} \sum_{i=1}^{N} u_i \), where \( u_i \) reflects perceived usefulness on a Likert scale from 1 (not useful) to 5 (extremely useful). The key distinction between USS and PU lies in their focus: USS captures emotional satisfaction, while PU emphasizes task-related effectiveness. \textbf{NPS}, which measures user loyalty and advocacy, is calculated as \( NPS = \%P - \%D \), where \( \%P \) denotes the percentage of promoters (rating 7–10) and \( \%D \) the percentage of detractors (rating 0–4). Passives, who rate the framework 5–6, are excluded from the calculation. While not a traditional Likert scale, NPS uses a numerical rating system to categorize responses and provide insights into long-term user engagement. Together, these evaluation metrics provide a holistic view of user experience, enabling an evaluation of the framework's effectiveness and identification of areas for improvement.


\clearpage
\newpage

\section{Patent Hypothesis Exploration Graphs(PHEG)}
Patent ideation involves generating and testing original ideas or hypotheses to develop inventions that are novel, useful, and non-obvious, thereby making them eligible for patent protection. Similar to the scientific method, it involves formulating and testing hypotheses grounded in empirical evidence and rational analysis. However, patent ideation prioritizes creating practical, marketable solutions over advancing theoretical knowledge. Unlike scientific research, which often emphasizes strict hypothesis testing based on a methodical scientific process, patent ideation involves a cycle of building, testing, and refining prototypes by prioritizing commercialization and legal protection of innovations. A critical component is the generation of novel hypotheses—systematically formulated conceptual statements explaining how an invention solves specific problems or improves existing technologies. This approach fosters innovative solutions to technical challenges, emphasizing the invention's problem-solving capabilities. The process of constructing exploration graphs begins by extracting hypotheses from existing patents. This involves identifying novel ideas, scientific conjectures, or assumptions within patent documents, which serve as foundations for further innovation or experimentation. The focus is on uncovering implicit or explicit hypotheses, often embedded in technical summaries, background sections, or detailed descriptions, with claims guiding the scope of innovation. To systematically analyze and extract these hypotheses, we employ Subject-Action-Object (SAO) triplet analysis—a powerful method for examining existing patents and facilitating the discovery and refinement of hypotheses for innovation. These triplets populate a knowledge graph, identifying and linking related innovations across patents. SAO triplet analysis breaks down patent descriptions into three core components: the subject (the core invention or technology), the action (the method or process applied), and the object (the outcome or product resulting from the action). This structured breakdown allows inventors to explore relationships within technical processes, identify gaps in prior art, uncover unexplored combinations, and propose new applications or improvements for existing technologies. Its structured nature provides a strong foundation for generating innovative hypotheses and driving technological progress. In our work, the SAO method facilitates novel hypothesis generation by enabling the identification of patterns, gaps, and potential combinations within the exploration graphs derived from existing patent literature data. We apply this method, using advanced LLMs such as GPT-4, to analyze patents and infer cause-effect relationships and assumptions from patent documents as SAO triplets. These triplets, encapsulating each patent's innovations, aid in discovering novel problem-solution patterns and form the basis of our ontological knowledge graph—the Patent Hypothesis Exploration Graph (PHEG). This property graph, representing subjects and objects as nodes and actions as edges, facilitates patent analysis, identifies technological gaps and opportunities, and serves as a dynamic foundation for generating and testing innovative hypotheses, thereby advancing technological progress.  Table \ref{tab:universal_transformer_sao} presents a concise, structured overview of the Universal Transformer system patent's technical claims \cite{US10740433B2}, using a SAO triplet framework and a synthesized hypothesis.


\vspace{2mm}
\begin{table}[ht!]
\vspace{-2mm}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, title=Universal Transformer System]
\footnotesize
\vspace{-1mm}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, title=Subject]
Universal Transformer system
\end{tcolorbox}
\vspace{-1mm}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, title=Action]
The system iteratively refines sequence representations using self-attention and adaptive computation time (ACT), dynamically adjusting processing steps based on input complexity.
\end{tcolorbox}
\vspace{-1mm}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, title=Object]
Improved generalization and computational efficiency for sequence-to-sequence tasks (machine translation, language modeling, program evaluation, and algorithmic processes).
\end{tcolorbox}
\vspace{-1mm}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=black, colbacktitle=white, title=Hypothesis (Claim)]
Using recurrent depth transformations with self-attention and dynamic halting (ACT), the Universal Transformer improves efficiency and performance over standard Transformers and LSTMs for complex sequence processing.
\end{tcolorbox}
\vspace{-3mm}
\end{tcolorbox}
\vspace{-3mm}
\caption{SAO Triplet and Hypothesis for the Universal Transformer System \cite{US10740433B2}.}
\label{tab:universal_transformer_sao}
\vspace{-3mm}
\end{table}

\vspace{0mm}
Table \ref{tab:universal_transformer_sao} interprets the patent's core functionality as an SAO triplet and presents a functional hypothesis—a statement of the system's expected outcome. This hypothesis, based on the described technology, is suitable for direct computational experimentation to validate the proposed technology. Figures \ref{fig:Code1}–\ref{fig:Code2} demonstrate an iterative process for extracting hypotheses from patent documents. This process guides a language model through structured reasoning, using an SAO breakdown and self-reflection at each step to refine the output and improve the hypothesis. The generation of exploration graphs (PHEGs) involves several computationally significant steps.  LLM-based SAO triplet extraction is a major factor, with complexity at least $O(N \times L)$ (where $N$ is the number of documents and $L$ is their length), although the actual cost depends on the specific LLM and its implementation. Hypothesis refinement adds complexity, at least $O(N \times I \times H)$ (where $I$ is the number of iterations and $H$ is the hypothesis length), and this is again highly dependent on the refinement algorithm.  Graph construction from the $T$ generated triplets has $O(T \times \log V)$ complexity (where $V$ is the number of vertices), assuming an efficient graph structure with logarithmic-time insertion. Finally, graph traversal to uncover novel hypotheses introduces further complexity, depending on the chosen algorithm and the graph's structure; for example, depth-first search has a complexity of $O(V+E)$, where $E$ is the number of edges. In the next section, we will demonstrate how the Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm utilizes graph neural networks(GNNs) and reinforcement learning to efficiently traverse exploration graphs, optimizing exploration and exploitation to discover novel hypotheses.

\vspace{-1mm}
\section{Adaptive Pathfinding for Hypothesis Discovery (APHD)} 
The APHD algorithm, using a robust mathematical framework, facilitates hypothesis discovery within Patent Hypothesis Exploration Graphs (PHEGs). It leverages GNNs and reinforcement learning to efficiently navigate complex exploration graphs and identify novel hypotheses. APHD employs Proximal Policy Optimization (PPO) within an actor-critic framework. The actor network balances exploration and exploitation to discover new hypotheses. The actor chooses a node to visit (the action), and the critic estimates the value of that choice in terms of its potential to lead to valuable hypotheses. The critic's value estimations guide the actor's learning process. PPO optimizes the policy by maximizing cumulative rewards using a clipped objective function. Furthermore, PPO incorporates entropy regularization to promote diverse actions and applies redundancy penalties to prevent overlapping paths. This integrated approach ensures the efficient and reliable discovery of innovative hypotheses in complex exploration graphs. Formally, consider an exploration graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}) \), where \( \mathcal{V} \) denotes the set of nodes and \( \mathcal{E} \) denotes the set of edges. Each node \( i \in \mathcal{V} \) is initially represented by a feature vector \( \mathbf{h}_i^{(0)} \), and each edge \( (i, j) \in \mathcal{E} \) is represented by a feature vector \( \mathbf{e}_{ij} \).  The exploration graph is constructed from SAO triplets (Subject-Action-Object). Each node represents either the subject or object of a triplet, and each edge represents the action(relationship) connecting the subject and object within the same triplet. At layer $l=0$, $\mathbf{h}_i^{(0)}$ and $\mathbf{e}_{ij}$ are obtained from a sentence-embedding model. The graph neural network (GNN) operates through \( L \) layers, where each layer \( l \) updates the node embeddings based on the embeddings of neighboring nodes from the previous layer and the edge features. Specifically, the embedding of node \( i \) at layer \( l \) is computed as:

\resizebox{0.835\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\mathbf{h}_i^{(l)} = \sigma \bigg( \mathbf{W}^{(l)} \cdot \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(l)} \left( \mathbf{h}_j^{(l-1)} + \mathbf{W}_e^{(l)} \cdot \mathbf{e}_{ij} \right) \bigg)
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( \mathbf{W}^{(l)} \) is the learnable weight matrix for node embeddings at layer \( l \), and \( \mathbf{W}_e^{(l)} \) is the learnable weight matrix for edge features at layer \( l \). The attention coefficients \( \alpha_{ij}^{(l)} \) determine the importance of neighbor \( j \) to node \( i \) at layer \( l \), and \( \sigma \) is a non-linear activation function, such as ReLU. The coefficients \( \alpha_{ij}^{(l)} \) are computed based on both node and edge features as follows:

\resizebox{0.80\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\hspace{-5mm}\begin{aligned}
    \alpha_{ij}^{(l)} = \frac{\exp\left( \text{LReLU}\left( \mathbf{a}^{(l)^\top} 
    [\mathbf{W}^{(l)} \mathbf{h}_i^{(l-1)} \parallel \mathbf{W}^{(l)} \mathbf{h}_j^{(l-1)} 
    \parallel \mathbf{W}_e^{(l)} \mathbf{e}_{ij}] \right) \right)}{ \sum_{k \in \mathcal{N}(i)} 
    \exp\left( \text{LReLU}\left( \mathbf{a}^{(l)^\top} [\mathbf{W}^{(l)} 
    \mathbf{h}_i^{(l-1)} \parallel \mathbf{W}^{(l)} \mathbf{h}_k^{(l-1)} \parallel 
    \mathbf{W}_e^{(l)} \mathbf{e}_{ik}] \right) \right)}
\end{aligned}  
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( \mathbf{a}^{(l)} \) is a learnable attention vector for layer \( l \), and \( \parallel \) denotes concatenation. The attention-based aggregation of both node and edge features allows generating highly expressive and context-aware node embeddings \( \mathbf{h}_i^{(L)} \) after \( L \) layers by effectively weighting the influence of each neighbor based on both node and edge relevance. \( \text{LReLU} \) refers to the Leaky ReLU activation function. The APHD algorithm formulates hypothesis discovery as a Markov Decision Process (MDP), where a reinforcement learning (RL) agent navigates the graph by learning an optimal policy that balances exploration and exploitation to efficiently discover novel hypotheses. Effective graph traversal in the context of hypothesis discovery requires capturing both the neighborhood context and traversal history. To accomplish this, we first define the broader neighborhood context from nodes within \( k \)-hops of the current node \( u \):

\vspace{1mm}
\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{MultiHopContext}(u) = \sum_{j \in \mathcal{N}_k(u)} \alpha_{uj}^{(L)} \mathbf{h}_j^{(L)} \nonumber
\end{equation}
\end{minipage}
}

Here, \( \mathcal{N}_k(u) \) denotes the set of nodes within \( k \)-hops of node \( u \), \( \alpha_{uj}^{(L)} \) represents the attention coefficients for these neighboring nodes, and \( \mathbf{h}_j^{(L)} \) is the embedding of node \( j \) at layer \( L \). In addition to neighborhood context, incorporating traversal history is crucial to avoid cycles and promote efficient exploration. We define the traversal history of the path taken up to the current node \( u \) as:

\vspace{1mm}
\resizebox{0.955\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{PathHistory}(u) = \sum_{j \in \text{Path}(u)} \gamma_j \mathbf{h}_j^{(L)} \nonumber
\end{equation}
\end{minipage}
}

where \( \text{Path}(u) \) represents the sequence of nodes visited from the start node to the current node \( u \), and \( \gamma_j \) are weighting factors that prioritize certain nodes in the path history. The differentiable policy \( \pi(a|s) \) in the APHD algorithm determines the probability of selecting an action \( a \) (i.e., choosing the next node to traverse) based on the current state \( s \), which encapsulates the information of node \( u \), its neighboring nodes, and the traversal history up to that point. To enhance the policy's effectiveness in discovering novel hypotheses, the state \( s \) is defined through an attention-based fusion mechanism that integrates the current node's embedding, the multi-hop neighborhood context, and the path history as follows:

\vspace{1mm}
\resizebox{0.865\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\mathbf{s} = \beta_1 \mathbf{h}_{u}^{(L)} + \beta_2 \bigg( \sum_{j \in \mathcal{N}_k(u)} \alpha_{uj}^{(L)} \mathbf{h}_j^{(L)} \bigg) + \beta_3 \bigg( \sum_{j \in \text{Path}(u)} \gamma_j \mathbf{h}_j^{(L)} \bigg) \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
The attention weights \( \beta_i \) control the importance of each component, ensuring that the policy remains contextually relevant and focused on the target node while exploring the graph. These weights are learned dynamically during the training process, allowing the model to adapt to the specific context in which the policy operates. Integrating additional relevant information into the state representation through attention-based fusion enables informed, goal-directed navigation of the exploration graph. This facilitates the discovery of novel, non-obvious hypotheses, enhancing the effectiveness of hypothesis discovery. Note: In our work, graph traversal is viewed as a sequence of state-action pairs. The policy network function \( f_\theta(s) \) is defined as follows:

\resizebox{1\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\pi(a|s; \theta) = \text{Softmax}\big( W^{(2)} \big( W^{(1)} s + \mathbf{b}^{(1)} \big) + \mathbf{b}^{(2)} \big) \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( W^{(1)} \) and \( W^{(2)} \) are weight matrices, and \( \mathbf{b}^{(1)} \) and \( \mathbf{b}^{(2)} \) are bias vectors. The softmax function converts the output logits into probabilities for each possible action \( a \). The reward function is designed to balance the objectives of efficiently reaching the target and exploring novel connections:

\vspace{0mm}
\resizebox{0.945\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
r(s, a) = \gamma_{dist} \cdot \text{d}(u, T) - \lambda \cdot \text{len}(P) + \eta \cdot \text{novelty}(u)  \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
The reward function is crafted to balance three primary objectives for an agent navigating a graph: efficiently reaching the target node, minimizing the path length, and encouraging the exploration of less-visited nodes. The term \( \text{d}(u, T) = -\text{dist}(u, T) \) represents the negative shortest path distance from the current node \( u \) to the target node \( T \). The negative sign ensures that as the agent approaches \( T \) (i.e., as \( \text{dist}(u, T) \) decreases), the reward increases, motivating the agent to move closer to the target. Dijkstra's algorithm is used to compute all-pairs shortest paths, so \( \text{dist}(u, T) \) is pre-computed. The novelty function,  \( \text{novelty}(u) = \frac{1}{1 + c(u)} \) quantifies how unexplored a node is, where \( c(u) \) counts the number of times node \( u \) has been visited. Thus, nodes visited less frequently yield higher novelty scores and greater rewards, promoting exploration. \( \text{len}(P) \) represents the total number of steps or nodes in the path, indicating how far the agent has traveled within the graph. The term \( \lambda \cdot \text{len}(P) \) serves as a penalty for longer paths, encouraging the agent to find shorter, more efficient routes. While the agent is encouraged to explore less-visited nodes (via the novelty term), the path length penalty helps strike a balance between exploration and efficient target acquisition. The overall reward function is expressed as follows:

\vspace{-1mm}
\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
r(s, a) = -\gamma_{dist} \cdot \text{dist}(u, T) - \lambda \cdot \text{len}(P) + \eta \cdot \left( \frac{1}{1 + c(u)} \right) \nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( \gamma_{dist} \), \( \lambda \), and \( \eta \) are hyperparameters that adjust the importance of reaching the target quickly, penalizing longer paths, and exploring novel nodes, respectively. Tuning these parameters allows for an effective balance between exploration and exploitation, guiding the agent to not only reach the target efficiently but also discover new and potentially valuable paths within the graph. To enhance the diversity and effectiveness of path exploration within the exploration graph, we incorporate an entropy regularization term \( H(\pi(\cdot|s)) \) into the loss function. It measures the randomness or unpredictability of the policy \( \pi(a|s) \) and is calculated as follows:

\vspace{1mm}
\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
H(\pi(\cdot|s)) = -\sum_{a} \pi(a|s) \log \pi(a|s) 
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
By promoting higher entropy, the policy becomes less deterministic, encouraging the agent to explore a wider range of actions. The influence of this term is controlled by a hyperparameter \( \beta \), which balances exploration with the need to prevent excessive randomness. Additionally, a redundancy penalty \( \mathcal{P}(P) \) is applied to discourage the agent from traversing paths \( P \) that significantly overlap with previously discovered paths \( \mathcal{P} \). This penalty is calculated as:

\vspace{1mm}
\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\mathcal{P}(P) = \sum_{p \in \mathcal{P}} \text{Overlap}(P, p) 
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( P \) represents the complete path the agent is currently exploring, \( p \) refers to previously discovered paths in \( \mathcal{P} \), and \( \text{Overlap}(P, p) \) quantifies the degree of similarity between the current path \( P \) and each prior path \( p \), based solely on shared nodes. Specifically, the node-based overlap is computed as:

\vspace{-1mm}
\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{Overlap}(P, p) = \frac{|V_P \cap V_p|}{|V_P \cup V_p|}  
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( V_P \) and \( V_p \) are the sets of nodes in paths \( P \) and \( p \), respectively. By incorporating this redundancy penalty, the agent is incentivized to discover novel paths with minimal overlap with previous explorations. The policy \( \pi \) is then iteratively updated by maximizing the expected cumulative reward—augmented with the entropy regularization term and reduced by the redundancy penalty—through gradient ascent, formulated as:

\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\max_\pi \mathbb{E} \left[ \sum_{s, a} r(s, a) \right] + \beta H(\pi(\cdot|s)) - \delta_{\mathcal{P}} \mathcal{P}(P) 
\nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( \delta_{\mathcal{P}} \) is another hyperparameter controlling the weight of the redundancy penalty. This optimization achieves a balance between maximizing rewards, encouraging exploration, and promoting the discovery of novel, non-overlapping paths in the graph. In short, by incorporating the entropy regularization term and the redundancy penalty directly into the PPO loss function, the algorithm ensures that policy updates remain within a trust region while promoting exploration and discouraging redundant path traversal. The entropy term  encourages the policy to remain stochastic, avoiding premature convergence to suboptimal deterministic policies, while the redundancy penalty reduces the likelihood of revisiting paths that significantly overlap with those already explored. In the APHD algorithm, the five terms—\( \beta H(\pi(\cdot|s)) \), \( -\delta_{\mathcal{P}} \mathcal{P}(P) \), \( -\lambda \cdot \text{len}(P) \), \( \eta \cdot \text{novelty}(u) \), and \( -\gamma_{dist} \cdot \text{dist}(u, T) \)—serve complementary purposes in guiding the agent's navigation strategy within the Patent Hypothesis Exploration Graph (PHEG). The entropy term \( \beta H(\pi(\cdot|s)) \) encourages exploration by increasing randomness in action selection, thereby preventing convergence to suboptimal paths. The redundancy penalty \( -\delta_{\mathcal{P}} \mathcal{P}(P) \) discourages revisiting paths that overlap with previously explored routes, promoting novel pathways. The path length penalty \( -\lambda \cdot \text{len}(P) \) fosters efficiency by penalizing longer paths, while the novelty reward \( \eta \cdot \text{novelty}(u) \) incentivizes the agent to visit less-explored nodes. Additionally, the term \( -\gamma_{dist} \cdot \text{dist}(u, T) \) motivates the agent to move closer to the target node by rewarding shorter distances, helping to align the traversal with the target efficiently. Together, these terms balance exploration and exploitation by influencing policy randomness, path uniqueness, efficiency, target alignment, and node novelty, thereby enabling the effective discovery of novel hypotheses within complex graph structures. This comprehensive mathematical framework represents a significant advancement over traditional heuristic pathfinding methods. It enables APHD to navigate complex and exploration graphs more effectively, making it a robust and powerful tool for accelerating innovation and hypothesis generation in the realm of patent discovery. The policy network in the APHD algorithm is trained using Proximal Policy Optimization (PPO) technique. The training objective is designed to maximize the expected cumulative reward while incorporating both an entropy regularization term and a redundancy penalty to encourage exploration and avoid redundant paths. The objective function is formulated to maximize the expected cumulative discounted reward, minus the redundancy penalty, plus the entropy regularization term:

\resizebox{0.935\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\max_\theta \ \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T_{s}} \gamma^{t}_{\text{disc}} \left( r(s_t, a_t) - \delta_{\mathcal{P}} \mathcal{P}(P_t) \right) \right] + \beta \ \mathbb{E}_{\pi_\theta} \left[ H(\pi_\theta(\cdot|s_t)) \right] 
\nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( \pi_\theta \) is the policy network parameterized by \( \theta \) that outputs the probability distribution over actions given a state. \( \mathbb{E}_{\pi_\theta} \) denotes the expected value over trajectories generated by following the policy \( \pi_\theta \), \( T_{s} \) denotes the maximum number of steps (i.e., actions or transitions) that the agent is allowed to take within a traversal episode, and \( \gamma_{\text{disc}} \in [0, 1) \) is the discount factor balancing immediate and future rewards. The term \( r(s_t, a_t) \) represents the reward received after taking action \( a_t \) in state \( s_t \). The redundancy penalty \( \mathcal{P}(P_t) \) is calculated as \( \mathcal{P}(P_t) = \sum_{p \in \mathcal{P}} \text{Overlap}(P_t, p) \). Here, \( \mathcal{P} \) denotes the set of previously discovered paths, and \( \text{Overlap}(P_t, p) \) quantifies the similarity between the current path \( P_t \) and a prior path \( p \), based on the shared nodes \( V_{P_t} \) and \( V_p \). The entropy regularization term \( H(\pi_\theta(\cdot|s_t)) \) measures the randomness or unpredictability of the policy, promoting exploration by encouraging the agent to consider a wider range of actions. Using PPO, the policy is updated by maximizing a clipped surrogate objective to ensure stable and efficient learning. The loss function is defined as:

\vspace{1mm}
\resizebox{0.965\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{align*}
\mathcal{L}(\theta) = & \ \mathbb{E}_{t} \left[ \min \left( r_t(\theta) \hat{A}_t, \ \text{clip}\left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right] \\
& - \beta \ \mathbb{E}_{t} \left[ H(\pi_\theta(\cdot|s_t)) \right] - \delta_{\mathcal{P}} \ \mathbb{E}_{t} \left[ \mathcal{P}(P_t) \right]
\end{align*}
\end{minipage}
}

\vspace{2mm}
where \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \) is the probability ratio comparing the likelihood of taking action \( a_t \) under the current policy \( \pi_\theta \) to that under the old policy \( \pi_{\theta_{\text{old}}} \). The term \( \hat{A}_t = Q(s_t, a_t) - V(s_t) \) is the estimated advantage function, indicating how much better action \( a_t \) is compared to the average action in state \( s_t \). Here, \( Q(s_t, a_t) \) is the action-value function, estimating the expected return starting from state \( s_t \) and taking action \( a_t \), while \( V(s_t) \) is the state-value function, estimating the expected return starting from state \( s_t \). The term \( \text{clip}( r_t(\theta), 1 - \epsilon, 1 + \epsilon ) \) constrains the probability ratio within a range to prevent large policy updates (with \( \epsilon \) being a small positive hyperparameter, e.g., 0.05). \( \mathbb{E}_{t} \) denotes the expected value over steps. This comprehensive formulation enhances the RL agent's ability to discover novel and effective paths within the exploration graph, balancing exploration and exploitation. The APHD algorithm, effectively navigating complex, large-scale exploration graphs, becomes a robust and powerful tool for accelerating innovation and generating novel hypotheses in patent discovery. 

\begin{algorithm}[H]
\caption{Adaptive Pathfinding for Hypothesis Discovery (APHD)}
\label{alg:APHD_corrected}
\begin{algorithmic}[1]
    \STATE \textbf{Initialize} exploration graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}) \) with nodes \( \mathcal{V} \) and edges \( \mathcal{E} \)
    \STATE \textbf{Initialize} policy network \( \pi_\theta \) with weights \( \theta \)
    \STATE \textbf{Initialize} value network \( V_\phi \) with weights \( \phi \)
    \STATE \textbf{Initialize} Graph Neural Network (GNN) parameters with weights \( \theta_{GNN} \)
    \STATE \textbf{Initialize} hyperparameters \( \lambda, \eta, \gamma_{\text{dist}}, \beta, \delta_{\mathcal{P}}, \epsilon, \gamma, \lambda_{GAE} \)
    \STATE \textbf{Initialize} set of discovered paths \( \mathcal{P} \gets \emptyset \)
    \STATE \textbf{Initialize} set of discovered hypotheses \( \mathcal{H} \gets \emptyset \)
    
    \FOR{iteration \( k = 1 \) to \( K \)}
        \STATE \textbf{Initialize} empty set of trajectories \( \mathcal{D}_k \)
        \FOR{episode \( e = 1 \) to \( M \)}
            \STATE \textbf{Select} start node \( S \in \mathcal{V} \) and target node \( T \in \mathcal{V} \)
            \STATE \textbf{Compute} GNN embeddings \( \mathbf{h}_u^{(L)} \) for all \( u \in \mathcal{V} \) using \( \theta_{GNN} \)
            \STATE \textbf{Initialize} current node \( u \gets S \)
            \STATE \textbf{Initialize} path \( P \gets [S] \)
            \STATE \textbf{Initialize} path history \( \text{Path}(u) = \{u\} \)
            \FOR{step \( t = 1 \) to \( T \)}
                \STATE \textbf{Compute} state \( s_t \) 
                \STATE \textbf{Sample} action \( a_t \) from policy:
                \[
                a_t \sim \pi_\theta(a_t \mid s_t)
                \]
                \STATE \textbf{Execute} action \( a_t \), traverse to node \( v \)
                \STATE \textbf{Calculate} reward \( r_t \)
                \STATE \textbf{Update} path \( P \gets P \cup \{v\} \)
                \STATE \textbf{Update} path history \( \text{Path}(v) = \text{Path}(u) \cup \{v\} \)
                \STATE \textbf{Set} next state \( s_{t+1} = \mathbf{s}(v) \)
                \STATE \textbf{Store} transition \( (s_t, a_t, r_t, s_{t+1}) \) in trajectory \( \tau_e \)
                \IF{\( v = T \) or maximum steps reached}
                    \STATE \textbf{Extract} hypothesis from path \( P \)
                    \STATE \textbf{Add} hypothesis to \( \mathcal{H} \)
                    \STATE \textbf{Add} path \( P \) to \( \mathcal{P} \)
                    \STATE \textbf{Break}
                \ENDIF
                \STATE \textbf{Update} \( u \gets v \)
            \ENDFOR
            \STATE \textbf{Add} trajectory \( \tau_e \) to \( \mathcal{D}_k \)
        \ENDFOR
        \STATE \textbf{Compute} advantage estimates \( \hat{A}_t \) using trajectories in \( \mathcal{D}_k \)
        \STATE \textbf{Compute} returns \( R_t \) for each time step in trajectories
        \FOR{each trajectory \( \tau_e \) in \( \mathcal{D}_k \)}
            \FOR{each step \( t \) in \( \tau_e \)}
                \STATE Compute \( R_t \) using Monte Carlo Returns:
                \vspace{-3mm} 
                {\small
                \[
                R_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k}
                \]}
            \vspace{-5mm}    
            \ENDFOR
        \ENDFOR
        \STATE \textbf{Update} policy network \( \pi_\theta \) by maximizing the PPO objective:
        \vspace{-3mm}
{\small
        \begin{align*}
        \mathcal{L}(\theta) &= \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \ \text{clip}\left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right] \\
        &\quad - \beta \mathbb{E}_t \left[ H(\pi_\theta(\cdot \mid s_t)) \right] - \delta_{\mathcal{P}} \mathbb{E}_t \left[ \mathcal{P}(P_t) \right]
        \end{align*}
}\vspace{-3mm}
        \STATE \textbf{Update} value network \( V_\phi \) by minimizing:
        \vspace{-3mm}
{\small
        \[
        \mathcal{L}_V(\phi) = \mathbb{E}_t \left[ \left( V_\phi(s_t) - R_t \right)^2 \right] 
        \]
}  
        \vspace{-3mm}
        \vspace{-3mm} 
    \ENDFOR  
\end{algorithmic}
\end{algorithm}

operates on a patent hypothesis exploration graph (PHEG). This PHEG is a knowledge graph composed of nodes and edges representing patent-related entities and their relationships. Specifically, the PHEG is constructed from Subject-Action-Object (SAO) triples extracted from patent documents. Each triple represents a relationship where the Subject is the entity performing the action, the Action describes the process or method, and the Object is the entity being acted upon or the target of the action. The APHD algorithm employs a policy network ($\pi_\theta$) and a value network ($V_\phi$) to navigate this graph, enabling the discovery of novel hypotheses. In each iteration, the agent interacts with the PHEG to collect trajectories. Actions are selected based on the policy network, and rewards are assigned based on proximity to the target hypothesis node. These trajectories are used to compute advantage estimates and returns, guiding updates to both the policy and value networks through the PPO technique. This approach systematically navigates the complex patent landscape, facilitating effective hypothesis generation and promoting innovation.

\vspace{0mm} 
\section{Graph-Theoretic Foundations of APHD for Hypothesis Discovery}
\vspace{0mm}  
The Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm operates on Patent Hypothesis Exploration Graphs (PHEGs), structured using Subject-Action-Object (SAO) triplets extracted from patent documents.  SAO triplet analysis decomposes patent descriptions into three key components: the subject (core invention or technology), the action (method or process applied), and the object (outcome or result). This approach facilitates hypothesis generation by identifying and linking related innovations, uncovering implicit or explicit relationships embedded in technical summaries or claims.  In a PHEG, nodes represent the subject and object from SAO triplets, while edges encode the actions connecting them, forming a property graph. APHD leverages several graph-theoretic principles to efficiently navigate these graphs and generate novel hypotheses. It utilizes implicit betweenness centrality, prioritizing nodes acting as bridges between separate graph regions, and explicitly considers transitive relationships using multi-hop path aggregation, where the $k$-hop context of a node $u$ is represented as \( \text{MultiHopContext}(u) = \sum_{j \in \mathcal{N}_k(u)} \alpha_{uj}^{(L)} \mathbf{h}_j^{(L)} \), with \( \alpha_{uj}^{(L)} \) as attention coefficients. The algorithm also exploits the scale-free structure of PHEGs, where high-degree hubs represent widely cited patents.  The novelty reward \( \text{novelty}(u) = \frac{1}{1 + c(u)} \) (where $c(u)$ is the visit count) discourages over-exploration of hubs, promoting visits to less-explored nodes. Dense clusters (high clustering coefficient) facilitate incremental innovation, while sparse regions are targeted for disruptive discoveries.  Entropy regularization, \( H(\pi(\cdot|s)) = -\sum_{a} \pi(a|s) \log \pi(a|s) \), and path length penalties, \( -\lambda \cdot \text{len}(P) \), balance exploration and exploitation.  These principles ensure APHD efficiently navigates the PHEG to discover novel hypotheses, leveraging the structured knowledge of SAO triplets while balancing systematic exploration and targeted exploitation. In summary, the APHD algorithm combines graph-theoretic principles (betweenness centrality, transitive relationships, scale-free structure, clustering coefficient) with reinforcement learning techniques (entropy regularization, path length penalties) to effectively navigate and explore the PHEG. The graph-theoretic aspects define the structure and properties of the search space (the PHEG), while the reinforcement learning techniques guide the search strategy within that space. Having outlined the reinforcement learning framework of APHD algorithm, we now shift focus to the graph-theoretic principles that enable efficient navigation(traversal strategies) of Patent Hypothesis Exploration Graphs (PHEGs) for uncovering novel insights.

\vspace{0mm} 
\section{APHD Algorithm Computational Complexity}  
\vspace{0mm}  
The Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm traverses complex Patent Hypothesis Exploration Graphs (PHEGs) by integrating graph-based reasoning with reinforcement learning, and its computational complexity is influenced by several interrelated factors. The Graph Neural Network (GNN) updates node embeddings over \( L \) layers, and for sparse graphs (common in real-world scenarios), the per-layer complexity is \( O(E \times F) \), where \( E \) is the number of edges and \( F \) is the dimensionality of node embeddings, leading to a total complexity of \( O(L \times E \times F) \); for dense graphs where \( E \approx V^2 \), the complexity becomes \( O(L \times V^2 \times F) \). Constructing the state representation involves embedding the current node (\( O(F) \)), aggregating multi-hop neighbors (\( O(N_k(u) \times F) \), where \( N_k(u) \) is the number of nodes within \( k \)-hops of the current node \( u \)), and summarizing path history (\( O(P \times F) \), where \( P \) is the path length), resulting in a combined state representation complexity of \( O((N_k(u) + P + 1) \times F) \). The policy network's per-decision complexity is dominated by the forward pass through the network, which is \( O(F_p) \), where \( F_p \) is the number of parameters in the policy network; if action probabilities over \( A \) actions are considered, the complexity may be \( O(F_p + A) \). Reward computation takes \( O(1) \) time per step due to precomputed shortest path distances. Training the policy network using Proximal Policy Optimization (PPO) involves updates per episode, with per-update complexity of \( O(B \times F_p) \), where \( B \) is the batch size; over \( U \) updates per episode, the per-episode training complexity becomes \( O(U \times B \times F_p) \). Aggregating these components, the total per-episode complexity is 


\vspace{-1mm}  
\resizebox{0.925\linewidth}{!}{  
\begin{minipage}{\linewidth}  
\begin{equation}
\begin{aligned}
O\big( & L \times E \times F + T \times \big[ (N_k(u) + P + 1) \times F + F_p + A \big] \\
& + U \times B \times F_p \big)
\end{aligned} \nonumber
\end{equation}
\end{minipage}  
}  

\vspace{1mm}
where \( T \) denotes the maximum steps per episode. The dominant factors driving the complexity include the graph size (\( V \) and \( E \)), GNN depth (\( L \)), path length (\( T \)), embedding dimension (\( F \)), policy network size (\( F_p \)), and the number of actions (\( A \)). To efficiently handle moderate-scale PHEGs, optimization strategies are employed, such as efficient mini-batch training and precomputing distance matrices (e.g., precomputing shortest path distances for $O(1)$ reward computation per step). These optimizations enable the APHD algorithm to effectively balance exploration and exploitation, facilitating efficient hypothesis discovery and accelerating innovation in patent analysis. 


\vspace{1mm}
\section{Multimodal Patent(PatKG)/Scientific Knowledge Graph(SciKG)}
\label{sec:Kg_semsearch_retrieval}
\vspace{1mm}
Graph Retrieval-Augmented Generation (Graph RAG) \cite{edge2024local, hu2024grag} is an innovative framework in natural language processing that integrates graph-based data retrieval with generative models, such as Large Language Models (LLMs), to enhance the relevance and accuracy of generated text in open-domain question answering (ODQA). Graph RAG surpasses traditional retrieval methods, such as naive RAG, which often struggle with complex queries that require the integration of cross-referenced information from document databases. It improves LLM performance on ODQA tasks by retrieving relevant and contextually accurate information from structured, multimodal knowledge graph data through semantic search, enabling the generation of coherent, factual, and contextually relevant responses. In this work, we construct a Multimodal Patent Knowledge Graph(PatKG) and a Scientific Knowledge Graph (SciKG) for scholarly articles, providing a sophisticated tool for organizing, integrating, and efficiently retrieving patent and scholarly article information. The process begins by extracting structured information from unstructured documents, which is then integrated into knowledge graphs stored in graph databases such as Neo4j Aura, Amazon Neptune, or NebulaGraph. These knowledge graphs, composed of nodes and relationships with multiple key-value properties, organize domain-specific information while preserving contextual and semantic details that enable the language model to understand complex relationships and hierarchies. Semantic searches are conducted using vector-based retrieval to efficiently retrieve relevant subgraphs aligned with the input query. The retrieved subgraph information is then incorporated into language models, enriching the text generation with contextually accurate content. To build a knowledge graph, we perform document parsing to read content and extract text, tables, and images. The process starts by segmenting the text into smaller sections (chunking) and extracting triples—data structures consisting of a subject, predicate, and object, representing relationships between entities, and embedding them into vector representations. This enables efficient retrieval by capturing atomic concepts—basic entities and their relationships—within the knowledge graph. Text chunking breaks large documents into manageable sections, improving processing speed and retrieval accuracy. While smaller chunks enhance precision, they may lose context, whereas larger chunks maintain context but can reduce relevance. There is no universal optimal chunk size, as it varies with the task and dataset. A sliding window technique is used, where a fixed-size window moves over the text with a set stride, producing overlapping chunks to retain context. For a document \( D_{\text{doc}} \) containing \( N \) tokens, the document is divided into chunks (manageable segments) using a chunk length \( l \) (the number of tokens per chunk) and a stride \( s \) for further processing. The \( i \)-th chunk \( c_i \) is defined as:

\resizebox{\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
c_i = D_{\text{doc}}[(i-1) \times s : (i-1) \times s + l] \nonumber
\end{equation}
\end{minipage}
}

Here, \( D_{\text{doc}}[(i-1) \cdot s : (i-1) \cdot s + l] \) refers to the segment of the document starting at \( (i-1) \cdot s \) and spanning \( l \) tokens. \( i \) represents the index of the chunk in the sequence of chunks extracted from the document. The stride \( s \) controls the offset between consecutive chunks, while \( l \) specifies the length of each chunk. If \( s < l \), the chunks will overlap, whereas if \( s \geq l \), the chunks will not overlap. To overcome the limitations of the sliding window technique for local context retention, which often loses broader context when segmenting documents, we implement a method that retains surrounding context—typically from the paragraphs before and after each chunk \cite{anthropic_contextual_retrieval}. This ensures that chunks are understood within their broader narrative. We prompt the Gold-LLM, such as GPT-4o, to extract concise, chunk-specific context from the overall document. By adding this context, we create augmented chunks, ensuring that even smaller chunks provide accurate, contextually complete responses, thereby enhancing retrieval quality. For simplicity, we will henceforth refer to augmented text chunks simply as text chunks. Sentence embedding models convert each chunk into a dense vector \( \mathbf{v}(c_i) \in \mathbb{R}^d \), where \( d \) denotes the embedding dimension. The text segments are stored as chunk nodes in a graph database, with each node \( v_{t_i} \) corresponding to a specific text chunk \( c_i \), and \( \mathbf{v}(c_i) \) representing its vector embedding. The subscript \( t \) distinguishes these nodes as being related to text segments or chunks. These nodes are collectively represented as \( V_T = \{v_{t_1}, v_{t_2}, \ldots, v_{t_K}\} \), where \( K \) is the total number of chunks generated from the document using the sliding window technique. To improve querying and reasoning, text chunks (often containing multiple facts) are converted into atomic triples—more granular and distinct facts. Using a benchmark LLM, such as GPT-4o, triples are extracted from each text chunk. A chunk \( c_i \) can generate multiple triples, represented as \( \{ \tau_{i1}, \tau_{i2}, \ldots, \tau_{iM_i} \} \), where \( M_i \) denotes the number of triples generated from chunk \( i \). Each triple \( \tau_{ij} \) represents a relationship between entities, with \( e_{s_{ij}} \) as the subject, \( r_{ij} \) as the relation, and \( e_{o_{ij}} \) as the object. Here, \( i \) refers to the text chunk, and \( j \) identifies the specific triple within that chunk. The complete set of entity nodes from all triples is denoted as:

\vspace{-3mm}
\resizebox{\linewidth}{!}{  
\begin{minipage}{\linewidth}  
\begin{equation}  
V_E = \left\{ e_{s_{ij}}, e_{o_{ij}} \;\middle|\; \text{ for } i = 1, \dots, K \text{ and } j = 1, \dots, M_i \right\} \nonumber  
\end{equation}  
\end{minipage}  
}

\vspace{2mm} 
Each entity node is linked to a chunk node via a `MENTIONS' relationship, denoted as \( \text{Ment}(v_{t_i}, e_{s_{ij}}) \) for a subject or \( \text{Ment}(v_{t_i}, e_{o_{ij}}) \) for an object, indicating that the entity is mentioned within the chunk. Entity-to-entity triples connect basic entity nodes, enabling precise retrieval, while entity-to-text chunk triples link entities to their parent chunks, providing contextual depth for the entities.  The semantic ontology derived from these triples establishes a structured schema for the knowledge graph, clearly defining entity nodes (\( V_E \)) and their relationships (\( r_{ij}, \text{Ment}(v_{t_i}, e_{s_{ij}}), \text{Ment}(v_{t_i}, e_{o_{ij}}) \)). This schema enhances the organization and interpretation of information within the graph database, enabling more precise retrieval and analysis. Accurate knowledge triples in Subject-Predicate-Object format (see Table \ref{fig:Code2}) are generated using a Chain-of-Thought (CoT) reflection-based method. This iterative approach significantly improves knowledge graph construction from unstructured text compared to traditional methods. We perform entity deduplication to improve the efficiency of the knowledge graph, enabling more effective semantic search and retrieval. Duplicates often arise due to variations in naming conventions or differing representations of the same entity. To address this, we combine vector-based semantic similarity with string-based comparisons. The cosine similarity between the vector embeddings of subject entities \( e_{s_{ij}} \) and \( e_{s_{kl}} \), or object entities \( e_{o_{ij}} \) and \( e_{o_{kl}} \), is computed as:

\vspace{-2mm}
\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{sim}(\mathbf{v}(e_{x_{ij}}), \mathbf{v}(e_{x_{kl}})) = \frac{\mathbf{v}(e_{x_{ij}}) \cdot \mathbf{v}(e_{x_{kl}})}{\|\mathbf{v}(e_{x_{ij}})\| \|\mathbf{v}(e_{x_{kl}})\|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( e_{s_{ij}} \) represents the subject entity in the \( j \)-th position of the \( i \)-th chunk, while \( e_{o_{ij}} \) represents the object entity in the \( j \)-th position of the \( i \)-th chunk. Similarly, \( e_{s_{kl}} \) refers to the subject entity in the \( l \)-th position of the \( k \)-th chunk, and \( e_{o_{kl}} \) denotes the object entity in the \( l \)-th position of the \( k \)-th chunk. \( x \in \{s, o\} \) denotes subject (\( s \)) or object (\( o \)) entities, \( \mathbf{v}(e_{x_{ij}}) \) is the vector embedding of \( e_{x_{ij}} \), and \( \|\mathbf{v}(e_{x_{ij}})\| \) is its Euclidean norm. The Levenshtein similarity between entities is computed as:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{lev\_sim}(e_{x_{ij}}, e_{x_{kl}}) = 1 - \frac{\text{lev\_dist}(e_{x_{ij}}, e_{x_{kl}})}{\max(|e_{x_{ij}}|, |e_{x_{kl}}|)}  \nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( \text{lev\_dist} \) is the Levenshtein distance, and \( |e_{x_{ij}}| \) is the number of characters in \( e_{x_{ij}} \). The combined similarity measure is:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\begin{split}
\text{sim}_{\text{combined}}(e_{x_{ij}}, e_{x_{kl}}) &= \alpha \cdot \text{sim}(\mathbf{v}(e_{x_{ij}}), \mathbf{v}(e_{x_{kl}})) \\
&+ \beta \cdot \text{lev\_sim}(e_{x_{ij}}, e_{x_{kl}})  \nonumber
\end{split}
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( \alpha \) and \( \beta \) are hyperparameters balancing the contributions of vector-based and string-based similarity, with \( \alpha + \beta = 1 \). Entities are considered duplicates if their combined similarity exceeds a predefined threshold \( \theta_{\text{dup}} \). Once identified, duplicates are merged into a single canonical entity. This process improves the knowledge graph's coherence and retrieval accuracy, particularly in frameworks like Graph RAG, which rely on structured knowledge to enhance semantic search and response generation. The user query \( Q \) is converted into a vector embedding \( \mathbf{v}(Q) \in \mathbb{R}^d \), where \( d \) represents the embedding dimension. The top-\( U \) nodes \( \{e_{u_1}, e_{u_2}, \ldots, e_{u_U}\} \) are selected by computing the cosine similarity between \( \mathbf{v}(Q) \) and each entity node \( \mathbf{v}(e_i) \), as follows:

\vspace{-2mm}
\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\text{sim}(\mathbf{v}(Q), \mathbf{v}(e_i)) = \frac{\mathbf{v}(Q) \cdot \mathbf{v}(e_i)}{\|\mathbf{v}(Q)\| \|\mathbf{v}(e_i)\|} \nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
where \( e_i \) is the \( i \)-th node in the graph. For each selected node \( e_u \) (where \( u = 1, 2, \ldots, U \)), all one-hop neighbors \( \{e_{v_1}, e_{v_2}, \ldots, e_{v_{V_u}}\} \) are retrieved, forming triples:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\mathcal{T}_{u} = \{(e_u, r_{uv}, e_v) \mid v = 1, 2, \ldots, V_u\} \nonumber
\end{equation}
\end{minipage}
}

\vspace{2mm}
Here, \( r_{uv} \) represents the relationship between the selected node \( e_u \) and its one-hop neighbor \( e_v \). The complete set of retrieved triples across all selected nodes is:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
T_{u} = \bigcup_{u=1}^{U} \mathcal{T}_{u} \nonumber
\end{equation}
\end{minipage}
}
where \( V = \sum_{u=1}^U V_u \) is the total number of one-hop neighbors across all selected nodes. This process extracts a subgraph containing all relevant contextual information, enriching the generative model with structured and contextually accurate data for improved response generation. The framework retrieves the chunk nodes \( v_{t_k} \) associated with both \( e_u \) and \( e_v \), providing context for the triple \( (e_u, r_{uv}, e_v) \). The parent chunk nodes \( \mathcal{P}_{uv} \) are determined by combining shared chunk nodes (intersection) and all linked chunk nodes (union), while excluding duplicates:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\mathcal{P}_{uv} = (\mathcal{C}(e_u) \cap \mathcal{C}(e_v)) \cup \left( (\mathcal{C}(e_u) \cup \mathcal{C}(e_v)) \setminus (\mathcal{C}(e_u) \cap \mathcal{C}(e_v)) \right) \nonumber
\end{equation}
\end{minipage}
}

where \( \mathcal{C}(e_u) \) and \( \mathcal{C}(e_v) \) denote the sets of chunk nodes linked to \( e_u \) and \( e_v \), respectively. The retrieved relational data, $\mathcal{R}_{uv} = \{ (T_u, \mathcal{P}_{uv}) \}$, links the set of triples $T_u$ (representing relationships for entity $e_u$) with their corresponding parent chunk nodes, providing contextual information. This data is then fed into a language model to generate a human-readable and contextually relevant answer. Similarly, in the graph database for image management, images are represented as nodes, denoted by \( V_{img} = \{v_{img_1}, v_{img_2}, \dots, v_{img_I}\} \), where \( I \) refers to the total number of images stored. Each node \( v_{img_i} \) contains metadata, including details such as resolution, format, tags, URLs (linking to images stored in the local file system), and descriptions generated by large language models (LLMs) such as GPT-4o, which provide insight into the image content and scene. The images are encoded into low-dimensional vectors using OpenAI's CLIP model \cite{radford2021learning}, represented by \( \mathbf{v}(img_i) \in \mathbb{R}^d \), where \( d \) is the vector dimensionality. These vector embeddings capture the semantic meaning of the images and are stored as attributes of the image nodes, allowing for the efficient retrieval of images with similar semantic content. Each image node \( v_{img_i} \) is linked to its top-\( K_{\text{similar}} \) visually similar images using `VISUALLY\_SIMILAR' relationships, expressed as \(\text{Visually\_Similar}(v_{img_i}, v_{img_j})\), where \( v_{img_j} \) represents a similar image node. Here, \( K_{\text{similar}} \) denotes the number of similar images retrieved, facilitating the efficient retrieval of related images. The multimodal graph is composed of image nodes \( V_{img} \), which store both CLIP embeddings and metadata descriptions, and edges \( E = \left\{ \text{Visually\_Similar}(v_{img_i}, v_{img_j}) \mid v_{img_i}, v_{img_j} \in V_{img} \right\} \), representing the relationships between visually similar images. The actual image files are stored in the local file system, and only their file paths are kept within the metadata in the graph database. Our framework supports multimodal queries, enabling the retrieval of both images and relevant text based on embedding similarities, thereby enhancing the capabilities of multimodal Graph RAG. For a natural language query, we traverse the knowledge graph to retrieve relevant information (such as \(\mathcal{R}_{uv}\) and the top-\(K\) similar images in \(V_{img}\)) using vector semantic similarity searches for a natural language query, $Q$. This information is then used to prompt the language model to generate coherent, contextually accurate responses, grounding its outputs in structured data for more precise answers. In the domain of patents and scholarly articles, specifically for cross-document knowledge integration, a unified knowledge graph created from multiple sources — supported by LLM-driven ontology extraction and schema alignment — effectively organizes and standardizes the 

\begin{algorithm}[H]
\caption{Multimodal Knowledge Graph-Driven Semantic Search and Retrieval}
\label{alg:Multimodal_KG_Semantic_Search}
\begin{algorithmic}[1]
    \STATE \textbf{Input:}
    \STATE \quad Documents \( D = \{D_1, \ldots, D_N\} \) with text and images
    \STATE \quad Pre-trained models: LLM for triple extraction such as GPT-4o, sentence embedding model(text-embedding-3-small), image embedding model (e.g., CLIP)
    \STATE \quad Knowledge Graph Database \( \mathcal{G} = (V, E) \) \textbf{/* Initialize or clear existing graph */}
    \STATE \quad Hyperparameters: chunk size \( l \), stride \( s \), similarity thresholds \( \theta_{\text{dup}} \), \( \theta_{\text{sim}} \)
    
    \STATE \textbf{Output:} Multimodal Knowledge Graph \( \mathcal{G} \)
    
    \STATE \textbf{Procedure:}
    
    \FOR{each document \( D_i \) in \( D \)}
        \STATE \textbf{Text Processing:}
        \FOR{each text section in \( D_i \)}
            \STATE Split into chunks \( C = \{c_1, c_2, \ldots, c_K\} \) using sliding window with chunk size \( l \) and stride \( s \)
            \FOR{each chunk \( c_i \) in \( C \)}
                \STATE Add chunk node \( v_{t_i} \) corresponding to \( c_i \) to \( V \) 
                \STATE Embed chunk: \( \mathbf{v}(c_i) \leftarrow \text{EmbedText}(c_i) \)
                \STATE Extract triples \( \tau_{i} = \{(e_{s_{ij}}, r_{ij}, e_{o_{ij}})\} \) using LLM
                \FOR{each triple \( (e_{s_{ij}}, r_{ij}, e_{o_{ij}}) \) in \( \tau_{i} \)}
                    \STATE Add entity nodes \( e_{s_{ij}} \) and \( e_{o_{ij}} \) to \( V \)
                    \STATE Add `MENTIONS' edges: \( (v_{t_i}- \text{MENTIONS}- e_{s_{ij}}) \) and \( (v_{t_i} -\text{MENTIONS}- e_{o_{ij}}) \)
                    \STATE Add relationship edge: \( (e_{s_{ij}}) \xrightarrow{r_{ij}} (e_{o_{ij}}) \) to \( E \)
                \ENDFOR
            \ENDFOR
        \ENDFOR
        
        \STATE \textbf{Entity Deduplication:}
        \FOR{each pair \( (e_{x_{ij}}, e_{x_{kl}}) \) in \( V_E \)}
            \STATE Compute combined similarity: \( \text{sim}_{\text{combined}}(e_{x_{ij}}, e_{x_{kl}}) \)
            \IF{\( \text{sim}_{\text{combined}} \geq \theta_{\text{dup}} \)}
                \STATE Merge \( e_{x_{ij}} \) and \( e_{x_{kl}} \) into a canonical entity \( e_c \)
                \STATE Reassign all edges from \( e_{x_{ij}} \) and \( e_{x_{kl}} \) to \( e_c \)
                \STATE Remove duplicate nodes \( e_{x_{ij}} \) and \( e_{x_{kl}} \) from \( V \)
            \ENDIF
        \ENDFOR

        \STATE \textbf{Image Processing:}
        \FOR{each image \( v_{img_i} \) in \( D_i \)}
            \STATE Extract metadata for \( v_{img_i} \)
            \STATE Embed image: \( \mathbf{v}(img_i) \leftarrow \text{EmbedImage}(v_{img_i}) \)
            \STATE Add node \( v_{img_i} \) with embedding to \( V \)
            \FOR{each existing image node \( v_{img_j} \) in \( V_{img} \)}
                \IF{Cosine similarity \( \text{sim}(\mathbf{v}(img_i), \mathbf{v}(img_j)) \geq \theta_{\text{sim}} \)}
                    \STATE Add `VISUALLY\_SIMILAR' edge: \( (v_{img_i} \text{ VISUALLY\_SIMILAR } v_{img_j}) \)
                \ENDIF
            \ENDFOR
        \ENDFOR
    \ENDFOR
    
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption*{Continuation of Algorithm~\ref{alg:Multimodal_KG_Semantic_Search}: Semantic Search and Retrieval}
\label{alg:Multimodal_KG_Semantic}
\begin{algorithmic}[1]
    \STATE \textbf{Knowledge Graph Storage:}
    \STATE Store the knowledge graph \( \mathcal{G} = (V, E) \) in the graph database with the following components:
    \STATE \quad - \textbf{Text Chunk Nodes} \( V_T = \{v_{t_1}, v_{t_2}, \ldots, v_{t_K}\} \) representing text chunks with embeddings \( \mathbf{v}(c_i) \)
    \STATE \quad - \textbf{Entity Nodes} \( V_E = \{e_{s_{ij}}, e_{o_{ij}}\} \) linked to chunks via `MENTIONS' relationships
    \STATE \quad - \textbf{Image Nodes} \( V_{img} = \{v_{img_1}, v_{img_2}, \ldots, v_{img_I}\} \) with CLIP embeddings \( \mathbf{v}(img_i) \)
    \STATE \quad - \textbf{Edges} \( E \) representing:
    \STATE \quad \quad - `MENTIONS' relationships: \( \text{Ment}(v_{t_i}, e_{s_{ij}}) \) and \( \text{Ment}(v_{t_i}, e_{o_{ij}}) \)
    \STATE \quad \quad - Entity-to-entity relationships \( (e_{s_{ij}}) \xrightarrow{r_{ij}} (e_{o_{ij}}) \)
    \STATE \quad \quad - `VISUALLY\_SIMILAR' relationships: \( \text{Visually\_Similar}(v_{img_i}, v_{img_j}) \)
    
    \STATE \textbf{Semantic Search and Retrieval:}
    \FOR{each query \( Q \)}
        \STATE Embed the query: \( \mathbf{v}(Q) \leftarrow \text{EmbedText}(Q) \) \COMMENT{/* Convert query to embedding vector */}
        
        \STATE Retrieve the top-\( U \) entities \( \{e_{u_1}, e_{u_2}, \ldots, e_{u_U}\} \) by computing the cosine similarity:
        \STATE \quad \( \text{sim}(\mathbf{v}(Q), \mathbf{v}(e_i)) = \frac{\mathbf{v}(Q) \cdot \mathbf{v}(e_i)}{\|\mathbf{v}(Q)\| \|\mathbf{v}(e_i)\|} \)
        
        \FOR{each retrieved entity \( e_u \)}
            \STATE Extract one-hop neighbors \( \{e_{v_1}, e_{v_2}, \ldots, e_{v_{V_u}}\} \) and their relationships \( (e_u, r_{uv}, e_v) \) \COMMENT{/* Retrieve connected entities and relationships */}
            
            \STATE Form the set of triples: \( \mathcal{T}_u = \{(e_u, r_{uv}, e_v) \mid v = 1, 2, \ldots, V_u\} \)
            
            \STATE Identify the parent chunk nodes \( \mathcal{P}_{uv} \):
            \STATE \quad \( \mathcal{P}_{uv} = (\mathcal{C}(e_u) \cap \mathcal{C}(e_v)) \cup \left( (\mathcal{C}(e_u) \cup \mathcal{C}(e_v)) \setminus (\mathcal{C}(e_u) \cap \mathcal{C}(e_v)) \right) \) \COMMENT{Combine overlapping and unique chunk nodes to retain context}
            
            \STATE Retrieve associated text chunks \( v_{t_k} \) and image nodes \( v_{img_k} \) \COMMENT{/* Gather relevant contextual information */}
        \ENDFOR
        
        \STATE Form the retrieved subgraph \( \mathcal{R}_{uv} = \{ (\mathcal{T}_u, \mathcal{P}_{uv}) \} \):
        \STATE \quad - Triples \( \mathcal{T}_u \) representing relationships for entity \( e_u \)
        \STATE \quad - Context from parent chunk nodes \( \mathcal{P}_{uv} \)
        \STATE \quad - Relevant images \( V_{img} \) linked via `VISUALLY\_SIMILAR' relationships
        
        \STATE Aggregate the retrieved data (triples, text chunks, and images) \COMMENT{/* Combine structured and contextual data */}
        
        \STATE Generate response using Graph RAG with the LLM: \( \text{Response} \leftarrow \text{GraphRAG}(\mathcal{R}_{uv}) \) \COMMENT{/* Generate coherent and contextually accurate answer */}
        
        \STATE Output the generated response
    \ENDFOR
\end{algorithmic}
\end{algorithm}

data, enabling precise exploration. Please refer to  Algorithm~\ref{alg:Multimodal_KG_Semantic_Search} for constructing and querying a multimodal knowledge graph for semantic search and retrieval.

\vspace{1mm} 
\section{Computational Complexity in Multimodal Patent (PatKG) / Scientific Knowledge Graph (SciKG) Construction and Graph RAG}
\vspace{1mm}
The computational complexity of constructing and querying a multimodal patent knowledge graph (PatKG) or a scientific knowledge graph (SciKG) for Graph RAG involves several steps. First, processing a document with \( N \) tokens into overlapping chunks of length \( l \) using a stride \( s \) results in \( K = \lceil (N - l)/s + 1 \rceil \) chunks, where each chunk corresponds to a segment of the document \( D_{\text{doc}} \). Embedding these \( K \) chunks into vectors \(\mathbf{v}(c_i) \in \mathbb{R}^d\) typically costs \( O(K \cdot T_{\text{embed}}(l)) \), where \( T_{\text{embed}}(l) \) often scales as \( O(l^2) \) due to the quadratic complexity of attention in transformer-based embedding models. \( T_{\text{embed}}(l) \) denotes the computational cost of generating an embedding for a chunk of length \( l \), captures all the computational steps required to produce an embedding from an input chunk, including any encoding layers, attention operations, feed-forward networks, and etc. Extracting triples from each chunk using an LLM (e.g., GPT-4o)—a process that includes entity recognition and linking—scales approximately as \( O(K \cdot (l^2 + l)) \), dominated by the \( O(l^2) \) attention mechanism. Constructing the knowledge graph, which involves creating entity nodes \( V_E = \{e_{s_{ij}}, e_{o_{ij}}\} \), linking chunks \( v_{t_i} \) to entities via `MENTIONS` relationships, and storing triples \((e_{s_{ij}}, r_{ij}, e_{o_{ij}})\), results in space complexity proportional to \( O(K + |V_E| + \sum_{i=1}^{K} M_i) \), where \( |V_E| \) denotes the number of unique entities after deduplication and \( M_i \) is the number of triples extracted from the \( i \)-th chunk. Before deduplication, \( |V_E| \) can be as large as \(\sum_{i=1}^K 2M_i\). Entity deduplication employs approximate nearest neighbor (ANN) search on entity embeddings \(\mathbf{v}(e_{x_{ij}})\) and string-based metrics (like Levenshtein similarity), reducing indexing complexity to \( O(|V_E|\log|V_E|) \) and query time to \( O(\log|V_E|) \), thereby avoiding a full \( O(|V_E|^2) \) comparison. When querying, the user query \( Q \) is embedded into \(\mathbb{R}^d\), and similarity to each entity embedding is computed in \( O(|V_E| \cdot d) \). Selecting the top-\( U \) most similar entities (as defined in the original framework) can be done in \( O(|V_E| \log U) \) (or \( O(|V_E|\log|V_E|) \) in the worst case). Subsequent graph traversal to retrieve relevant subgraphs from these top-\( U \) entities scales approximately as \( O(K' \cdot \delta) \), where \( K' \) is the number of retrieved triples and \(\delta\) the average node degree. For images, embedding \( I \) images each split into \( p \) patches (e.g., for a vision transformer) costs \( O(I \cdot p^2) \), and retrieving visually similar images via ANN reduces complexity from \( O(I^2 \cdot d) \) to \( O(I \cdot \log I) \). To remain scalable as \( N \), \(|V_E|\), and \( I \) increase, the system leverages ANN indexing, parallelization (e.g., GPU/TPU acceleration), caching frequently accessed data, and careful chunking strategies that balance context retention with computational efficiency.

\vspace{1mm}
\section{Related Work}
\vspace{1mm}
Large Language Models (LLMs) have emerged as powerful tools for streamlining scientific discovery by automating complex tasks. To evaluate their effectiveness in autonomous data-driven research, benchmarks such as DiscoveryBench \cite{majumder2024discoverybench} and ScienceAgentBench \cite{chen2024scienceagentbenchrigorousassessmentlanguage} have been developed. DiscoveryBench focuses on multi-step workflows involved in hypothesis-driven research across diverse disciplines, including sociology, biology, and engineering. It defines each task using a dataset, its metadata, and a clearly articulated discovery objective in natural language, challenging LLMs to demonstrate skills in statistical evaluation and the semantic interpretation of scientific concepts. On the other hand, ScienceAgentBench is designed to assess the capabilities of LLMs in automating various workflows across fields like bioinformatics, computational chemistry, and cognitive neuroscience. The tasks, derived from peer-reviewed studies and validated by experts, involve generating executable Python programs from natural language prompts, incorporating dataset details and optional expert knowledge. This benchmark employs a detailed evaluation protocol to measure task completion, computational efficiency, and the relevance and plausibility of results within scientific contexts. Building on these evaluation datasets, various frameworks have been developed to enhance knowledge discovery and hypothesis generation. The LLM-Duo framework \cite{hu2024automating} employs a dual-agent architecture to enhance large-scale knowledge base curation using the Progressive Ontology Prompting (POP) algorithm. The explorer agent, leveraging Retrieval-Augmented Generation (RAG), processes scientific literature to extract information guided by POP-generated prompts based on a predefined ontology. This structured ontology, often defined by domain experts, represents the target knowledge domain. The evaluator agent ensures the explorer’s responses are accurate and complete through iterative feedback and refinement, achieving high-quality knowledge discovery. In parallel, the SciAgents framework \cite{ghafarollahi2024sciagents} automates scientific discovery in material science by employing a multi-agent system integrated with a large ontological knowledge graph and advanced LLMs. This framework systematically generates, critiques, and refines research hypotheses through the collaboration of agents assigned distinct roles, including planners, ontologists, scientists, and critics. By utilizing heuristic pathfinding within the knowledge graph, the framework identifies relationships between concepts and structures hypotheses into detailed, actionable research plans. Iterative refinement ensures the rigor of the process, while external tools like the Semantic Scholar API assess the novelty and feasibility of the generated hypotheses. This scalable approach expedites hypothesis generation and promotes innovative, cross-disciplinary insights in scientific research. DataVoyager \cite{majumderposition} automates the process of scientific discovery by enabling end-to-end, data-driven hypothesis generation and verification. It incorporates a role-based multi-agent architecture, including planners, programmers, data experts, and critics, to manage workflows from data understanding and hypothesis generation to verification and refinement. Operating in autonomous or user-guided modes, the framework facilitates dynamic hypothesis exploration, robust statistical evaluation, and iterative improvement through user feedback. Highlighting the innovative potential of LLMs, a large-scale evaluation of LLMs in research ideation \cite{si2024can} found that while LLM-generated ideas tend to be more novel, they may be less feasible compared to human-generated ideas. This study underscores both the creative promise and the current limitations of LLMs in scientific discovery, particularly the challenge of balancing creativity with practical feasibility. Addressing some of these limitations, the Scideator framework \cite{radensky2024scideator} is an LLM-powered scientific ideation tool that generates novel research ideas by recombining facets (purpose, mechanism, evaluation) extracted from user-provided and analogous research papers. Users input papers, and the system retrieves related works, extracts key facets, and facilitates the interactive selection and recombination of these facets to generate innovative ideas. A novelty checker, leveraging RAG and expert-labeled examples, assesses idea originality and provides suggestions for improvement. Through iterative refinement, the framework enables researchers to explore diverse idea spaces grounded in literature, fostering creativity and originality in scientific ideation. Most recently, the AI Scientist framework \cite{lu2024ai} fully automates scientific discovery using LLMs by generating ideas, conducting experiments, and drafting research papers with automated reviews. It integrates ideation, experimentation, and refinement to mimic human scientific processes, enabling scalable and efficient research generation at low costs. Collectively, these studies underscore the expanding role of LLMs in automating various aspects of scientific discovery—from knowledge extraction to hypothesis generation and ideation—but also highlight several limitations of data-driven discovery, such as hallucinations, the cost of operating at scale, potential misuse, legal ramifications, and inherent biases. While LLMs have advanced scientific discovery, their application to patent ideation and drafting remains largely unexplored. Patent ideation leverages knowledge extraction from patent literature and scientific discovery to generate feasible hypotheses for novel inventions. Patent drafting translates these inventions into legally sound documents. Integrating LLMs into multi-agent frameworks offers significant potential to revolutionize patent processes. This promises to streamline patent creation and accelerate technological innovation. 


\vspace{1mm}
\section{Additional Experiments}

\vspace{1mm}
\section{Monte Carlo Tree Search for Exploring Patent Hypothesis Graphs as an Alternative to APHD}
\vspace{1mm}
We employ Monte Carlo Tree Search (MCTS) as an alternative to the Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm for uncovering patent hypotheses within the Patent Hypothesis Exploration Graph (PHEG). The PHEG, constructed from Subject-Action-Object (SAO) triplets extracted from patents using advanced language models like GPT-4, represents entities as nodes and actions or relationships as edges. MCTS systematically explores the PHEG through four interconnected phases. In the selection phase, starting at the root node, the algorithm evaluates immediate neighbors by calculating the Upper Confidence Bound for Trees (UCT) value to balance exploration and exploitation: \( \text{UCT}(v) = \frac{w_v}{n_v} + c \sqrt{\frac{\ln n_p}{n_v}} \), where \( w_v \) is the total reward accumulated by node \( v \), \( n_v \) is the visit count of node \( v \), \( n_p \) is the visit count of the parent node, and \( c \) is an exploration constant. The term \( \frac{w_v}{n_v} \) promotes nodes with higher average rewards, exploiting known promising paths, while \( c \sqrt{\frac{\ln n_p}{n_v}} \) encourages exploration by favoring less-visited nodes, preventing the search from converging prematurely to suboptimal solutions. Nodes with no visits (\( n_v = 0 \)) are assigned \( \text{UCT}(v) = \infty \) to prioritize their exploration, and the node with the highest UCT value is selected for expansion. During the expansion phase, the selected node \( v \) is added to the search tree, initializing \( n_v = 0 \) and \( w_v = 0 \). In the simulation phase, the algorithm simulates a path starting from \( v \) to estimate the likelihood of reaching a target node \( T \), initializing \( v_{\text{sim}} = v \) and path \( P = [v_{\text{sim}}] \). A temperature parameter \( \tau \) introduces stochasticity to mitigate greedy bias. The simulation proceeds until \( T \) is reached, the maximum path length \( L_{\text{max}} \) is exceeded, or a node with no successors is encountered. Adjacent nodes of \( v_{\text{sim}} \) are evaluated using a softmax function over their cosine similarities: \( h(v') = \frac{\exp\left(\cos(\mathbf{h}_{v'}, \mathbf{h}_T) / \tau\right)}{\sum_{v''} \exp\left(\cos(\mathbf{h}_{v''}, \mathbf{h}_T) / \tau\right)} \), where \( \mathbf{h}_{v'} \) and \( \mathbf{h}_T \) are the embeddings of \( v' \) and \( T \), respectively, computed via sentence embedding models like \texttt{text-embedding-3-small}. To prevent infinite loops, the algorithm tracks visited nodes and terminates if a node is revisited. State pruning strategies, such as threshold-based pruning (eliminating nodes with low heuristic values), and depth-limited search (restricting path depth to \( L_{\text{max}} \)), eliminate redundant or low-potential nodes, ensuring efficient exploration. When the simulation completes, the backpropagation phase begins by calculating a reward \( R \) for the path. If the simulation reaches the target node \( T \), the reward is computed as \( R = R_{\text{max}} - \lambda \cdot \text{len}(P) \); otherwise, it is given by \( R = -\gamma_{\text{dist}} \cdot \text{dist}(v_{\text{sim}}, T) - \lambda \cdot \text{len}(P) \), where \( R_{\text{max}} \) is a positive reward for reaching \( T \), \( \text{len}(P) \) is the path length, \( \lambda \) penalizes longer paths, \( \text{dist}(v_{\text{sim}}, T) \) estimates the distance from \( v_{\text{sim}}, T \), and \( \gamma_{\text{dist}} \) controls the penalty weight. The algorithm updates statistics for each node along the path by incrementing the visit counts \( n_v \leftarrow n_v + 1 \) and rewards \( w_v \leftarrow w_v + R \). By iteratively executing these phases, MCTS builds a search tree that balances exploration and exploitation through the UCT formula and steers simulations toward promising regions using stochastic heuristic evaluations based on pre-computed embeddings. Unlike the APHD algorithm, which models the problem as a Markov Decision Process (MDP) and employs reinforcement learning with PPO to train a policy network via gradient descent, MCTS incrementally constructs its search tree through statistical sampling and heuristic guidance, avoiding the need for neural network training or loss function minimization. This makes MCTS computationally scalable, especially for moderate-scale PHEGs, by reducing the overhead associated with neural networks. However, careful tuning of parameters such as \( c \), \( R_{\text{max}} \), \( \lambda \), \( \gamma_{\text{dist}} \), and \( \tau \) is crucial for optimal performance, as these parameters directly influence the trade-off between exploration and exploitation, path efficiency, and the discovery of non-obvious hypotheses within the PHEG. To compare MCTS with APHD, key performance metrics


\begin{algorithm}[H]
\caption{MCTS for Patent Hypothesis Discovery}
\label{alg:MCTS_PHEG}
\begin{algorithmic}[1]
\STATE \textbf{Initialize:}
\STATE \quad Initialize search tree \( \mathcal{T} \) with root node \( S \in \mathcal{V} \)
\STATE \quad Initialize \( n_S \leftarrow 0 \), \( w_S \leftarrow 0 \)

\WHILE{number of iterations \( < N_{\text{max}} \)}
    \STATE \textbf{Selection Phase:}
    \STATE \quad Set current node \( u \leftarrow S \)
    \WHILE{node \( u \) is fully expanded \textbf{and} not a terminal node}
        \FOR{each child \( v \) of \( u \)}
            \IF{\( n_v = 0 \)}
                \STATE \( \text{UCT}(v) \leftarrow \infty \) \COMMENT{Prioritize unvisited nodes}
            \ELSE
                \STATE \( \text{UCT}(v) \leftarrow \frac{w_v}{n_v} + c \sqrt{\frac{\ln n_u}{n_v}} \)
            \ENDIF
        \ENDFOR
        \STATE Select child \( v \) with the highest \( \text{UCT}(v) \)
        \STATE Update \( u \leftarrow v \)
    \ENDWHILE

    \STATE \textbf{Expansion Phase:}
    \IF{node \( u \) is not terminal \textbf{and} not fully expanded}
        \STATE Choose an unvisited child \( v_{\text{new}} \) of \( u \)
        \STATE Add \( v_{\text{new}} \) to \( \mathcal{T} \) with \( n_{v_{\text{new}}} \leftarrow 0 \), \( w_{v_{\text{new}}} \leftarrow 0 \)
    \ELSE
        \STATE Set \( v_{\text{new}} \leftarrow u \)
    \ENDIF

    \STATE \textbf{Simulation Phase:}
    \STATE \quad Initialize \( v_{\text{sim}} \leftarrow v_{\text{new}} \), path \( P \leftarrow [v_{\text{sim}}] \)
    \WHILE{\( v_{\text{sim}} \neq T \) \textbf{and} \( \text{len}(P) < L_{\text{max}} \) \textbf{and} \( \mathcal{N}(v_{\text{sim}}) \neq \emptyset \)}
        \STATE Retrieve successors \( \mathcal{N}(v_{\text{sim}}) \) from PHEG
        \STATE Compute cosine similarities:
        \STATE \( s(v') \leftarrow \cos(\mathbf{h}_{v'}, \mathbf{h}_T) \) for each \( v' \in \mathcal{N}(v_{\text{sim}}) \)
        \STATE Compute probabilities using softmax:
        \STATE \( h(v') \leftarrow \frac{\exp\left(\frac{s(v')}{\tau}\right)}{\sum_{v'' \in \mathcal{N}(v_{\text{sim}})} \exp\left(\frac{s(v'')}{\tau}\right)} \)
        \STATE Sample the next node \( v' \) based on \( h(v') \)
        \STATE Update \( v_{\text{sim}} \leftarrow v' \) and append \( v' \) to path \( P \)
    \ENDWHILE

    \STATE \textbf{Backpropagation Phase:}
    \STATE \quad Calculate reward \( R \):
    \IF{\( v_{\text{sim}} = T \)}
        \STATE \( R \leftarrow R_{\text{max}} - \lambda \cdot \text{len}(P) \)
    \ELSE
        \STATE \( R \leftarrow -\gamma_{\text{dist}} \cdot \text{dist}(v_{\text{sim}}, T) - \lambda \cdot \text{len}(P) \)
    \ENDIF
    \STATE \quad Initialize \( \gamma \leftarrow 1 \)
    \FOR{each node \( v \) in \( P \) in reverse order}
        \STATE \( n_v \leftarrow n_v + 1 \)
        \STATE \( w_v \leftarrow w_v + \gamma \cdot R \)
        \STATE \( \gamma \leftarrow \gamma \cdot \gamma_{\text{decay}} \) \COMMENT{Optional discounting}
    \ENDFOR
\ENDWHILE

\STATE \textbf{Best Path Extraction:}
\STATE \quad Initialize \( \text{BestPath} \leftarrow [S] \)
\STATE \quad Set \( u \leftarrow S \)
\WHILE{children of \( u \) exist}
    \STATE Select child \( v \) of \( u \) with highest \( n_v \)
    \STATE Append \( v \) to \( \text{BestPath} \)
    \STATE Update \( u \leftarrow v \)
\ENDWHILE

\STATE \textbf{Return:} The best hypothesis path \( \text{BestPath} \) and the search tree \( \mathcal{T} \)
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{MCTS for Patent Hypothesis Discovery at Inference Time}
\label{alg:MCTS_Inference}
\begin{algorithmic}[1]
\STATE \textbf{Initialize:}
\STATE \quad Initialize search tree \( \mathcal{T} \) with root node \( S \in \mathcal{V} \)
\STATE \quad Initialize \( n_S \leftarrow 0 \), \( w_S \leftarrow 0 \)

\WHILE{number of iterations \( < N_{\text{max}} \)}
    \STATE \textbf{Selection Phase:}
    \STATE \quad Set current node \( u \leftarrow S \)
    \WHILE{node \( u \) is fully expanded \textbf{and} not a terminal node}
        \FOR{each child \( v \) of \( u \)}
            \IF{\( n_v = 0 \)}
                \STATE \( \text{UCT}(v) \leftarrow \infty \) \COMMENT{Prioritize unvisited nodes}
            \ELSE
                \STATE \( \text{UCT}(v) \leftarrow \frac{w_v}{n_v} + c \sqrt{\frac{\ln n_u}{n_v}} \)
            \ENDIF
        \ENDFOR
        \STATE Select child \( v \) with the highest \( \text{UCT}(v) \)
        \STATE Update \( u \leftarrow v \)
    \ENDWHILE

    \STATE \textbf{Expansion Phase:}
    \IF{node \( u \) is not terminal \textbf{and} not fully expanded}
        \STATE Choose an unvisited child \( v_{\text{new}} \) of \( u \)
        \STATE Add \( v_{\text{new}} \) to \( \mathcal{T} \) with \( n_{v_{\text{new}}} \leftarrow 0 \), \( w_{v_{\text{new}}} \leftarrow 0 \)
    \ELSE
        \STATE Set \( v_{\text{new}} \leftarrow u \)
    \ENDIF

    \STATE \textbf{Simulation Phase:}
    \STATE \quad Initialize \( v_{\text{sim}} \leftarrow v_{\text{new}} \), path \( P \leftarrow [v_{\text{sim}}] \)
    \WHILE{\( \text{len}(P) < L_{\text{max}} \) \textbf{and} \( \mathcal{N}(v_{\text{sim}}) \neq \emptyset \)}
        \STATE Retrieve successors \( \mathcal{N}(v_{\text{sim}}) \) from PHEG
        \STATE Compute heuristic scores \( h(v') \) for each \( v' \in \mathcal{N}(v_{\text{sim}}) \)
        \STATE Compute probabilities using softmax:
        \STATE \( p(v') \leftarrow \frac{\exp\left(\frac{h(v')}{\tau}\right)}{\sum_{v'' \in \mathcal{N}(v_{\text{sim}})} \exp\left(\frac{h(v'')}{\tau}\right)} \)
        \STATE Sample the next node \( v' \) based on \( p(v') \)
        \STATE Update \( v_{\text{sim}} \leftarrow v' \) and append \( v' \) to path \( P \)
    \ENDWHILE

    \STATE \textbf{Backpropagation Phase:}
    \STATE \quad Calculate reward \( R \) based on heuristics:
    \STATE \quad \( R \leftarrow \text{novelty}(P) + \text{diversity}(P)  - \lambda \cdot \text{len}(P) \)
    \STATE \quad Initialize \( \gamma \leftarrow 1 \)
    \FOR{each node \( v \) in \( P \) in reverse order}
        \STATE \( n_v \leftarrow n_v + 1 \)
        \STATE \( w_v \leftarrow w_v + \gamma \cdot R \)
        \STATE \( \gamma \leftarrow \gamma \cdot \gamma_{\text{decay}} \) \COMMENT{Optional discounting}
    \ENDFOR
\ENDWHILE

\STATE \textbf{Best Path Extraction:}
\STATE \quad Initialize \( \text{BestPath} \leftarrow [S] \)
\STATE \quad Set \( u \leftarrow S \)
\WHILE{children of \( u \) exist}
    \STATE Select child \( v \) of \( u \) with the highest \( w_v \) or \( n_v \)
    \STATE Append \( v \) to \( \text{BestPath} \)
    \STATE Update \( u \leftarrow v \)
\ENDWHILE

\STATE \textbf{Return:} The best hypothesis path \( \text{BestPath} \) and the search tree \( \mathcal{T} \)
\end{algorithmic}
\end{algorithm}


include the Hypothesis Discovery Rate (percentage of valid hypotheses found), Path Efficiency (average path length), and Diversity of Discovered Hypotheses (range of unique hypotheses). This approach enables MCTS to efficiently uncover novel patent hypotheses within moderate-scale PHEGs by balancing systematic exploration, heuristic guidance, and computational efficiency \cite{swiechowski2023monte}. As shown in Algorithm~\ref{alg:MCTS_Inference}, the inference-time MCTS algorithm is designed to work without a predefined target node by relying on heuristics to guide exploration and reward paths. This approach balances exploration and exploitation, making it suitable for uncovering novel hypotheses or exploring patent hypothesis graphs effectively. The reward \(R\) at inference time quantifies the quality of a simulated path \(P\) during the exploration of an exploration graph. This heuristic-based rewards guide MCTS to discover meaningful paths even without a specific target node \(T\).


\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
R = \sum_{v_i \in P} \frac{1}{n_{v_i} + \epsilon} + \frac{1}{| \mathcal{P} |} \sum_{P_j \in \mathcal{P}} (1 - \cos(\mathbf{h}_P, \mathbf{h}_{P_j})) - \lambda \cdot \text{len}(P) \nonumber
\end{equation}
\end{minipage}
}

where \(P\) represents the path taken during the simulation phase of the Monte Carlo Tree Search (MCTS). It is a sequence of nodes \([v_1, v_2, \dots, v_{\text{sim}}]\), starting from the expanded node and ending at the simulated node \(v_{\text{sim}}\). The reward calculation balances several factors: the novelty term \(\sum_{v_i \in P} \frac{1}{n_{v_i} + \epsilon}\) rewards paths that visit less-explored nodes, while the diversity term \(\frac{1}{| \mathcal{P} |} \sum_{P_j \in \mathcal{P}} (1 - \cos(\mathbf{h}_P, \mathbf{h}_{P_j}))\) encourages exploring new regions by rewarding paths that differ from previously explored ones. Additionally, the path length penalty \(-\lambda \cdot \text{len}(P)\) discourages unnecessarily long paths. During training, the target node \(T\) guides learning by influencing the reward calculation. In inference, heuristic-based rewards (novelty, diversity, and node quality) replace target-dependent rewards, making MCTS suitable when \(T\) is unknown. This adaptation allows MCTS to efficiently explore the PHEG and uncover meaningful paths even without a predefined target.

\vspace{1mm}
\section{Comparative Study of Community-Detection-Based Graph RAG and Contextualized Graph RAG for Multimodal Patent Graphs}
\vspace{1mm}
In this section, we construct and query a multimodal Patent Knowledge Graph (PatKG) and a Scientific Knowledge Graph (SciKG) using the Community-Detection-Based Graph RAG technique and compare it with our Contextualized Graph RAG method. The primary distinction lies in their retrieval strategies and the use of contextual information. While Graph RAG employs community detection to cluster nodes (representing subjects or atomic entities in atomic triples) into coherent subgraphs based on the relations between these entities, enhancing query-focused summarization, it does not retrieve parent-augmented chunks. In contrast, our Contextualized Graph RAG bypasses community detection and improves semantic search by retrieving atomic triples along with their parent-augmented chunks (original text chunks augmented with chunk-specific context extracted from the overall document). This approach retains essential content from surrounding paragraphs, making it more effective for complex, cross-document patent and scientific analysis. Graph RAG represents the knowledge graph as \( G = (V, E) \), where \( V \) consists of entity nodes (\( V_E \)), text chunk nodes (\( V_T \)), and image nodes (\( V_{\text{img}} \)), while \( E \) captures relationships such as MENTIONS, VISUALLY\_SIMILAR, and atomic triple relations (subject-relation-object). To ensure coherence and eliminate redundancy, entity deduplication is performed using a combined similarity measure:  $ \text{sim}_{\text{combined}}(e_{x_{ij}}, e_{x_{kl}}) = \alpha \cdot \text{sim}(\mathbf{v}(e_{x_{ij}}), \mathbf{v}(e_{x_{kl}})) + \beta \cdot \text{lev\_sim}(e_{x_{ij}}, e_{x_{kl}})$, where \( \alpha + \beta = 1 \). Following deduplication, the hierarchical Leiden algorithm detects communities by maximizing modularity:
\[
M = \frac{1}{2m} \sum_{i, j} \left[ A_{ij} - \frac{k_i k_j}{2m} \right] \delta(c_i, c_j),
\]
where \( m \) is the total number of edges, \( A_{ij} \) is the adjacency matrix, \( k_i \) and \( k_j \) are node degrees, and \( \delta(c_i, c_j) \) is the Kronecker delta function ensuring only intra-community connections contribute to modularity. The modularity function evaluates the strength of connections within a community compared to a random graph with the same degree distribution. By maximizing modularity, the algorithm identifies subgraphs where nodes are more densely connected to each other than to nodes outside. These subgraphs naturally align with entities and relationships that frequently occur together in the knowledge graph. For each community \( C_i \), atomic triples and their multi-hop relationship paths are extracted and summarized using an LLM: $ S_i = \text{LLM}(R_i) = \arg\max_{S} P(S \mid R_i)$, where \( P(S \mid R_i) \) denotes the probability of summary \( S \) given paths \( R_i \). Summaries \( S_i \) are embedded into vectors \( \mathbf{v}(S_i) \) and cached for efficient retrieval. Upon receiving a user query \( Q \), it is embedded as \( \mathbf{v}(Q) \), and the cosine similarity $ d(Q, C_i) = \frac{\langle \mathbf{v}(Q), \mathbf{v}(S_i) \rangle}{| \mathbf{v}(Q) | , | \mathbf{v}(S_i) |}$ is computed to identify the top-\( K \) most relevant communities \( C_Q \). The summary \( S_{C_Q} \) provides context for generating a query-aligned response, with multiple relevant communities aggregated to ensure accuracy and completeness.

\vspace{1mm}
\section{Optimized Prompting for Structured Knowledge Extraction}
\vspace{1mm}
Meta-prompting is an advanced technique where language models (LMs) iteratively generate or refine prompts to improve their own or other models' outputs. By dynamically enhancing prompts or decomposing complex tasks, LMs can self-improve, apply adaptive reasoning strategies, and select optimal tools. Inspired by human meta-reasoning, this approach boosts performance and efficiency, particularly in structured information extraction from unstructured patent or scholarly articles. In this section, we present strategies for crafting effective prompts to optimize the retrieval and analysis of structured knowledge from patents or scholarly articles. The objective is to improve the quality and accuracy of information extraction by using precise, structured instructions that guide LMs effectively. We leverage OpenAI's prompt generation methodologies, employing meta-prompts and meta-schemas to refine or develop optimal prompts tailored to specific tasks. The focus is on clarity, specificity, and well-structured guidance to ensure accurate knowledge extraction for various downstream tasks. For constructing Patent Hypothesis Knowledge Graphs (PHEGs), our method integrates Chain-of-Thought (CoT) reasoning with self-reflection to improve the accuracy of Subject, Action, Object (SAO) analyses. Carefully designed prompts guide LLMs to analyze patent content, extract hypotheses, and decompose the information into SAO components. The LLM then applies self-reflection to evaluate and refine each decomposition iteratively, enhancing precision and reliability in hypothesis extraction. Code examples (see Figure~\ref{fig:Code1}) illustrate how CoT reasoning and self-reflection are combined in prompts to facilitate this process. To generate multimodal knowledge graphs from unstructured patent or scholarly articles for constructing Patent Knowledge Graphs (PatKG) or Scientific Knowledge Graphs (SciKG), we employ a CoT reflection-based approach to extract triples in the (Subject, Predicate, Object) format. This method prompts the LLM to iteratively decompose text into knowledge triples and refine extractions through self-reflection. This iterative feedback cycle improves precision over single-pass methods, leading to more accurate and context-aware knowledge graph construction. (See Figure~\ref{fig:Code2} for code snippets.) By incorporating optimized prompts, meta-prompting techniques, and CoT-based self-reflection, our approach enhances structured knowledge extraction, supports tool integration, and ensures high-fidelity analysis for patent-related tasks.


\vspace{1mm}
\section{Experimental Settings}

\vspace{1mm}
\subsection{Configuration and Tuning Parameters of APHD Algorithm}
\vspace{1mm}
The Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm employs a comprehensive set of experimental settings, configured before training, to optimize its performance in exploring complex knowledge graphs, such as Patent Hypothesis Exploration Graphs (PHEGs). These settings govern crucial trade-offs between exploration and exploitation, reward structure, and policy stability. The reward function parameters—$\gamma_{\text{dist}} = 0.7$, $\lambda = 0.1$, and $\eta = 0.3$—encourage efficient pathfinding while promoting the discovery of less-explored nodes. Specifically, $\gamma_{\text{dist}}$ prioritizes minimizing the distance to target nodes, $\lambda$ penalizes longer paths to enhance efficiency, and $\eta$ rewards visiting novel nodes. Policy optimization settings—$\beta = 0.01$, $\delta_{\mathcal{P}} = 0.5$, $\epsilon = 0.05$, and $\gamma = 0.95$—ensure effective learning and stability. The entropy weight $\beta$ fosters exploration by adding randomness to action selection, while the redundancy penalty $\delta_{\mathcal{P}}$ discourages revisiting overlapping paths. The clipping parameter $\epsilon$ stabilizes Proximal Policy Optimization (PPO) updates, and the discount factor $\gamma$ balances immediate and future rewards. Graph Neural Network (GNN) settings, such as $L = 2$ layers and feature dimensionality $F = 64$, define the depth and expressiveness of node embeddings, enabling effective information propagation. Training configurations—iterations $K = 100$, episodes per iteration $M = 5$, maximum steps per episode $T = 25$, policy network update frequency $N = 10$ steps, and batch size $B = 16$—balance computational efficiency with sufficient exploration for meaningful learning. Each iteration involves up to $M \times T = 5 \times 25 = 125$ steps. With $N = 10$ steps per policy update, the policy network is updated approximately $\frac{125}{10} = 12.5$ times per iteration. Iterations ($K$) represent full cycles of trajectory collection and updates; episodes ($M$) denote sets of steps within each iteration; steps ($T$) are actions taken in an episode; and the PPO update frequency ($N$) is the number of updates applied to the policy network per batch. Additionally, attention weights—$\beta_1 = 0.50$, $\beta_2 = 0.30$, and $\beta_3 = 0.20$—adjust the contributions of different components within the state representation (current node embeddings, multi-hop context, and path history), ensuring context-aware decision-making. A grid search is employed to identify optimal configurations, enhancing APHD's ability to navigate large-scale graphs and accelerate hypothesis discovery. This structured framework empowers APHD to explore intricate graphs effectively, fostering innovation through strategic exploration and efficient pathfinding.

\vspace{1mm}
\subsection{Configuration and Tuning Parameters for Contextualized Graph RAG for Multimodal Patent/Scientific Knowledge Graphs}
\vspace{1mm}
Optimizing the Contextualized Graph RAG framework for Multimodal Patent Knowledge Graphs (PatKG) and Scientific Knowledge Graphs (SciKG) involves careful selection of key parameters to achieve optimal performance, scalability, and precision when handling complex patent and scientific datasets. Chunk size \(l = 2048\) and stride \(s = 512\) are configured to balance context retention and processing efficiency, where a larger \(l\) preserves more context and \(s\) ensures overlap between adjacent chunks. The Duplicate Similarity Threshold \(\theta_{\text{dup}} = 0.9\) and Image Similarity Threshold \(\theta_{\text{sim}} = 0.85\) govern entity deduplication and visually similar image linking. The weights Alpha \(\alpha = 0.8\) and Beta \(\beta = 0.2\) balance cosine and Levenshtein similarity measures, ensuring \(\alpha + \beta = 1\). An embedding dimension \(d = 768\), based on the text-embedding-3-small model from OpenAI, captures detailed semantic relationships and improves computational efficiency. The retrieval parameters, Top-\(K\) Entities \(K = 10\) and Top-\(K\) Similar Images \(K_{\text{similar}} = 3\), manage the scope of relevant results. The choice of pre-trained models, such as GPT-4o for triple extraction and OpenAI CLIP for vision embeddings, significantly influences retrieval performance. Finally, One-Hop Triple Extraction ensures efficient graph traversal by focusing on direct connections.

\vspace{1mm}
\section{Hyperparameter Tuning}
\vspace{1mm}
Hyperparameter tuning enhances the performance of machine learning models by refining key parameters that influence learning dynamics. It balances objectives such as exploration, exploitation, efficiency, and retrieval precision, ensuring that the Adaptive Pathfinding for Hypothesis Discovery (APHD) algorithm and the MPKG/SciKG-Driven Semantic Search and Retrieval framework perform optimally.  For the APHD algorithm, hyperparameter tuning is essential to achieve optimal exploration of Patent Hypothesis Exploration Graphs (PHEGs) while maintaining efficiency in reaching target nodes. The reward distance weight \( \gamma_{\text{dist}} \), explored in the range \(\{0.6, 0.7, 0.8\}\), controls the emphasis on minimizing the distance to target nodes. The entropy regularization weight \( \beta \), tuned within \(\{0.005, 0.01, 0.02\}\), introduces randomness to the policy, encouraging exploration and preventing premature convergence. The redundancy penalty \( \delta_{\mathcal{P}} \), explored across \(\{0.4, 0.5, 0.6\}\), discourages redundant path traversal by penalizing overlapping paths, ensuring more diverse exploration. Additionally, the discount factor \( \gamma \), tuned within \(\{0.9, 0.95, 0.99\}\), balances immediate and future rewards in policy optimization, supporting a long-term graph traversal strategy. In the MPKG/SciKG-Driven Semantic Search and Retrieval framework, hyperparameter tuning ensures a balance between retrieval precision and computational efficiency. The chunk size \( l \), explored across \(\{1024, 2048, 4096\}\), affects the trade-off between context retention and processing speed, with larger chunks preserving more context but increasing computational cost. The duplicate similarity threshold \( \theta_{\text{dup}} \), with values in the range \(\{0.85, 0.9, 0.95\}\), determines how aggressively similar entities are merged to avoid redundancy in the knowledge graph. The Top-\(K\) parameter \( K \), explored within \(\{5, 10, 15\}\), controls the number of retrieved entities, balancing relevance and recall. A One-Factor-at-a-Time (OFAT) tuning technique is used to systematically explore the parameter space, identifying optimal configurations for both the APHD algorithm and the MPKG/SciKG framework. These tuned parameters enable the APHD algorithm to efficiently explore large-scale graphs and discover innovative hypotheses, while the MPKG/SciKG framework ensures accurate and scalable retrieval for multimodal patent and scientific literature analysis.


\vspace{1mm}
\section{\textbf{Knowledge Graph Quality Evaluation}}
\vspace{1mm}
The proposed multi-agent framework, \textbf{Inventa}, for automating patent ideation and drafting involves the construction of three key knowledge graphs: Patent Hypothesis Exploration Graphs (PHEGs), which extract functional hypotheses from patents using Subject-Action-Object triplet analysis; Multimodal Patent Knowledge Graphs (MPKGs), designed to enhance information retrieval across patent documents; and Multimodal Scientific Knowledge Graphs (SCKGs), which capture relationships among scientific concepts and findings in scholarly literature. To construct these graphs, we employ advanced language models like GPT-4 to extract entities and relationships from unstructured text, facilitating automated taxonomy creation and ontology expansion. These models help develop comprehensive knowledge graphs by identifying key concepts and their connections. To evaluate the constructed knowledge graphs against those built with contemporary language models, such as Claude 3.5 Sonnet, we employ several important metrics to ensure their quality and accuracy for applications like semantic search and data integration. To assess entity coherence, we use clustering metrics like modularity and conductance. Modularity \(M\) is defined as:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
M = \frac{1}{2m} \sum_{i,j} \left( A_{ij} - \frac{k_i k_j}{2m} \right) \delta(c_i, c_j) \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \(A_{ij}\) is the adjacency matrix, \(m\) is the total number of edges, \(k_i\) is the degree of node \(i\), and \(c_i\) represents the cluster assignment. A higher modularity score indicates stronger clustering. Conductance \(\Phi\) is measured by:

\resizebox{0.95\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
\Phi(C) = \frac{|\text{cut}(C)|}{\min(\text{vol}(C), \text{vol}(V \setminus C))} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
where \( |\text{cut}(C)| \): Number of edges that connect nodes in cluster \(C\) to nodes outside \(C\).  \( \text{vol}(C) \): Sum of the degrees of all nodes in cluster \(C\).
\( \text{vol}(V \setminus C) \): Sum of the degrees of all nodes not in \(C\). Lower conductance values signify more cohesive clusters (fewer edges connecting clusters to the rest of the graph). \textbf{Graph Completeness (GC)} evaluates the extent to which the graph covers all relevant entities and relationships, expressed as:

\resizebox{0.925\linewidth}{!}{
\begin{minipage}{\linewidth}
\begin{equation}
GC = \frac{\text{Number of Entities/Relationships in the Generated Graph}}{\text{Total Number of Entities/Relationships in the Reference Graph}} \nonumber
\end{equation}
\end{minipage}
}

\vspace{1mm}
Modularity ranges from -1 to 1, with values closer to 1 indicating strong clustering. Conductance ranges from 0 to 1, where lower values represent better cohesion. GC also spans from 0 to 1, with a value of 1 indicating complete coverage of relevant entities and relationships.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
% First part of the code in figure* spanning one page
\begin{tcolorbox}[colback=codebg, colframe=black, title= Patent Hypothesis Extraction (Part 1), sharp corners, boxrule=1pt]
\begin{lstlisting}[style=mypython]
import re
import textwrap

def cot_reflection(system_prompt, initial_query, client, model: str, return_full_response: bool=False, max_iterations: int=10):
    """
    Iteratively extracts a hypothesis from a patent document using the SAO triplet method.

    Parameters:
        system_prompt (str): The system prompt guiding the LLM's behavior.
        initial_query (str): The initial patent text to analyze.
        client: The API client instance for interacting with the LLM.
        model (str): The name of the LLM model to use.
        return_full_response (bool): If True, returns the full response; otherwise, returns only the hypothesis.
        max_iterations (int): The maximum number of iterations for refinement.

    Returns:
        str: The extracted hypothesis or a fallback message.
    """
    current_query = initial_query
    iteration = 0
    reflections = ""
    
    # Define the block of text separately
    sao_instructions = """
        You are an AI assistant that extracts hypotheses from patent documents using the Subject-Action-Object (SAO) triplet structure. Follow these steps:
        1. Identify and break down the patent into its core components: Subject, Action, and Object within the <sao_triplet> tags.
        2. Reflect on your breakdown to check for any errors or improvements within the <reflection> tags.
        3. Adjust your SAO triplet if necessary based on your reflection.
        4. Provide the final, concise hypothesis based on the SAO analysis within the <output> tags.

        Important: The <sao_triplet> and <reflection> sections are for your internal reasoning process only.
        Do not include any part of the final hypothesis in these sections.
        The actual hypothesis should be contained entirely within the <output> tags.
        If you decide to generate a final output, do not include any additional SAO triplets or reflection sections.

        Use the following format for your response:
        <sao_triplet>
        Subject: [Identify the core invention or technology]
        Action: [Describe the method or process applied]
        Object: [Specify the outcome or product resulting from the action]
        </sao_triplet>
        <reflection>
        [Reflect on your breakdown, checking for errors or improvements]
        </reflection>
        <output>
        [Provide the final hypothesis based on the SAO analysis. This is the only part that will be shown to the user.]
        </output>
    """
    
\end{lstlisting}
\end{tcolorbox}
\end{figure*}


\begin{figure*}[t]
% Second part of the code in figure* continuing from the previous page
\begin{tcolorbox}[colback=codebg, colframe=black, title=Patent Hypothesis Extraction (Part 2), sharp corners, boxrule=1pt]
\begin{lstlisting}[style=mypython]

    while iteration < max_iterations:
        iteration += 1
        logging.info(f"Starting iteration {iteration}")

        # Prepare the user prompt without including the system prompt
        cot_prompt = textwrap.dedent(f"""
            {sao_instructions}
            {"Reflection from previous iteration: " + reflections if reflections else ""}
            {"Current Query: " + current_query}
        """)

        # Make the API call
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": cot_prompt}
                ],
                temperature=0.7,
                max_tokens=1500  # Adjusted based on model capacity
            )
        except Exception as e:
            logging.error(f"API call failed: {e}")
            return "An error occurred while processing the patent."

        # Extract the full response
        full_response = response.choices[0].message.content
        logging.debug(f"Full response: {full_response}")

        # Use regex to extract the content within <sao_triplet>, <reflection>, and <output> tags (case-insensitive)
        sao_match = re.search(r"<sao_triplet>\s*(.*?)\s*</sao_triplet>", full_response, re.DOTALL | re.IGNORECASE)
        reflection_match = re.search(r"<reflection>\s*(.*?)\s*</reflection>", full_response, re.DOTALL | re.IGNORECASE)
        output_match = re.search(r"<output>\s*(.*?)\s*</output>", full_response, re.DOTALL | re.IGNORECASE)

        sao_triplet = sao_match.group(1).strip() if sao_match else None
        reflection = reflection_match.group(1).strip() if reflection_match else ""
        output = output_match.group(1).strip() if output_match else None

        logging.info(f"SAO Triplet: {sao_triplet}")
        logging.info(f"Reflection: {reflection}")
        logging.info(f"Output: {output}")

\end{lstlisting}
\end{tcolorbox}
\end{figure*}

% Third part of the code in figure* continuing from the previous page
\begin{figure*}[t]
\begin{tcolorbox}[colback=codebg, colframe=black, title=Patent Hypothesis Extraction (Part 3), sharp corners, boxrule=1pt]
\begin{lstlisting}[style=mypython]

        # If an <output> tag is found, return the output and stop
        if output:
            logging.info("Final hypothesis extracted.")
            return output

        # If SAO triplet is found, use it and the reflection for the next iteration
        if sao_triplet:
            current_query = sao_triplet
            reflections = reflection
        else:
            # If neither SAO nor output is found, terminate the loop
            logging.warning("Neither <sao_triplet> nor <output> found in the response. Terminating iterations.")
            break

    # Default fallback if max iterations are reached or no valid output is found
    logging.warning("Max iterations reached or no valid hypothesis extracted.")
    return full_response if return_full_response else (output if output else "No valid hypothesis extracted.")
    
def get_hypothesis_from_patent(model_name, patent_text, client):
    """
    Extracts a hypothesis from a patent document using the SAO triplet method.

    Parameters:
        model_name (str): The name of the LLM model to use.
        patent_text (str): The text content of the patent document.
        client: The API client instance for interacting with the LLM.

    Returns:
        str: The extracted hypothesis or a fallback message.
    """
    # Define the system prompt with clear instructions
system_prompt = (
    "You are a hypothesis extraction assistant. Focus on analyzing the provided patent text using the Subject-Action-Object (SAO) structure. "
    "Your primary task is to identify key elements: subject, action, and object, essential to forming a hypothesis from the given text. "
    "Follow this sequence:\n"
    "- Extract the relevant SAO triplet.\n"
    "- Reflect and refine if needed, based on identified gaps.\n"
    "- Present only the final hypothesis in the appropriate output section.\n\n"
    "Strict Rule: Return only the refined hypothesis inside <output> tags. Ensure intermediate steps, such as SAO triplets and reflections, are not shared with the user."
)
   
    initial_query = patent_text

    # Run the CoT reflection logic iteratively
    final_response = cot_reflection(
        system_prompt=system_prompt,
        initial_query=initial_query,
        client=client,
        model=model_name
    )

    return final_response
\end{lstlisting}
\end{tcolorbox}
\vspace{-5mm}
\caption{Python code implementing an iterative Chain-of-Thought (CoT) reflection method with Subject-Action-Object (SAO) triplets to extract hypotheses from patent documents. The process involves analyzing the patent text, generating SAO triplets, reflecting on the output, and refining it across multiple iterations via an LLM API until a concise hypothesis is produced.}
\label{fig:Code1}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% First part of the code in figure* spanning one page
\begin{figure*}[htbp]
\begin{tcolorbox}[colback=codebg, colframe=black, title=Atomic Triples Extraction (Part 1), sharp corners, boxrule=1pt]
\begin{lstlisting}[style=mypython]
import re

def cot_reflection_for_triples(system_prompt, initial_query, client, model: str, 
                               num_triples: int, return_full_response: bool = False, 
                               max_iterations: int = 10):
    current_query = initial_query
    iteration = 0
    thoughts = ""
    while iteration < max_iterations:
        iteration += 1

        # Define the block of text separately
        knowledge_triplets_instructions = f"""
        You are an AI assistant that extracts knowledge triples from text in the 
        form of (Subject, Predicate, Object). Follow these steps:
        
        1. Identify and break down the text into its core components: Subject, Predicate, and Object within the <triple_extraction> tags.
        2. Reflect on your breakdown to check for any errors or improvements within the <reflection> tags.
        3. Adjust your triple extraction if necessary based on your reflection.
        4. Provide the final, concise triples based on the analysis within the <output> tags.

        Important: The <triple_extraction> and <reflection> sections are for your internal reasoning process only. Do not include any part of the
        final extracted triples in these sections. The actual triples should be contained entirely within the <output> tags.
        If you decide to generate a final output, do not include any additional triples or reflection sections.

        Extract exactly {num_triples} triples.

        Use the following format for your response:
        <triple_extraction>
        [Break down the text into Subject, Predicate, and Object. This is your internal reasoning.]
        Subject: [Identify the subject or entity in the text]
        Predicate: [Identify the action or relation connecting the subject and object]
        Object: [Identify the object or result]
        </triple_extraction>
        <reflection>
        [Reflect on your breakdown, checking for errors or improvements]
        </reflection>
        <output>
        [Provide the final triples in the format (Subject, Predicate, Object). This is the only part shown to the user.]
        </output>
        """

        # Format the prompt with the current thoughts and instructions
        cot_prompt = f"""{system_prompt}\n{thoughts}\n{knowledge_triplets_instructions}"""
\end{lstlisting}
\end{tcolorbox}
\end{figure*}

% Second part of the code in figure* continuing from the previous page
\begin{figure*}[htbp]
\begin{tcolorbox}[colback=codebg, colframe=black, title=Atomic Triples Extraction (Part 2), sharp corners, boxrule=1pt]
\begin{lstlisting}[style=mypython]        
        # Make the API call with exception handling
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": cot_prompt},
                    {"role": "user", "content": current_query}
                ],
                temperature=0.7,
                max_tokens=4096
            )
            full_response = response.choices[0].message.content
        except Exception as e:
            print(f"API call failed: {e}")
            return None

        # Use regex to extract content within <triple_extraction> and <output> tags
        triple_match = re.search(r"<triple_extraction>(.*?)</triple_extraction>", 
                                 full_response, re.DOTALL)
        output_match = re.search(r"<output>(.*?)</output>", full_response, re.DOTALL)

        triple_extraction = triple_match.group(1).strip() if triple_match else None
        output = output_match.group(1).strip() if output_match else None

        # If an <output> tag is found, return the output and stop
        if output:
            return output
        elif triple_extraction:
            current_query = triple_extraction
        else:
            break

    # Default fallback if max iterations are reached or no valid output is found
    return full_response if return_full_response else output

def get_triples_from_text(model_name, input_text, num_triples):
    """
    Extracts knowledge triples from the input text using a CoT reflection logic.
    """
    # Define the system prompt and initial query
    system_prompt = (
        "You are a helpful assistant for extracting knowledge triples from text "
        "using the Subject-Predicate-Object (SPO) method for knowledge graph construction. "
        f"Analyze the text step by step and provide exactly {num_triples} SPO triplets."
    )
    initial_query = input_text

    # Run the CoT reflection logic iteratively
    final_response = cot_reflection_for_triples(
        system_prompt=system_prompt,
        initial_query=initial_query,
        client=client,
        model=model_name,
        num_triples=num_triples
    )

    return final_response
\end{lstlisting}
\end{tcolorbox}
\vspace{-5mm}
\caption{Python Code Code for extracting subject-predicate-object (SPO) triples using Chain-of-Thought (CoT) reflection. The user sets the number of triples via `num\_triples'. The process includes generating prompts, interacting with a language model, validating results, and returning the final triples. }
\label{fig:Code2}
\end{figure*}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Hypothesis Generation Agent with Reflection, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Analyze inventor’s input and Patent Knowledge Graphs (PKGs) to identify technology gaps, trends, and relevant prior inventions, using the APHD algorithm for Hypothesis Exploration. Present the final hypothesis in an SAO triplet format.
%
%\begin{itemize}
%    \item \textbf{Step 1: Analyze Inventor’s Input}
%    \begin{itemize}
%        \item Review problem statements, ideas, and challenges from the inventor.
%        \item Identify constraints, requirements, or relevant insights for the invention.
%    \end{itemize}
%
%    \item \textbf{Step 2: Extract Insights from PKGs}
%    \begin{itemize}
%        \item Search PKGs to find related patents, technologies, and concepts.
%        \item Identify trends, gaps, and prior inventions connected to the inventor's problem.
%        \item Highlight unsolved problems or gaps the hypothesis should address.
%    \end{itemize}
%
%    \item \textbf{Step 3: Link PKG Insights with Inventor’s Input}
%    \begin{itemize}
%        \item Combine PKG insights with the inventor’s input to determine:
%        \begin{itemize}
%            \item Which gaps or trends align with the inventor's ideas.
%            \item How prior inventions relate to the problem, suggesting improvements or new directions.
%        \end{itemize}
%    \end{itemize}
%
%    \item \textbf{Step 4: Invoke APHD Algorithm to Explore PHEGs}
%    \begin{itemize}
%        \item Apply the APHD algorithm on PHEGs.
%        \item Extract SAO triplets from exploration paths to identify new patterns or hypotheses.
%        \item Log exploration paths for reflection.
%    \end{itemize}
%
%    \item \textbf{Step 5: Formulate Hypothesis in SAO Format}
%    \begin{itemize}
%        \item Structure the hypothesis using this SAO format:
%        \begin{verbatim}
%<sao_triplet>
%Subject: [Core invention or technology]
%Action: [Process or method applied]
%Object: [Intended outcome or product]
%</sao_triplet>
%        \end{verbatim}
%    \end{itemize}
%    \vspace{-5mm}
%    \item \textbf{Step 6: Reflect on and Improve the Hypothesis}
%    \begin{itemize}
%        \item Review the SAO triplet for missing details, gaps, or overlaps with prior inventions.
%        \item Refine the hypothesis for novelty, feasibility, and non-obviousness.
%        \item Ensure uniqueness and alignment with trends using PKGs.
%    \end{itemize}
%
%    \item \textbf{Step 7: Iterate Until Hypothesis is Finalized}
%    \begin{itemize}
%        \item Refine until the hypothesis meets the following criteria:
%        \begin{itemize}
%            \item Novelty: Introduces new concepts or improvements.
%            \item Feasibility: Practically achievable.
%            \item Non-obviousness: Not an obvious extension of existing knowledge.
%        \end{itemize}
%        \item Stop after meeting criteria or 10 iterations.
%    \end{itemize}
%
%    \item \textbf{Step 8: Generate the Final Output}
%    \begin{itemize}
%        \item Provide the finalized hypothesis as directed, ensuring no intermediate steps are shown.
%    \end{itemize}
%\end{itemize}
%
%\textbf{Output Format}
%\begin{itemize}
%    \item Only include the final hypothesis in:
%    \begin{verbatim}
%<output>
%Hypothesis: [Provide the final hypothesis here]
%</output>
%    \end{verbatim}
%\end{itemize}
%\vspace{-4mm}
%\textbf{Notes}
%\begin{itemize}
%    \item Do not display intermediate SAO triplets, reflections, or path logs.
%    \item Ensure the hypothesis is concise and ready for validation by the scientific discovery agent.
%\end{itemize}
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Scientific Discovery Agent with Reflection, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Validate, refine, and improve hypotheses using scholarly-article-based knowledge graphs (ScKGs) to ensure scientific validity and readiness for patent ideation. Utilize the hypotheses provided by the hypothesis generation agent and follow the structured steps to enhance these hypotheses through scientific evaluation.
%
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{1. Receive Input from the Hypothesis Generation Agent}
%    \begin{itemize}
%        \item Review hypotheses, identify gaps, and assess trends provided by the hypothesis generation agent using PKGs.
%    \end{itemize}
%
%    \item \textbf{2. Explore Scholarly-Article-Based Knowledge Graphs (ScKGs)}
%    \begin{itemize}
%        \item Search ScKGs to find relevant scientific data, experimental methods, or breakthroughs.
%        \item Uncover evidence or concepts that support the hypothesis.
%        \item Identify further experiments or data needed for refinement.
%    \end{itemize}
%
%    \item \textbf{3. Validate and Reflect on the Hypothesis}
%    \begin{itemize}
%        \item Align the hypothesis with scientific data from ScKGs.
%        \item Identify gaps or inconsistencies and log reflections using the following format:
%        \begin{verbatim}
%<reflection>
%[Identify gaps, inconsistencies, or potential improvements 
%based on ScKG findings]
%</reflection>
%        \end{verbatim}
%        \vspace{-3mm}
%        \item Refine the hypothesis to ensure novelty, feasibility, and non-obviousness.
%    \end{itemize}
%
%    \item \textbf{4. Iterate Until the Hypothesis is Refined and Validated}
%    \begin{itemize}
%        \item Use reflections for iterative improvement of the hypothesis.
%        \item Repeat the process until the hypothesis is valid. If necessary, re-engage with the hypothesis generation agent.
%    \end{itemize}
%
%    \item \textbf{5. Generate Final Scientific Validation Output}
%    \begin{itemize}
%        \item Provide the refined and validated hypothesis using the following format:
%        \begin{verbatim}
%<output>
%[Provide the validated hypothesis here]
%</output>
%        \end{verbatim}
%        \vspace{-3mm} 
%        \item Ensure alignment with scientific evidence and readiness for patent ideation transition.
%    \end{itemize}
%\end{itemize}
%
%\textbf{Output Format}
%\begin{itemize}
%    \item Log reflections during validation using the `$<$reflection$>$` format.
%    \item Deliver the finalized hypothesis using the `$<$output$>$` format.
%    \item Omit reflections in the final output to present only the validated hypothesis.
%\end{itemize}
%
%\textbf{Notes}
%\begin{itemize}
%    \item Each iteration must improve the hypothesis.
%    \item Critical gaps should prompt re-engagement with the hypothesis generation agent.
%    \item The output hypothesis must be scientifically grounded and structured for integration into patent ideation workflows.
%\end{itemize}
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Ideation Agent with Reflection, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Transform validated hypotheses from scientific discovery agents into practical, novel, and patentable inventions. These inventions must address the inventor’s challenges, align with technological trends, and be prepared for patent drafting.
%
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{Receive and Review Inputs}
%    \begin{itemize}
%        \item Review hypotheses, insights, and identified gaps from scientific discovery agents.
%        \item Understand the inventor’s problem statements, challenges, and desired outcomes.
%    \end{itemize}
%
%    \item \textbf{Transform Hypotheses into Patentable Inventions}
%    \begin{itemize}
%        \item Develop innovative solutions that address identified gaps and align with the inventor’s goals.
%        \item Ensure inventions are feasible, novel, and non-obvious, considering current technological and scientific landscapes.
%        \item Avoid overlaps with existing patented technologies and ensure the inventions are marketable and protectable.
%    \end{itemize}
%
%    \item \textbf{Iterate and Reflect to Improve the Invention}
%    \begin{itemize}
%        \item Identify necessary improvements or adjustments through reflection.
%        \item Iterate until the invention meets feasibility, novelty, and non-obviousness criteria.
%    \end{itemize}
%
%    \item \textbf{Organize and Document the Invention for Patent Drafting}
%    \begin{itemize}
%        \item Provide a concise, well-defined description including:
%        \begin{itemize}
%            \item Purpose and objectives of the invention.
%            \item Technical specifications and unique features.
%            \item Functionalities and how the invention solves identified challenges.
%        \end{itemize}
%    \end{itemize}
%
%    \item \textbf{Provide Final Output}
%    \begin{itemize}
%        \item Format the validated invention using the following structure:
%        \begin{verbatim}
%<output>
%[Provide the final, refined invention description with necessary details]
%</output>
%        \end{verbatim}
%    \vspace{-3mm}    
%    \end{itemize}
%
%    \item \textbf{Provide JSON Output for Patent Drafting}
%    \begin{verbatim}
%{
%    "validated_invention": "Detailed description of the invention",
%    "purpose": "Purpose and objectives of the invention",
%    "novelty": "Explanation of what makes the invention novel",
%    "feasibility": "Justification of the technical feasibility",
%    "non_obviousness": "Rationale demonstrating non-obviousness"
%}
%    \end{verbatim}
%    \vspace{-3mm}
%\end{itemize}
%
%\textbf{Output Format}
%\begin{itemize}
%    \item The validated invention should be provided in a detailed descriptive paragraph format.
%    \item Final output should be formatted as JSON for patent drafting.
%\end{itemize}
%
%\textbf{Important Instructions}
%\begin{itemize}
%    \item Reflections should not be included in the final output.
%    \item Ensure the invention description is concise, aligned with the inventor’s goals, and ready for meta-agent coordination for patent drafting.
%\end{itemize}
%
%\textbf{Goal:}  
%Deliver a practical and patentable invention that addresses the inventor’s challenges, aligns with technological trends and scientific advancements, and meets the criteria of feasibility, novelty, and non-obviousness.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Prior Art Search Agent, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Conduct a comprehensive prior art search across PKGs and scientific literature to identify overlaps, similar solutions, and technological trends. Assess the novelty, non-obviousness, and feasibility of the invention proposed by the ideation agent. Provide structured feedback focused on prior art findings to assist the patentability evaluation agent.
%
%\vspace{3mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{1. Analyze the Refined Invention}
%    \begin{itemize}
%        \item Review the proposed invention, including:
%        \begin{itemize}
%            \item Purpose, objectives, and technical specifications.
%            \item Functionalities, unique features, and challenges addressed.
%        \end{itemize}
%    \end{itemize}
%
%    \item \textbf{2. Conduct a Comprehensive Prior Art Search}
%    \begin{itemize}
%        \item Search PKGs and scientific literature for relevant prior solutions.
%        \item Identify existing patents, related inventions, and trends in the domain.
%        \item Highlight overlaps, similarities, or technological gaps addressed by the proposed invention.
%    \end{itemize}
%
%    \item \textbf{3. Assess and Reflect on Prior Art Findings}
%    \begin{itemize}
%        \item Compare the proposed invention with identified prior art.
%        \item Reflect on similarities, differences, and gaps:
%        \begin{verbatim}
%<reflection>
%[Identify relevant overlaps, gaps, or trends]
%</reflection>
%        \end{verbatim}
%        \vspace{-2mm}
%        \item Assess if the proposed invention is novel, non-obvious, and feasible.
%    \end{itemize}
%
%    \item \textbf{4. Generate JSON Output for Meta-Agent}
%    \begin{verbatim}
%{
%    "invention_summary": "Summary of the proposed invention",
%    "prior_art_summary": "Relevant prior art findings",
%    "novelty_and_non_obviousness_assessment": 
%        "Assessment of novelty, non-obviousness, and gaps",
%}
%    \end{verbatim}
%\end{itemize}
%
%\vspace{-2mm}
%\textbf{Important Instructions}
%\begin{itemize}
%    \item Focus reflections on prior art findings, such as overlaps, trends, and technological gaps.
%    \item Do not refine the invention or hypothesis.
%    \item The output will assist the patentability evaluation agent in validating the invention against prior solutions.
%\end{itemize}
%
%\vspace{3mm}
%\textbf{Goal:}  
%Evaluate the proposed invention through a thorough prior art search. Provide actionable insights to support the patentability evaluation agent in determining the novelty, feasibility, and patentability of the invention.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Patentability Evaluation Agent, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Assess the patentability of the proposed invention based on **novelty, non-obviousness, feasibility, and utility**. Use insights from the prior art search agent and provide reflective feedback if further refinement is required. Notify the meta-agent if the invention does not meet the criteria, enabling iterative refinement by other agents.
%
%\vspace{3mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{1. Review the Invention and Supporting Insights}
%    \begin{itemize}
%        \item Analyze the invention’s description, including:
%        \begin{itemize}
%            \item Purpose, objectives, and technical specifications.
%            \item Functionalities, unique features, and identified challenges.
%        \end{itemize}
%        \item Integrate prior art insights, focusing on identified overlaps, gaps, and trends.
%    \end{itemize}
%
%    \item \textbf{2. Evaluate Patentability Criteria}
%    \begin{itemize}
%        \item Assess the invention against the following criteria:
%        \begin{itemize}
%            \item Novelty: Confirm the invention introduces features not found in prior art.
%            \item Non-obviousness: Ensure it is not an obvious extension of existing solutions.
%            \item Feasibility: Verify that the invention is technically and practically achievable.
%            \item Utility: Ensure it has practical or industrial applications.
%        \end{itemize}
%    \end{itemize}
%
%    \item \textbf{3. Provide Reflective Feedback and Coordinate Iteration}
%    \begin{itemize}
%        \item If the invention meets the criteria, confirm it is ready for patent drafting.
%        \item If it does not meet the criteria, provide reflective feedback to guide improvements:
%        \begin{verbatim}
%<feedback>
%[Reflection: Highlight areas for improvement, such as novelty, 
%non-obviousness, feasibility, or utility, referencing specific 
%gaps or overlaps identified in prior art.]
%</feedback>
%        \end{verbatim}
%        \vspace{-3mm}
%        \item Notify the meta-agent to coordinate further iterations with relevant agents.
%    \end{itemize}
%
%    \item \textbf{4. Generate JSON Output for Meta-Agent}
%    \begin{verbatim}
%{
%    "invention_summary": "Summary of the invention evaluated",
%    "novelty_assessment": "Assessment of novelty",
%    "non_obviousness_assessment": "Assessment of non-obviousness",
%    "feasibility_assessment": "Assessment of feasibility",
%    "utility_assessment": "Assessment of utility",
%    "prior_art_references": "Relevant prior art findings",
%    "reflection": "Suggested improvements for iteration (if applicable)",
%    "feedback": "Suggestions for refinement based on patentability criteria"
%}
%    \end{verbatim}
%     \vspace{-3mm}
%\end{itemize}
%
%\textbf{Important Instructions}
%\begin{itemize}
%    \item Ensure reflective feedback aligns with the patentability criteria and references prior art.
%    \item Notify the meta-agent if further iterations are required for refinement.
%    \item The output will inform hypothesis generation for continued improvement.
%\end{itemize}
%
%\textbf{Goal:}  
%Deliver a comprehensive evaluation of the invention’s patentability. Ensure it meets the criteria of novelty, non-obviousness, feasibility, and utility, providing reflective feedback for iterative refinement if necessary.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,
%    colframe=black!70,
%    title=Instructional Prompt for Background Writing Agent, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Draft the background section of the patent application by synthesizing outputs from the prior-art search agent and ideation agent. The section should introduce the field, review existing solutions, identify gaps, define the problem, and justify the invention’s need. Use reflection to ensure the background aligns with prior-art findings and is suitable for the patent drafting process.
%
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{1. Introduce the Technological Field}
%    \begin{itemize}
%        \item State the general field or technological area related to the invention.
%        \item Provide relevant context to help readers understand the invention's application.
%    \end{itemize}
%
%    \item \textbf{2. Review Prior Art and Existing Solutions}
%    \begin{itemize}
%        \item Summarize relevant patents, technologies, or solutions identified by the prior-art search agent.
%        \item Highlight patterns and trends in existing solutions.
%    \end{itemize}
%
%    \item \textbf{3. Identify Limitations or Gaps in Prior Art}
%    \begin{itemize}
%        \item Discuss inefficiencies, challenges, or unmet needs in existing solutions.
%        \item Highlight technological gaps relevant to the inventor's challenge.
%    \end{itemize}
%
%    \item \textbf{4. Define the Problem to be Solved}
%    \begin{itemize}
%        \item Clearly state the technical problem or challenge addressed by the invention.
%        \item Ensure alignment between the identified problem and the gaps in prior art.
%    \end{itemize}
%
%    \item \textbf{5. Justify the Need for the Invention}
%    \begin{itemize}
%        \item Explain how the invention addresses the identified problem and gaps.
%        \item Emphasize the invention’s novelty, relevance, and value within the current technological landscape.
%    \end{itemize}
%
%    \item \textbf{6. Reflect on and Improve the Background Section}
%    \begin{itemize}
%        \item Use reflection to assess the completeness and alignment of the background section.
%        \item Identify missing details or inconsistencies between prior art insights and the invention:
%        \begin{verbatim}
%<reflection>
%[Identify areas for improvement or missing details in 
%the background section based on prior art and invention alignment]
%</reflection>
%        \end{verbatim}
%        \vspace{-3mm}
%        \item Iterate until the background section is well-aligned with prior-art findings and addresses the invention's relevance comprehensively.
%    \end{itemize}
%
%    \item \textbf{7. Generate the Final Background Section}
%    \begin{verbatim}
%<background>
%[Comprehensive description of the field, prior art, gaps, 
%problem, and need for the invention]
%</background>
%    \end{verbatim}
%    \vspace{-3mm}
%    \item \textbf{8. Provide Structured JSON Output}
%    \begin{verbatim}
%{
%    "field": "General field of the invention",
%    "prior_art": "Summary of existing technologies and patents",
%    "gaps": "Identified limitations in prior solutions",
%    "problem": "Specific problem to be solved",
%    "justification": "Reason why the invention is needed",
%    "reflection": "Insights on any areas for improvement"
%}
%    \end{verbatim}
%    \vspace{-3mm}
%\end{itemize}
%\vspace{-3mm}
%\textbf{Important Instructions}  
%\begin{itemize}
%    \item Ensure the background section aligns with prior-art insights and contextualizes the invention for patent drafting.
%    \item Use reflection to identify and address any missing elements or inconsistencies.
%    \item Keep the section comprehensive but concise to support the patent application effectively.
%\end{itemize}
%
%\textbf{Goal:}  
%Deliver a background section that introduces the field, reviews prior solutions, identifies gaps, defines the problem, and justifies the invention’s relevance. Ensure the section is refined through reflection to align with prior-art findings and support seamless patent drafting.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray, 
%    colframe=black!70, 
%    title=Instructional Prompt for Detailed Description Agent, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Generate the detailed description section of the patent, providing comprehensive technical details on the invention’s features, components, and implementation. Ensure the content aligns with the inventor’s challenges, insights from previous agents, and supports claim drafting. Use reflection to identify gaps, inconsistencies, or areas for refinement.
%
%\vspace{1mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{1. Receive Inputs from Prior Agents}
%    \begin{itemize}
%        \item Review problem statements, hypotheses, and solutions from the hypothesis generation, scientific discovery, and ideation agents.
%        \item Ensure alignment with the background section and prepare the content for claim generation.
%    \end{itemize}
%
%    \item \textbf{2. Describe Core Features and Components}
%    \begin{itemize}
%        \item Identify the key components of the invention.
%        \item Provide detailed explanations of each component’s function and role.
%    \end{itemize}
%
%    \item \textbf{3. Explain Component Interactions and Functionality}
%    \begin{itemize}
%        \item Describe how components interact and function together.
%        \item Include diagrams or schematics, if needed, to clarify critical interactions.
%    \end{itemize}
%
%    \item \textbf{4. Elaborate on Functionalities}
%    \begin{itemize}
%        \item Explain how the invention achieves its objectives through its functionalities.
%        \item Connect functionalities to potential claim elements.
%    \end{itemize}
%
%    \item \textbf{5. Provide Technical Specifications}
%    \begin{itemize}
%        \item Include relevant technical specifications, materials, or configurations.
%        \item Highlight unique configurations that support claim drafting.
%    \end{itemize}
%
%    \item \textbf{6. Include Use Cases or Examples}
%    \begin{itemize}
%        \item Provide scenarios or examples demonstrating the invention’s use.
%        \item Ensure examples align with technical specifications and potential claims.
%    \end{itemize}
%
%    \item \textbf{7. Reflect and Improve the Detailed Description}
%    \begin{itemize}
%        \item Use reflection to assess the completeness and coherence of the description.
%        \item Identify gaps, inconsistencies, or areas needing refinement:
%        \begin{verbatim}
%<reflection>
%[Identify missing elements, inconsistencies, or improvements 
%needed in the description]
%</reflection>
%        \end{verbatim}
%        \vspace{-4mm}
%        \item Iterate and refine until the description is aligned with prior inputs and ready for claim generation.
%    \end{itemize}
%
%    \item \textbf{8. Generate the Final Detailed Description}
%    \begin{verbatim}
%<detailed_description>
%[Comprehensive description of features, components, 
%functionalities, and technical details]
%</detailed_description>
%    \end{verbatim}
%    \vspace{-5mm}
%    \item \textbf{9. Provide JSON Output for Structured Information}
%    \begin{verbatim}
%    "components": "List and description of components",
%    "interactions": "Explanation of how components interact",
%    "functionalities": "Detailed functionalities of the invention",
%    "technical_specifications": "Technical specifications for implementation",
%    "use_cases": "Examples demonstrating usage",
%    "claimable_features": "Key features supporting claim generation",
%    "reflection": "Insights on areas needing improvement"
%    \end{verbatim}
%    \vspace{-4mm}
%\end{itemize}
%
%\vspace{-3mm}
%\textbf{Important Instructions}  
%\begin{itemize}
%    \item Ensure the description aligns with outputs from prior agents and is ready for claim drafting.
%    \item Use reflection to refine the description iteratively.
%    \item Avoid unnecessary elaboration while maintaining clarity.
%\end{itemize}
%
%\textbf{Goal:}  
%Deliver a comprehensive, technically accurate description that covers components, interactions, functionalities, and specifications. Ensure alignment with prior insights and iterative refinement through reflection to provide a solid foundation for claim generation and seamless patent drafting.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,            
%    colframe=black!70,                
%    title=\textbf{Instructional Prompt for Claim Generation Agent}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Your task is to generate precise, enforceable patent claims that define the legal scope of the invention. These claims must reflect the novelty, feasibility, and utility captured in the detailed description, background, and novel hypothesis. Use legal language to ensure clarity and consistency across independent and dependent claims. Incorporate reflection to identify gaps or ambiguities for refinement.
%
%\begin{itemize}
%    \item \textbf{Step 1: Review Inputs from Prior Agents}
%    \begin{itemize}
%        \item Examine the \textbf{detailed description} to identify components, functionalities, and technical features.
%        \item Review the \textbf{background section} to ensure alignment with prior art and existing solutions.
%        \item Integrate insights from the \textbf{novel hypothesis} to capture the invention’s novelty, feasibility, and utility.
%    \end{itemize}
%
%    \item \textbf{Step 2: Draft Independent Claims}
%    \begin{itemize}
%        \item Write broad claims that define the essential elements of the invention.
%        \item Ensure independent claims stand alone without referencing other claims.
%        \item Capture the core components and functionalities, balancing scope with enforceability to cover potential variations.
%    \end{itemize}
%
%    \item \textbf{Step 3: Draft Dependent Claims}
%    \begin{itemize}
%        \item Write dependent claims that narrow or specify independent claims.
%        \item Include additional features such as specific configurations, modes of operation, or optional elements.
%        \item Ensure coherence by referencing the appropriate independent claims.
%    \end{itemize}
%
%    \item \textbf{Step 4: Ensure Legal Precision and Clarity}
%    \begin{itemize}
%        \item Use precise legal language to avoid ambiguity (e.g., avoid vague terms like “about” or “essentially”).
%        \item Ensure the claims are comprehensive to prevent loopholes or unintended interpretations.
%    \end{itemize}
%
%    \item \textbf{Step 5: Align Claims with Patentability Criteria}
%    \begin{itemize}
%        \item Verify that claims reflect \textbf{novelty, non-obviousness, feasibility, and utility}.
%        \item Ensure consistency with the detailed description and background sections.
%    \end{itemize}
%
%    \item \textbf{Step 6: Incorporate Reflection for Gaps and Ambiguities}
%    \begin{itemize}
%        \item Review the drafted claims to identify missing elements, overlaps, or unclear aspects:
%\begin{verbatim}
%<reflection>
%[Identify any gaps, ambiguities, or missing elements in the claims]
%</reflection>
%\end{verbatim}
%        \item Use reflections to refine the claims iteratively until they meet required legal standards.
%    \end{itemize}
%
%    \item \textbf{Step 7: Generate the Final Claims Section}
%    \begin{itemize}
%        \item Provide the claims in the following format:
%\begin{verbatim}
%<claims>
%1. An [invention] comprising: [essential components and functionalities].
%2. The invention of claim 1, wherein: [additional specific features or limitations].
%3. The invention of claim 1, further comprising: [optional elements or configurations].
%</claims>
%\end{verbatim}
%    \end{itemize}
%
%    \item \textbf{Step 8: Provide Structured JSON Output}
%    \begin{itemize}
%        \item Output the claims in JSON format:
%\begin{verbatim}
%{
%  "independent_claims": [
%    "An [invention] comprising: [essential components and functionalities]."
%  ],
%  "dependent_claims": [
%    "The invention of claim 1, wherein: [specific features or limitations].",
%    "The invention of claim 1, further comprising: [optional elements or configurations]."
%  ]
%}
%\end{verbatim}
%    \end{itemize}
%\end{itemize}
%
%\textbf{Important Instructions:}  
%Ensure claims align with the detailed description, background, and hypothesis. Use reflection to identify and resolve ambiguities or gaps. Claims must be legally precise, comprehensive, and enforceable.
%
%\vspace{2mm}
%\textbf{Goal:}  
%Deliver a set of well-structured claims defining the invention's legal boundaries, ensuring alignment with prior sections and reflecting novelty, feasibility, and utility.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,           
%    colframe=black!70,                
%    title=\textbf{Instructional Prompt for Summary Generation Agent with Reflection}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled,
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Generate a concise, high-level summary that accurately captures the invention’s key aspects, advantages, and alignment with identified challenges. The summary should provide a seamless transition between the technical description and legal claims, ensuring clarity for stakeholders. Use reflection to ensure consistency, identify missing elements, and correct ambiguities.
%
%\vspace{2mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{Step 1: Review Inputs from Prior Agents}  
%    \begin{itemize}
%        \item Examine the \textbf{background section} for context on the problem, prior art, and identified gaps.
%        \item Review the \textbf{detailed description} to capture core features, components, and functionalities.
%        \item Ensure the summary reflects the legal scope captured in the \textbf{claims}.
%    \end{itemize}
%
%    \item \textbf{Step 2: Outline the Key Aspects of the Invention}  
%    \begin{itemize}
%        \item Summarize the invention’s purpose, objectives, and innovative elements.
%        \item Highlight essential technical features and functionalities.
%    \end{itemize}
%
%    \item \textbf{Step 3: Emphasize Key Advantages}  
%    \begin{itemize}
%        \item Explain how the invention addresses the identified challenges or gaps.
%        \item Highlight advantages such as improved performance, cost-efficiency, or ease of use.
%    \end{itemize}
%
%    \item \textbf{Step 4: Summarize Broader Applications and Impact}  
%    \begin{itemize}
%        \item Describe how the invention aligns with technological trends and market needs.
%        \item Outline potential impacts on the industry or field.
%    \end{itemize}
%
%    \item \textbf{Step 5: Incorporate Reflection for Improvement}  
%    \begin{itemize}
%        \item Identify any missing elements, ambiguities, or misalignments:
%        \begin{verbatim}
%<reflection>
%[Identify areas for improvement or missing elements]
%</reflection>
%        \end{verbatim}
%        \vspace{-3mm}
%        \item Use reflections to iteratively refine the summary until it is concise and complete.
%    \end{itemize}
%
%    \item \textbf{Step 6: Generate the Final Summary}  
%    \begin{verbatim}
%<summary>
%[Provide a concise summary of the invention, highlighting its key aspects, 
%advantages, and how it addresses challenges.]
%</summary>
%    \end{verbatim}
%     \vspace{-4mm} 
%    \item \textbf{Step 7: Provide Structured JSON Output}  
%    \begin{verbatim}
%{
%    "summary": "Concise summary of the invention",
%    "key_aspects": "Overview of core features",
%    "advantages": "Key advantages of the invention",
%    "impact": "Potential industry impact",
%    "challenges_addressed": "Challenges the invention solves",
%}
%    \end{verbatim}
%    \vspace{-4mm}
%\end{itemize}
%
%\vspace{0mm}
%\textbf{Important Instructions}  
%\begin{itemize}
%    \item Ensure the summary is aligned with the detailed description, background, and claims.
%    \item Use reflection to identify and address missing elements or inconsistencies.
%    \item Maintain conciseness while ensuring clarity for stakeholders.
%\end{itemize}
%
%\textbf{Goal:}  
%Deliver a concise, high-level summary that captures the invention’s key aspects, advantages, and alignment with challenges. The summary should bridge the technical details and legal claims, ensuring seamless communication within the patent application process.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray, 
%    colframe=black!70, 
%    title=\textbf{Instructional Prompt for Abstract Generation Agent with Reflection}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Draft a concise abstract that provides an overview of the invention, including its purpose, key features, functionalities, and applications. The abstract should align with the technical and legal context outlined in the detailed description, claims, and summary, and serve as an accurate, high-level representation of the invention.
%
%\vspace{2mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{Step 1: Review Inputs from Prior Agents}  
%    \begin{itemize}
%        \item Analyze the \textbf{detailed description} to capture the invention's core features and functionalities.
%        \item Review the \textbf{claims} to align the abstract with the legal scope of the patent.
%        \item Use insights from the \textbf{summary} to ensure the abstract reflects the invention’s advantages and applications.
%    \end{itemize}
%
%    \item \textbf{Step 2: Identify Core Elements for the Abstract}  
%    \begin{itemize}
%        \item Highlight the invention’s purpose, primary components, and main functionalities.
%        \item Identify unique features that differentiate the invention from prior art.
%        \item Ensure alignment with the broader field of application or industry relevance.
%    \end{itemize}
%
%    \item \textbf{Step 3: Draft a Concise Abstract}  
%    \begin{itemize}
%        \item Keep the abstract concise (typically fewer than 150 words) while accurately representing the invention.
%        \item Avoid jargon and ensure clarity, making the abstract accessible to a broad audience.
%        \item Ensure the abstract reflects the novelty and utility of the invention.
%    \end{itemize}
%
%    \item \textbf{Step 4: Align with Patent Guidelines}  
%    \begin{itemize}
%        \item Ensure the abstract accurately reflects the claimed invention without suggesting unclaimed elements.
%        \item Verify consistency between the abstract, detailed description, and claims.
%    \end{itemize}
%
%    \item \textbf{Step 5: Use Reflection to Refine the Abstract}  
%    \begin{itemize}
%        \item Review for gaps, inconsistencies, or missing elements:
%\begin{verbatim}
%<reflection>
%[Identify any missing aspects or improvements needed in the abstract]
%</reflection>
%\end{verbatim}
%        \item Use reflections to refine the abstract iteratively until it aligns with prior inputs and patent standards.
%    \end{itemize}
%
%    \item \textbf{Step 6: Generate the Final Abstract}  
%    \begin{verbatim}
%<abstract>
%[Provide a concise and complete overview of the invention]
%</abstract>
%    \end{verbatim}
%
%    \item \textbf{Step 7: Provide JSON Output}  
%    \begin{verbatim}
%{
%    "abstract": "Concise and descriptive overview of the invention"
%}
%    \end{verbatim}
%\end{itemize}
%
%\vspace{2mm}
%\textbf{Important Instructions:}  
%Ensure the abstract aligns with the invention's scope and purpose, as outlined in the detailed description, claims, and summary. It should provide a clear, concise overview while adhering to patent requirements.
%
%\vspace{2mm}
%\textbf{Goal:}  
%Deliver a well-structured abstract that captures the essence of the invention, ensuring clarity, alignment, and compliance with patent standards. The abstract should facilitate seamless communication of the invention’s value and purpose.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray, 
%    colframe=black!70, 
%    title=\textbf{Instructional Prompt for Title Generation Agent with Reflection}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Generate a concise and descriptive title for the patent application that accurately reflects the invention’s purpose, components, and functionalities. Ensure the title aligns with the technical and legal context, representing the invention’s scope effectively.
%
%\vspace{2mm}
%\textbf{Steps}
%
%\begin{itemize}
%    \item \textbf{Step 1: Review Inputs from Prior Agents}  
%    \begin{itemize}
%        \item Review the \textbf{background section} to understand the problem and technological gaps.
%        \item Capture the invention's \textbf{core features} from the detailed description.
%        \item Align the title with the \textbf{claims} to ensure legal consistency.
%        \item Use insights from the \textbf{summary} to reflect key aspects and advantages.
%    \end{itemize}
%
%    \item \textbf{Step 2: Identify Core Elements for the Title}  
%    \begin{itemize}
%        \item Capture the invention’s primary purpose and functionalities.
%        \item Identify key components that distinguish the invention from prior art.
%        \item Ensure alignment with the invention's technical field.
%    \end{itemize}
%
%    \item \textbf{Step 3: Formulate a Concise and Descriptive Title}  
%    \begin{itemize}
%        \item Write a concise title (typically under 20 words) that conveys the invention’s nature.
%        \item Avoid jargon or overly technical terms for clarity.
%        \item Highlight the novelty and unique features of the invention.
%    \end{itemize}
%
%    \item \textbf{Step 4: Ensure Legal Consistency}  
%    \begin{itemize}
%        \item Verify that the title does not imply unclaimed elements or features.
%        \item Ensure the title reflects the claims and aligns with the invention’s scope.
%    \end{itemize}
%
%    \item \textbf{Step 5: Use Reflection to Refine the Title}  
%    \begin{itemize}
%        \item Identify any inconsistencies or missing elements:
%        \begin{verbatim}
%<reflection>
%[Identify areas for improvement or missing elements in the title]
%</reflection>
%        \end{verbatim}
%        \item Use reflections to refine the title iteratively until it aligns perfectly.
%    \end{itemize}
%
%    \item \textbf{Step 6: Generate the Final Title}  
%    \begin{verbatim}
%<title>
%[Concise and descriptive title reflecting the invention’s purpose and scope]
%</title>
%    \end{verbatim}
%
%    \item \textbf{Step 7: Provide JSON Output}  
%    \begin{verbatim}
%{
%    "title": "Precise and descriptive title of the invention"
%}
%    \end{verbatim}
%\end{itemize}
%
%\vspace{2mm}
%\textbf{Important Instructions:}  
%Ensure the title aligns with the invention’s scope as outlined in the detailed description, claims, and summary. It must be concise, legally sound, and accurately reflect the invention’s purpose.
%
%\vspace{2mm}
%\textbf{Goal:}  
%Deliver a well-structured title that captures the essence of the invention, supporting the clarity and legal scope of the patent.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray, 
%    colframe=black!70, 
%    title=\textbf{Instructional Prompt for Legal Compliance Agent with Reflection}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Ensure that the entire patent application is legally compliant with patent office rules, verifying consistency, completeness, clarity, and formal accuracy. Review the alignment between sections (title, abstract, detailed description, and claims) to ensure they meet key requirements such as novelty, non-obviousness, and utility.
%
%\textbf{Steps}  
%\begin{itemize}
%    \item \textbf{1. Review Inputs from Prior Agents}  
%    \begin{itemize}
%        \item Examine the title, abstract, and summary for completeness and legal alignment.
%        \item Analyze the detailed description and claims to ensure accuracy and consistency.
%        \item Confirm the application reflects novelty and non-obviousness.
%    \end{itemize}
%
%    \item \textbf{2. Verify Consistency Across Sections}  
%    \begin{itemize}
%        \item Ensure alignment between the title, abstract, detailed description, and claims.
%        \item Confirm that the claims are adequately supported by the detailed description.
%    \end{itemize}
%
%    \item \textbf{3. Check Legal Compliance and Formal Requirements}  
%    \begin{itemize}
%        \item Verify the application meets patent office standards (e.g., terminology, structure).
%        \item Ensure all claims satisfy requirements for novelty, non-obviousness, and utility.
%    \end{itemize}
%
%    \item \textbf{4. Identify Potential Legal Issues or Risks}  
%    \begin{itemize}
%        \item Highlight inconsistencies or ambiguities that may affect legal enforceability.
%        \item Identify missing elements that could lead to rejection during the examination.
%    \end{itemize}
%
%    \item \textbf{5. Provide Feedback and Reflection for Refinement}  
%    \begin{itemize}
%        \item Offer detailed feedback for each section that needs revision or refinement.
%        \item Collaborate with other agents to resolve issues and enhance compliance:
%\begin{verbatim}
%<reflection>
%[Identify issues, inconsistencies, or missing elements 
%that need refinement or improvement]
%</reflection>
%\end{verbatim}
%    \end{itemize}
%
%    \item \textbf{6. Generate Legal Compliance Report}  
%    \begin{verbatim}
%<legal_compliance_report>
%- Title: [Feedback on title compliance]
%- Abstract: [Feedback on abstract compliance]
%- Claims: [Feedback on claim structure and compliance]
%- Description: [Feedback on description alignment]
%- Legal Risks: [Identified risks and mitigation suggestions]
%</legal_compliance_report>
%    \end{verbatim}
%
%    \item \textbf{7. Provide Structured JSON Output}  
%    \begin{verbatim}
%{
%    "title_compliance": "Feedback on title compliance",
%    "abstract_compliance": "Feedback on abstract compliance",
%    "claim_compliance": "Feedback on claims structure",
%    "description_alignment": "Feedback on description alignment",
%    "legal_risks": "Identified risks and mitigation suggestions"
%}
%    \end{verbatim}
%\end{itemize}
%
%\textbf{Important Instructions:}  
%Ensure all sections are legally compliant, consistent, and meet patent office requirements. Collaborate with the meta-agent as needed for refinement.
%
%\vspace{2mm}
%\textbf{Goal:}  
%Deliver a legally accurate, complete, and consistent patent application ready for submission. Ensure enforceability by identifying and resolving potential risks or issues through reflection and collaboration.
%
%\end{tcolorbox}
%\end{figure*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,            
%    colframe=black!70,                
%    title=\textbf{Instructional Prompt for Gold-LLM-as-a-Judge}, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Act as an expert reviewer to evaluate the patent draft. Identify errors, inconsistencies, or missing elements, and attribute them to the appropriate agents for corrective actions. Provide actionable feedback aligned with the patent’s objectives to ensure high-quality submission.
%
%\textbf{Steps}  
%\begin{itemize}
%    \item \textbf{1. Review the Patent Sections for Coherence and Completeness}  
%    \begin{itemize}
%        \item Review the title, abstract, detailed description, claims, and summary.
%        \item Ensure alignment between sections for coherence and logical consistency.
%    \end{itemize}
%
%    \item \textbf{2. Attribute Errors to Relevant Agents}  
%    \begin{itemize}
%        \item Trace issues to responsible agents (e.g., claims or detailed-description agents).
%        \item Provide specific suggestions for corrective actions.
%    \end{itemize}
%
%    \item \textbf{3. Handle Conflicting Feedback Across Agents}  
%    \begin{itemize}
%        \item Identify inconsistencies in feedback from agents.
%        \item Recommend resolutions to the meta-agent to reconcile conflicting inputs.
%    \end{itemize}
%
%    \item \textbf{4. Generate Review Report and Provide JSON Output}  
%    \begin{verbatim}
%{
%    "section": "Claims",
%    "score": 8,
%    "feedback": "Claims are well-structured but need clearer 
%                 support in the description.",
%    "next_step": "Reassign to detailed-description agent for refinement."
%}
%    \end{verbatim}
%
%    \item \textbf{5. Submit Feedback to the Meta-Agent}  
%    \begin{itemize}
%        \item Report findings to the meta-agent for final orchestration and task reassignment.
%    \end{itemize}
%\end{itemize}
%
%\textbf{Goal:}  
%Ensure the patent document meets high-quality standards through expert-level review, providing actionable feedback for iterative improvement.
%
%\end{tcolorbox}
%\end{figure*}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray,            
%    colframe=black!70,                
%    title=\textbf{Instructional Prompt for Reward-Model-as-a-Judge}, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%Your task is to assign quantitative scores to each section of the patent application and provide actionable feedback to guide iterative refinement. Use the scores to prioritize areas needing improvement, ensuring the final document meets quality and legal standards.
%
%\textbf{Steps}
%\begin{itemize}
%    \item \textbf{1. Assign Scores to Patent Sections}  
%    \begin{itemize}
%        \item Evaluate the clarity, precision, and completeness of each section (e.g., title, claims, description).  
%        \item Assign scores on a scale of 1 to 10, with 10 indicating the highest quality.
%    \end{itemize}
%
%    \item \textbf{2. Provide Weighted Scores for Critical Sections}  
%    \begin{itemize}
%        \item Prioritize scores for sections critical to patentability, such as claims and the description.  
%        \item Adjust scores based on relevance and section importance.
%    \end{itemize}
%
%    \item \textbf{3. Generate Feedback Based on Scores}  
%    \begin{itemize}
%        \item Provide actionable suggestions for sections with low scores.  
%        \item Example: “Score: 6 - Claims need clearer alignment with the description.”
%    \end{itemize}
%
%    \item \textbf{4. Submit the Feedback Report in JSON Format}  
%    \begin{verbatim}
%{
%    "section": "Abstract",
%    "score": 7,
%    "feedback": "Abstract captures the key points but lacks clarity.",
%    "next_step": "Refine with summary-generation agent."
%}
%    \end{verbatim}
%
%    \item \textbf{5. Report Findings to the Meta-Agent}  
%    \begin{itemize}
%        \item Submit the scores and recommendations to the meta-agent for coordination.
%    \end{itemize}
%\end{itemize}
%
%\textbf{Goal:}  
%Guide the refinement process using quantitative scores and actionable feedback, ensuring the patent document achieves high standards before submission.
%
%\end{tcolorbox}
%\end{figure*}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{figure*}[h!]
%\begin{tcolorbox}[
%    colback=white!95!gray, 
%    colframe=black!70, 
%    title=\textbf{Instructional Prompt for Meta-Agent}, 
%    fonttitle=\bfseries\large, 
%    sharp corners, 
%    boxrule=1.2pt, 
%    coltitle=black, 
%    title filled, 
%    boxsep=4pt, left=4pt, right=4pt, top=4pt, bottom=4pt
%]
%
%\textbf{Task for Meta-Agent:}  
%
%As the central orchestrator of the \textbf{Inventa framework}, your role is to coordinate specialized agents powered by large language models (LLMs) to generate and validate hypotheses, conduct prior art searches, draft inventions, and compile a legally compliant patent document. Below is the logical task flow with iterative feedback loops and refinements aligned with the framework’s methodology.
%
%\begin{itemize}
%    \item \textbf{Step 1: Hypothesis Generation}
%    \begin{itemize}
%        \item Engage the \textbf{Hypothesis Generation Agent} to generate new hypotheses using the APHD algorithm on Patent Hypothesis Exploration Graphs (PHEGs).
%        \item Extract insights from the \textbf{Patent Knowledge Graph (PKG)} to identify technological gaps and trends.
%    \end{itemize}
%
%    \item \textbf{Step 2: Scientific Validation of Hypotheses}
%    \begin{itemize}
%        \item Assign the \textbf{Scientific Discovery Agent} to validate hypotheses using insights from the \textbf{Scientific Knowledge Graph (ScKG)}.
%        \item If the hypothesis lacks scientific grounding, reassign it to the Hypothesis Generation Agent for refinement.
%    \end{itemize}
%
%    \item \textbf{Step 3: Ideation and Transformation into Patentable Inventions}
%    \begin{itemize}
%        \item Engage the \textbf{Ideation Agent} to transform validated hypotheses into practical and patentable inventions.
%        \item Ensure the inventions align with both the identified technological gaps and scientific advancements.
%    \end{itemize}
%
%    \item \textbf{Step 4: Prior Art Search and Patentability Evaluation (Sequential Dependency)}
%    \begin{itemize}
%        \item Assign the \textbf{Prior Art Search Agent} to conduct a thorough search across patent databases and identify related inventions.
%        \item Engage the \textbf{Patentability Evaluation Agent} to assess the novelty, feasibility, non-obviousness, and utility of the invention.
%        \item If prior art reveals similar inventions, refine the invention by re-engaging the Hypothesis Generation Agent.
%    \end{itemize}
%
%    \item \textbf{Step 5: Background Writing}
%    \begin{itemize}
%        \item Assign the \textbf{Background-Writing Agent} to draft the background section, contextualizing the invention and identifying existing solutions and gaps.
%    \end{itemize}
%
%    \item \textbf{Step 6: Detailed Description and Claim Generation (Sequential Workflow)}
%    \begin{itemize}
%        \item Engage the \textbf{Detailed-Description Agent} to document technical features and functionalities.
%        \item Assign the \textbf{Claim-Generation Agent} to draft independent and dependent claims aligned with the description.
%        \item During claim generation, engage the \textbf{Legal-Compliance Agent} to ensure claims meet legal standards.
%    \end{itemize}
%
%    \item \textbf{Step 7: Summary, Abstract, and Title Generation}
%    \begin{itemize}
%        \item Assign the \textbf{Summary-Generation Agent} to outline the invention’s key aspects.
%        \item Engage the \textbf{Abstract-Generation Agent} to draft a concise abstract aligned with the claims.
%        \item Use the \textbf{Title-Generation Agent} to generate a precise title reflecting the invention’s purpose and novelty.
%    \end{itemize}
%
%    \item \textbf{Step 8: Legal Compliance Review and Iterative Feedback}
%    \begin{itemize}
%        \item Perform a final legal review using the \textbf{Legal-Compliance Agent} to ensure compliance across all sections.
%        \item Submit the draft to \textbf{Gold-LLM-as-a-Judge} and \textbf{Reward-Model-as-a-Judge} for feedback.
%        \item If errors are detected, reassign the relevant sections to appropriate agents for correction.
%    \end{itemize}
%
%    \item \textbf{Step 9: Final Validation and Submission}
%    \begin{itemize}
%        \item Perform a final validation to ensure all sections are aligned and consistent.
%        \item Compile the finalized patent document and prepare it for submission to the relevant patent office.
%    \end{itemize}
%\end{itemize}
%
%\vspace{2mm}
%\noindent
%\textbf{Goal:}  
%The meta-agent ensures efficient coordination, manages dependencies, resolves conflicts, and integrates feedback from multiple agents to produce a high-quality, legally compliant patent document ready for submission.
%
%\end{tcolorbox}
%\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\begin{figure}[!ht]
%\vspace{-2mm}
%\centering
%\resizebox{0.775\linewidth}{!}{%
%    \hspace*{0mm}\includegraphics[keepaspectratio,trim=0.0cm 0cm 0cm 0.15cm,clip]{Images/Figure_1.pdf} % left, bottom, right, top
%}
%\vspace{2mm} % Space between the two figures
%\resizebox{0.775\linewidth}{!}{%
%    \hspace*{0mm}\includegraphics[keepaspectratio,trim=0.0cm 0cm 0cm 0.15cm,clip]{Images/Figure_2.pdf} % left, bottom, right, top
%}
%\vspace{-2mm}
%\caption{
%This figure shows visual representations of graphs projected into a circular layout, highlighting clusters and specific paths. The graphs are created using random geometric models, with edges limited to maintain clarity. Nodes are distributed in circular regions by assigning polar coordinates with random radii and angles. Clustering is performed using KMeans, assigning nodes to distinct groups color-coded for differentiation. In selected clusters (e.g., blue and green), nearby nodes are connected using black lines to highlight short paths, emphasizing local relationships within clusters. This approach provides a visually intuitive way to explore graph structures and node proximities within clusters.
%}
%\label{fig:figure1}
%\vspace{-3mm}
%\end{figure}


\end{document}








