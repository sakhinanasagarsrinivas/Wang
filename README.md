\begin{abstract}
Recent advancements in generative AI have accelerated the discovery of novel chemicals and materials, yet transitioning these discoveries to industrial-scale production remains a critical bottleneck. Current AI methods cannot auto-generate Process Flow Diagrams (PFDs) or Piping and Instrumentation Diagrams (PIDs)—essential for scaling chemical processes—while adhering to engineering constraints. We introduce a closed-loop, physics-aware framework that automates the generation of industrially viable PFDs and PIDs by integrating domain-specialized Small Language Models (SLMs) trained for chemical process QA tasks (e.g., equipment selection, control logic validation)with first-principles simulation validation. Our approach combines: (1) a hierarchical knowledge graph of 1,020+ chemicals for retrieval-augmented generation, (2) synthetic data pipelines fine-tuned via multi-stage training (Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction Tuning (RAIT)), and (3) simulator-in-the-loop validation (DWSIM) to ensure feasibility. Experiments demonstrate that the framework generates simulator-validated process descriptions with high fidelity, outperforms baseline methods in correctness and complexity, and generalizes to unseen chemicals. By bridging AI-driven design with industrial-scale feasibility, this work significantly reduces R\&D timelines from lab discovery to plant deployment.
\end{abstract}

\vspace{-2mm} 
\subsection{Results} 
We present a comparative evaluation of Llama-3.2-1B and SmolLM-135M across successive fine-tuning stages, using held-out test splits and a comprehensive set of performance metrics. Evaluation was conducted using both token-level n-gram overlap metrics (BLEU, ROUGE-1/2/L, METEOR, SacreBLEU) and embedding-based semantic similarity metrics (BERTScore and Sentence-BERT cosine similarity), with all scores normalized to the \([0,1]\) interval, as shown in Figures~\ref{fig:LlamaQA_metrics}–\ref{fig:SmolLMRAG_metrics}. Following supervised fine-tuning (SFT) on the train splits of the Factual QA, SynDIP, and LogiCore datasets, Llama-3.2-1B (Figure~\ref{fig:LlamaQA_metrics}) demonstrates strong semantic alignment. This is evidenced by high BERTScore and sentence similarity, despite lower performance on n-gram metrics, indicating a preference for paraphrastic generation over lexical overlap. In contrast, SmolLM-135M (Figure~\ref{fig:SmolLMQA_metrics}), fine-tuned on the same training data, exhibits relatively higher n-gram overlap scores and sentence similarity, while achieving moderate BERTScore, suggesting a tendency toward surface-level fidelity. Subsequent to Direct Preference Optimization (DPO), trained on the train split of the DPO dataset and evaluated on its test split, Llama-3.2-1B (Figure~\ref{fig:LlamaDPO_metrics}) maintains its profile of high semantic similarity, whereas SmolLM-135M (Figure~\ref{fig:SmolLMDPO_metrics}) displays balanced improvements across both lexical and semantic metrics, reflecting effective alignment tuning via reward modeling. 
Following Retrieval-Augmented Instruction Tuning (RAIT), which was performed using the train splits of the Local and Global RAIT datasets and evaluated on their respective test splits, Llama-3.2-1B (Figure~\ref{fig:LlamaRAG_metrics}) continues to show dominant semantic scores relative to n-gram metrics. SmolLM-135M (Figure~\ref{fig:SmolLMRAG_metrics}) exhibits comparatively lower scores across most metrics, with sentence similarity remaining the strongest signal, suggesting diminished generalization capacity under retrieval-augmented long-context settings. 
These plots provide phase-by-phase performance diagnostics, highlighting how successive fine-tuning regimes induce distinct response behaviors across models in terms of semantic coherence, lexical fidelity, and alignment with training objectives. In addition, we conduct a systematic evaluation of how fine-tuning (FT) and Graph Retrieval-Augmented Generation (Graph RAG) affect qualitative performance across six language model variants, comprising two model architectures at different scales: the larger Llama-3.2-1B and more compact SmolLM2-135M. Each model variant represents a distinct configuration: (a) Llama-3.2-1B with both FT and Graph RAG (W/FT W/Graph RAG), (b) Llama-3.2-1B with FT but without Graph RAG (W/FT W/o Graph RAG), (c) Llama-3.2-1B without either FT or Graph RAG (W/o FT W/o Graph RAG), (d) Llama-3.2-1B without FT but with Graph RAG (W/o FT W/Graph RAG), (e) SmolLM2-135M with both FT and Graph RAG (W/FT W/Graph RAG), and (f) SmolLM2-135M with FT but without Graph RAG (W/FT W/o Graph RAG). Performance is rigorously assessed using the NVIDIA Nemotron-4-340B reward model across five key qualitative dimensions: helpfulness (measuring practical utility), correctness (factual accuracy), coherence (logical flow), complexity (depth of content), and verbosity (response length), with detailed results presented in Figures~\ref{fig:sAF}a-e. The evaluation reveals several important findings regarding model scale and methodological impact. Among Llama-3.2-1B variants, the FT+RAG configuration (variant a) demonstrates superior performance, achieving the highest scores in correctness and complexity by effectively combining fine-tuned capabilities with retrieved knowledge, though this comes with increased verbosity due to the incorporation of supplementary content from the knowledge graph. The FT-only variant (b) maintains strong performance in coherence and helpfulness but shows limitations in knowledge-intensive tasks without retrieval support. Notably, even without fine-tuning, the Graph RAG-enabled Llama variant (d) outperforms the baseline (c) in correctness, demonstrating that retrieval augmentation can partially compensate for the absence of task-specific tuning. However, the complete absence of both methods (variant c) results in the weakest performance, highlighting the limitations of relying solely on pretrained knowledge. For the smaller SmolLM2-135M models, Graph RAG improves correctness (variant e vs. f), but both configurations underperform relative to the corresponding Llama-3.2-1B variants across all metrics, notably in coherence and complexity. This performance gap underscores the importance of model scale in effectively utilizing both fine-tuning and retrieval augmentation. The results demonstrate that while FT substantially enhances overall response quality across all metrics by aligning the model with domain-specific requirements, Graph RAG provides complementary benefits primarily for factual accuracy. This combination proves particularly valuable in specialized domains like chemical process synthesis, where both task adaptation and external knowledge integration are crucial for high-quality generation. The study conclusively shows that the optimal configuration—Llama-3.2-1B with both FT and Graph RAG—achieves the most balanced performance across all evaluation dimensions, successfully integrating structured knowledge retrieval with fine-tuned language understanding capabilities while maintaining reasonable verbosity levels. These findings have important implications for deploying language models in technical domains where both factual precision and contextual understanding are paramount.

\section{Conclusion}
Automating the generation of industrially viable Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (PIDs) is critical for accelerating chemical process scale-up. Our closed-loop framework achieves this by integrating domain-adapted small language models (SLMs) with physics-aware validation to enable end-to-end automation. The approach combines multi-stage SLM fine-tuning—leveraging synthetic datasets and retrieval augmentation from a hierarchical chemical knowledge graph—with rigorous simulation-based validation using DWSIM. Results demonstrate that the synergy between fine-tuning and Graph Retrieval-Augmented Generation (RAG) enables high-fidelity PFD/PID generation with strong generalization capabilities. Specifically, the framework exhibits robust performance in zero-shot synthesis of novel chemical production processes and excels at core engineering QA tasks, including PFD/PID interpretation and analysis. By unifying generative AI with first-principles engineering constraints, the framework effectively bridges the gap between digital discovery and industrial deployment, addressing key R\&D bottlenecks. Future work will focus on expanding capabilities through deeper integration with closed-loop simulation for automated process optimization and improving usability through direct generation of standard-format engineering diagrams (e.g., Visio, CAD). This work presents a validated pathway toward more efficient and reliable AI-driven chemical process design.
